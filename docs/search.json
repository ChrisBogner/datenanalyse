[{"path":"index.html","id":"vorwort","chapter":"Vorwort","heading":"Vorwort","text":"‚Äúhoney, ‚Äôre gonna style‚Äù","code":""},{"path":"index.html","id":"organisatorisches","chapter":"Vorwort","heading":"Organisatorisches","text":"\nDie Coronaviruspandemie hat unser Leben und unser Lernen ver√§ndert.\nDie vergangenen digitalen Semester haben ein paar n√ºtzliche Werkzeuge\netabliert, die ich gerne die Pr√§senzlehre √ºbernehmen m√∂chte.\ndieser Veranstaltung werden wir folgende Werkzeuge verwenden:ILIAS: die Online-Lernplattform der UzK. Entweder sind Sie bereits automatisch dem Kurs registriert oder werden von mir per Hand angemeldet.Campuswire: die Chatplattform dient der allgemeinen Kommunikation und der Selbstorganisation des Lernens. Verwenden Sie diese, um Fragen mit Ihren Kommilitonen*innen und mir zu diskutieren. Sie sollten eine Einladungsmail zu Campuswire erhalten haben.Zoom: die Videokonferenz-Software bleibt unser Notfall-Werkzeug, falls keine Pr√§senz m√∂glich ist.","code":""},{"path":"index.html","id":"verwendete-literatur","chapter":"Vorwort","heading":"Verwendete Literatur","text":"Wir werden diesem Kurs haupts√§chlich das freie, englischsprachige Buch ModernDive: Statistical Inference via Data Science benutzen (Ismay Kim 2021). Bitte lassen Sie sich nicht davon abschrecken, dass das Buch englischsprachig ist. Es ist sehr gut verst√§ndlich und bietet einen modernen Zugang zur Datenanalyse. Als ein weiteres Buch empfehle ich Sauer (2019).Gelegentlich werde ich Ihnen auch andere Literatur empfehlen. F√ºr Ihren Abschlussbericht werden Sie auch selbstst√§ndig weitere Literatur recherchieren.","code":""},{"path":"index.html","id":"sinn-und-unsinn-dieses-skripts","chapter":"Vorwort","heading":"Sinn und Unsinn dieses Skripts","text":"Dieses Skript ist ein lebendiges Begleitdokument des Kurses. Es wird laufend angepasst und aktualisiert.Ich nutze verschiedenfarbige Bl√∂cke, um wichtige Stellen hervorzuheben:\nInfoblock\n\nAchtung, wichtig!\n\nDefinition\n\nLernziele\n","code":""},{"path":"index.html","id":"inspiration-quellen-und-danksagung","chapter":"Vorwort","heading":"Inspiration, Quellen und Danksagung","text":"Dieses Skript baut stark auf folgenden freien Quellen auf:r4ds: Wickham Grolemund (2021)ggplot2: Wickham (2020)ModernDive: Ismay Kim (2021)Den Autoren dieser B√ºcher gilt ein gro√üer Dank f√ºr Ihren Beitrag zur -Community !","code":""},{"path":"index.html","id":"reproduzierbarkeit","chapter":"Vorwort","heading":"Reproduzierbarkeit","text":"Dieses Buch wurde RStudio mit Bookdown geschrieben und R version 4.2.0 (2022-04-22) gebaut. Folgende Pakete werden f√ºr die Beispiele und √úbungen ben√∂tigt:Die komplette Information zur Session lautet:Dieses Skript ist lizenziert unter Creative Commons Namensnennung - Nicht-kommerziell - Weitergabe unter gleichen Bedingungen 4.0 International.","code":"## R version 4.2.0 (2022-04-22)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 22.04 LTS\n## \n## Matrix products: default\n## BLAS:   /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3\n## LAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3\n## \n## locale:\n##  [1] LC_CTYPE=de_DE.UTF-8       LC_NUMERIC=C              \n##  [3] LC_TIME=de_DE.UTF-8        LC_COLLATE=de_DE.UTF-8    \n##  [5] LC_MONETARY=de_DE.UTF-8    LC_MESSAGES=de_DE.UTF-8   \n##  [7] LC_PAPER=de_DE.UTF-8       LC_NAME=C                 \n##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C       \n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] emo_0.0.0.9000    DT_0.22           forcats_0.5.1     stringr_1.4.0    \n##  [5] dplyr_1.0.9       purrr_0.3.4       readr_2.1.2       tidyr_1.2.0      \n##  [9] tibble_3.1.7      ggplot2_3.3.6     tidyverse_1.3.1   kableExtra_1.3.4 \n## [13] fontawesome_0.2.2\n## \n## loaded via a namespace (and not attached):\n##  [1] tufte_0.12        svglite_2.1.0     lubridate_1.8.0   rprojroot_2.0.3  \n##  [5] assertthat_0.2.1  digest_0.6.29     utf8_1.2.2        R6_2.5.1         \n##  [9] cellranger_1.1.0  backports_1.4.1   reprex_2.0.1      evaluate_0.15    \n## [13] highr_0.9         httr_1.4.2        pillar_1.7.0      rlang_1.0.2      \n## [17] readxl_1.4.0      rstudioapi_0.13   jquerylib_0.1.4   rmarkdown_2.14   \n## [21] desc_1.4.1        webshot_0.5.3     htmlwidgets_1.5.4 munsell_0.5.0    \n## [25] broom_0.8.0       compiler_4.2.0    modelr_0.1.8      xfun_0.30        \n## [29] pkgconfig_2.0.3   systemfonts_1.0.4 htmltools_0.5.2   downlit_0.4.0    \n## [33] tidyselect_1.1.2  bookdown_0.26     fansi_1.0.3       viridisLite_0.4.0\n## [37] withr_2.5.0       crayon_1.5.1      tzdb_0.3.0        dbplyr_2.1.1     \n## [41] grid_4.2.0        jsonlite_1.8.0    gtable_0.3.0      lifecycle_1.0.1  \n## [45] DBI_1.1.2         magrittr_2.0.3    scales_1.2.0      cli_3.3.0        \n## [49] stringi_1.7.6     cachem_1.0.6      fs_1.5.2          xml2_1.3.3       \n## [53] bslib_0.3.1       ellipsis_0.3.2    generics_0.1.2    vctrs_0.4.1      \n## [57] tools_4.2.0       glue_1.6.2        hms_1.1.1         fastmap_1.1.0    \n## [61] yaml_2.3.5        colorspace_2.0-3  sessioninfo_1.2.2 rvest_1.0.2      \n## [65] memoise_2.0.1     knitr_1.39        haven_2.5.0       sass_0.4.1"},{"path":"einfuehrung.html","id":"einfuehrung","chapter":"Der Kurs","heading":"Der Kurs","text":"","code":""},{"path":"einfuehrung.html","id":"zuordnung-zum-modul-und-leistungsnachweis","chapter":"Der Kurs","heading":"Zuordnung zum Modul und Leistungsnachweis","text":"Dieser Kurs geh√∂rt zum Modul Fachmethodik oder Fachmethodik II und ist aus 4 SWS Praktikum und 2 SWS Seminar aufgebaut. Das wichtigste Ziel besteht darin, Ihnen einen sicheren Umgang mit R beizubringen.Den Leistungsnachweis bildet ein benoteter Praktikumsbericht.","code":""},{"path":"einfuehrung.html","id":"lernziele-des-kurses","chapter":"Der Kurs","heading":"Lernziele des Kurses","text":"\nDaten f√ºr Analysen vorbereiten\n\nDaten einlesen und visualisieren\n\nCode und Dokumentation R Markdown schreiben\n\neigene Funktionen schreiben\n\nreproduzierbare Datenanalysen durchf√ºhren\n\ngelernte Methoden auf einen neuen Datensatz anwenden\n\nErgebnisse reproduzierbar im Praktikumsbericht darstellen\n","code":""},{"path":"einfuehrung.html","id":"was-mir-im-umgang-miteinander-wichtig-ist","chapter":"Der Kurs","heading":"Was mir im Umgang miteinander wichtig ist","text":"P√ºnktlichkeit bei Pr√§senz- und ZoomsitzungenGute Vorbereitung durch Erledigen der HausaufgabenRespektieren anderer MeinungenOffenheit gegen√ºber neuen Sichtweisen, Themen und MethodenGeduld mit sich selbst und den anderen üòÑ","code":""},{"path":"erste-schritte.html","id":"erste-schritte","chapter":"Kapitel 1 Erste Schritte in R","heading":"Kapitel 1 Erste Schritte in R","text":"\nLayout und Bedeutung einzelner Fenster RStudio kennen\n\nAnweisungen aus dem Skript die Konsole schicken\n\nR als Taschenrechner benutzen\n\nerste Funktionen aufrufen\n\nObjekte mit eckigen Klammern [ ] ansprechen\n\nR-Hilfeseiten aufrufen\n\nIhren ersten Olot erstellen\n","code":""},{"path":"erste-schritte.html","id":"was-ist","chapter":"Kapitel 1 Erste Schritte in R","heading":"1.1 Was ist ?","text":"R ist eine Programmiersprache f√ºr Datenanalyse und statistische Modellierung. Es ist frei verf√ºgbar (open source software) und neben Python einer der meisten benutzten Programmiersprachen zur Datenanalyse und -visualisierung. R wurde von Ross Ihaka und Robert Gentleman 1996 ver√∂ffentlicht (Ihaka Gentleman 1996). Es gibt f√ºr R eine Vielzahl von Zusatzpaketen, die die Funktionalit√§t und die Einsatzm√∂glichkeiten enorm erweitern.Sie k√∂nnen R f√ºr Ihren Computer auf der offiziellen R-Seite https://www.r-project.org/ herunterladen und installieren. Eine kurze Anleitung finden Sie auf ILIAS, zusammen mit der Liste der Pakete, die wir diesm Kurs brachen werden. Zus√§tzlich k√∂nnen Sie sich hier ein Video zur Installation ansehen.Auf der offiziellen R-Seite finden Sie auch zus√§tzliche Pakete, und zwar unter CRAN (Comprehensive R Archive Network). Manche Pakete sind auf den CRAN-Seiten thematische sogen. CRAN Task Views gegliedert. F√ºr den Umweltbereich sind folgende Paketsammlungen besonders relevant:Environmetrics: Analyse von UmweltdatenMultivariate: Multivariate StatistikSpatial: Analyse von r√§umlichen DatenTimeSeries: ZeitreihenanalyseZu Beginn des Kurses werden wir jedoch nicht auf Ihren lokalen Rechnern arbeiten, sondern auf der RStudio Server Pro, der extra f√ºr die digitale Lehre mit R der UzK eingef√ºhrt wurde. Das erm√∂glicht einen schnelleren Einstieg R und bietet eine fast-live Unterst√ºtzung durch den Dozenten beim Programmieren. Daher biete ich zu diesem fr√ºhen Zeitpunkt im Kurs keine Unterst√ºtzung bei der Installation von R auf Ihren Privatrechnern. F√ºr die ganz Ungeduldigen, gibt es hier eine kurze Einleitung zur Installation.","code":""},{"path":"erste-schritte.html","id":"was-ist-rstudio","chapter":"Kapitel 1 Erste Schritte in R","heading":"1.2 Was ist RStudio?","text":"RStudio Desktop ist eine Entwicklungsumgebung f√ºr R. Sie k√∂nnen die open source Version kostenlos f√ºr Ihren Rechner hier herunterladen. Es gibt eine live Einf√ºhrung RStudio im Kurs. Zus√§tzlich k√∂nnen Sie hier ein Video dazu ansehen.\nAbbildung 1.1: Aufbau von RStudio\nSie sollten auch auf Ihrem eigenen Rechner einen Ordner f√ºr die Veranstaltung anlegen und darin jeweils einen Ordner f√ºr Daten, Skripte und Notebooks.","code":""},{"path":"erste-schritte.html","id":"lesestoff","chapter":"Kapitel 1 Erste Schritte in R","heading":"1.3 Lesestoff","text":"Kapitel 1.1 und 1.2 Ismay Kim (2021).","code":""},{"path":"erste-schritte.html","id":"aufgaben","chapter":"Kapitel 1 Erste Schritte in R","heading":"1.4 Aufgaben","text":"\nBitte speichern Sie Ihr Skript regelm√§√üig ab!\n","code":""},{"path":"erste-schritte.html","id":"ars-haushaltsbuch","chapter":"Kapitel 1 Erste Schritte in R","heading":"1.4.1 Ars Haushaltsbuch","text":"Der angehende Datenanalyst Ar Stat m√∂chte dem Rat seiner Mutter folgen und ein Haushaltsbuch anlegen. Zuerst m√∂chte er sich einen √úberblick √ºber seine Ausgaben der Uni-Mensa verschaffen und erstellt die folgende Tabelle:\nTabelle 1.1: Ars Mensaausgaben\nWie viel hat Ar insgesamt der Woche ausgegeben?Wie viel hat er im Schnitt pro Tag ausgegeben?Wie stark schwanken seine Ausgaben?Leider hat Ar sich beim √úbertragen der Daten vertippt. Er hat Dienstag seine Freundin zum Essen eingeladen und 7,95 ‚Ç¨ statt 2,90 ‚Ç¨ ausgegeben.Korrigieren Sie Ars Fehler.Wie und warum ver√§ndern sich die Ergebnisse aus den Teilaufgaben 1 bis 3?","code":""},{"path":"erste-schritte.html","id":"rob2","chapter":"Kapitel 1 Erste Schritte in R","heading":"1.4.2 Fehlende Werte","text":"R kodiert fehlende Werte mit NA. Ar Stat hat Montag der darauffolgenden Woche der Mensa gegessen, aber vergessen die Ausgaben zu notieren.\nTabelle 1.2: Ars Mensaausgaben, cont.\nWie √§ndert der fehlende Wert die Berechnung der Summe?Lesen Sie, passiert, wenn der Datenvektor bei der Berechnung der Summe fehlende Werte enth√§lt. Rufen Sie dazu die Hilfe auf, .e.¬†?sum.Korrigieren Sie die Berechnung der Summe entsprechend.","code":""},{"path":"erste-schritte.html","id":"firstplot","chapter":"Kapitel 1 Erste Schritte in R","heading":"1.4.3 Ihr erster Plot","text":"Vor allem Anfang kann die Lernkurve R recht flach verlaufen. Daher sollten Sie nicht vergessen, warum Sie R lernen, n√§mlich um echte Datens√§tze zu analysieren.Auch wenn Sie den Code unten noch nicht (ganz) verstehen, kopieren Sie ihn Ihr R und lassen Sie ihn laufen.Welche Daten sind diesem Datensatz enthalten? Nutzen Sie die Hilfe, .e.¬†?gapminder.stellen die Farben der Abbildung dar?wird durch die Symbolgr√∂√üe dargestellt?Wie w√ºrden Sie den Zusammenhang zwischen den Variablen GDP per capita und Life expectancy beschreiben?","code":"\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder2007 <- gapminder %>% \n  filter(year == 2007)\n\nggplot(gapminder2007, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  geom_point() +\n  scale_x_log10() +\n  xlab('GDP per capita') +\n  ylab('Life expectancy') +\n  labs(title = 'Gapminder data for the year 2007')"},{"path":"erste-schritte.html","id":"r-als-taschenrechner","chapter":"Kapitel 1 Erste Schritte in R","heading":"1.4.4 R als Taschenrechner","text":"R ist ein gro√üer Taschenrechner mit vielen bereits definierten Funktionen. Es gelten die √ºblichen Rechenregeln wie z.B. Punkt-vor-Strich und die Klammern.Schreiben Sie den Code, der 2 und 10 addiertDas korrekte Multiplikationszeichen R ist *.Geben Sie den folgenden Befehl korrekt R ein: (2 + 10) \\(\\times\\) 27Bei Dezimalzahlen wird der Dezimalpunkt und nicht das Komma verwendet. Das ist wichtig zu beachten, wenn Sie sp√§ter Daten R einlesen m√∂chten.Berechnen Sie die Summe von 2,34 und 4,98.","code":""},{"path":"erste-schritte.html","id":"zuweisungen","chapter":"Kapitel 1 Erste Schritte in R","heading":"1.4.5 Zuweisungen","text":"R arbeitet man mit Objekten. Ein Objekt kann alles M√∂gliche sein: eine Variable, Daten, Vektoren etc. Wenn also das Ergebnis einer Berechnung oder ein Datenobjekt im R-Arbeitsbereich (workspace) zur Verf√ºgung stehen soll, muss daraus ein Objekt erstellt werden.Objekte erstellt man, indem man ihnen Namen gibt. Diesen Vorgang nennt man Zuweisung (assignment). Im Beispiel unten wird ein Objekt, diesem Fall ein Skalar, namens x erzeugt, mit dem Wert 42. Um den Wert von x anzuzeigen, tippen Sie x ein.Zuweisungen k√∂nnen R entweder mit dem = erfolgen oder mit <-. Beide Varianten sind gleichwertig. Dabei ist allerdings Pfeilrichtung entscheidend! x <- 42 bedeutet: Die linke Seite (Zahl 42) wird dem Objekt x zugeordnet. Wenn man die Pfeilrichtung umdreht, macht die Zuweisung keinen Sinn und man erh√§lt eine Fehlermeldung.Objektnamen k√∂nnen (fast) frei gew√§hlt werden. Sie m√ºssen mit einem Buchstaben beginnen und d√ºrfen keine Sonderzeichen enthalten. Bei l√§ngeren Namen empfiehlt sich ein _. Streng verboten sind Namen von vordefinierten Funktionen!Erstellen Sie ein Objekt namens mein_objekt und weisen Sie ihm das Ergebnis der Berechnung \\(23^{2}\\) zu. Eine Potenz berechnen Sie mit ^.Eine Zuweisung kann auch kompliziertere Anweisungen enthalten. Hier erstellen wir z.B. einen Vektor mithilfe der Funktion c (concatenate) und weisen das Ergebnis dem Objekt my_a zu.","code":"\nx <- 42\n\n# Zeige den Wert von x\nx\n# Sinnvolle Zuweisung\nx <- 42\n# Gleichwertige sinnvolle Zuweisung\nx = 42\n# Sinnloser Ausdruck. Fehlermeldung!\nx -> 42## Error in 42 <- x: ung√ºltige (do_set) linke Seite in Zuweisung\nmy_a <- c(32, 54, 1.2, 398)"},{"path":"erste-schritte.html","id":"funktionsaufruf","chapter":"Kapitel 1 Erste Schritte in R","heading":"1.4.6 Funktionsaufruf","text":"R gibt es eine Vielzahl von vordefinierten Funktionen. Ein Funktionsaufruf hat immer die gleiche Form: mach_das(damit) oder mach_das(damit, und_mit_dieser_einstellung). Z.B. wird die Summe auf einem Objekt mein_objekt mit sum(mein_objekt) berechnet.Erstellen Sie einen Vektor mit den Zahlen 32, 54, 1,2 und 398 und weisen Sie ihn der Variablen my_a zu.Berechnen Sie die summe von my_a.Sie k√∂nnen im √úbrigen auch Vektoren sinnvoll addieren.Erstellen Sie einen Vektor my_b mit der passenden L√§nge und addieren Sie ihn zum Vektor my_a. Die Addition erfolgt elementweise.H√§ufig wollen wir f√ºr unsere Daten den Mittelwert berechnen.Berechnen Sie den Mittelwert von my_aBerechnen Sie die Standardabweichung von my_a.","code":""},{"path":"erste-schritte.html","id":"objekte-ansprechen","chapter":"Kapitel 1 Erste Schritte in R","heading":"1.4.7 Objekte ansprechen","text":"Um das ‚ÄúInnenleben‚Äù der Objekte R anzusprechen, gibt es verschieden M√∂glichkeiten. diesem Tutorial konzentrieren wir uns auf Vektoren. Um die einzelnen Komponenten im Vektor anzusprechen, benutzt man eckige Klammern [ ]. Um eine bestimmte Komponente zu adressieren (anzusprechen), schreibt man die Platznummer der Komponente die Klammer. Wenn man im Vektor my_c, z.B. die dritte Komponente extrahieren m√∂chte, dann schreibt man my_c[3]Wir k√∂nnen auch Vektoren erstellen, bei denen einzelne Elemente benannt sind.Elemente solchen Vektoren kann man mit Namen eckigen Klammern ansprechen. Die Namen m√ºssen Anf√ºhrungszeichen geschrieben werden. Es spielt keine Rolle, ob Sie einfache oder doppelte Anf√ºhrungszeichen benutzen.Fragen Sie nach dem Element Koeln im Vektor benannt.","code":"\nmy_c <- c(2, 45.7, pi, sqrt(23), 2^6)\nmy_c[3]## [1] 3.141593\nbenannt <- c('Koeln' = 50667, 'Berlin' = 10965, \"Stuttgart\" = 70173)"},{"path":"erste-schritte.html","id":"ihre-arbeit-einreichen","chapter":"Kapitel 1 Erste Schritte in R","heading":"1.5 Ihre Arbeit einreichen","text":"Speichern Sie Ihre .R Datei ab.Laden Sie die Datei auf ILIAS der dazugeh√∂rigen √úbung hoch.Nach der Abgabe erhalten Sie die Musterl√∂sung.Vergleichen Sie Ihre L√∂sung selbstst√§ndig mit der Musterl√∂sung.Stellen Sie entweder Campuswire (im #class-chat) oder der n√§chsten Sitzung Fragen, falls Sie bei den Aufgaben etwas nicht verstanden haben und die Musterl√∂sung es nicht aufkl√§ren konnte.\nBeachten Sie die Deadline f√ºr das Hochladen der Hausaufgaben!\n","code":""},{"path":"erste-schritte.html","id":"lesestoff-1","chapter":"Kapitel 1 Erste Schritte in R","heading":"1.6 Lesestoff","text":"r4ds, Kapitel 4 (Wickham Grolemund 2021)","code":""},{"path":"reproduzieren.html","id":"reproduzieren","chapter":"Kapitel 2 R Markdown f√ºr reproduzierbare Forschung","heading":"Kapitel 2 R Markdown f√ºr reproduzierbare Forschung","text":"\nWichtigkeit der Reproduzierbarkeit erkl√§ren\n\nBegriff literate programming definieren\n\nAufbau einer RMarkdown-Datei erkl√§ren\n\nEinen einfachen ersten reproduzierbaren Bericht schreiben\n","code":""},{"path":"reproduzieren.html","id":"warum-reproduzierbarkeit-in-der-forschung-wichtig-ist","chapter":"Kapitel 2 R Markdown f√ºr reproduzierbare Forschung","heading":"2.1 Warum Reproduzierbarkeit in der Forschung wichtig ist","text":"Als Motivation f√ºr dieses Thema empfehle ich das Video von Prof.¬†Roger Peng der John Hopkins Bloogmerg School Public Health.","code":""},{"path":"reproduzieren.html","id":"literate-programming-idee-von-donald-knuth","chapter":"Kapitel 2 R Markdown f√ºr reproduzierbare Forschung","heading":"2.2 Literate Programming Idee von Donald Knuth","text":"Die Idee, dass man den Code und die dazugeh√∂rige Interpretation (Text, Bericht etc.) nicht voneinander trennen sollte, geht auf Knuth (1984) zur√ºck. Mit Literate Programming meinte Knuth, dass Programme auch nichts anderes wie literarische Werke sind. Er setzte den Fokus darauf, mit Programmen menschlichen Benutzern zu erkl√§ren, man den Computer machen lassen m√∂chte. Also weg vom computer- hin zum mensch-zentrierten Zugang. wird Programmieren und unserem Fall die Datenanalyse verst√§ndlich und vor allem reproduzierbar.Leider ist es unserer modernen Forschungslandschaft immer noch nicht Standard. Das Trennen von Analyseergebnissen und Berichten (Forschungsartikeln) sorgt f√ºr viele (unentdeckte und unn√∂tige) Fehler und Frust.","code":""},{"path":"reproduzieren.html","id":"reproduzierbare-berichte-mit-r-markdown","chapter":"Kapitel 2 R Markdown f√ºr reproduzierbare Forschung","heading":"2.3 Reproduzierbare Berichte mit R Markdown","text":"R hat sein eigenes System von reproduzierbaren Berichten, genannt R Markdown (Xie, Allaire, Grolemund 2021). Es ist benutzerfreundlich und erm√∂glicht unterschiedliche Formate von Berichten, wie HTML-Dokumente, PDF-Dateien, Pr√§sentationsfolien usw.Es wird Sie vielleicht √ºberraschen, aber das Skript, das Sie gerade lesen, ist nichts anderes als ein ‚Äúliterarisch‚Äù programmiertes Buch R Bookdown (Xie, Allaire, Grolemund 2021), einem R-Paket speziell f√ºr lange R Markdown-Dokumente.Wir werden vor allem mit R Notebooks arbeiten, die eine gute Interaktion zwischen dem geschriebenen Text und dem R-Code erm√∂glichen. Das Notebook kann sowohl ein HTML-Dokument als auch PDF oder Word als endg√ºltiges Dokument umgewandelt werden. Diesen Prozess nennt man knit.","code":""},{"path":"reproduzieren.html","id":"ein-neues-r-notebook-erstellen","chapter":"Kapitel 2 R Markdown f√ºr reproduzierbare Forschung","heading":"2.4 Ein neues R Notebook erstellen","text":"Um ein neues R Notebook zu erstellen, klicken Sie das kleine gr√ºne Plus oben links und w√§hlen Sie R Notebook aus. Sie k√∂nnen es erst einmal bei untitled belassen (Abbildung 2.1).\nAbbildung 2.1: Neues R Notebook anlegen\nWenn Sie ein neues Notebook erstellen, enth√§lt das Template etwas Code. Lesen Sie sich das ruhig noch einmal durch, da es ein paar hilfreiche Tastenk√ºrzel und Tipps. Danach k√∂nnen Sie den Text unterhalb des Headers l√∂schen.","code":""},{"path":"reproduzieren.html","id":"header","chapter":"Kapitel 2 R Markdown f√ºr reproduzierbare Forschung","heading":"2.5 Der Header eines Notebooks","text":"Ein R Notebook (und jedes andere R Markdown Dokument) besteht aus einem Header (Kopf) und dem eigentlichen Text und Code. Der Header hat dabei ein bestimmtes Layout, auf das Sie unbedingt achten m√ºssen (Rechtschreibung!). Der Header ist immer zwischen drei Minuszeichen --- eingeschlossen. Bei komplizierteren Headern gibt es auch Einr√ºckungen (mit der Tab-Taste), die auch Bedeutung haben (s. weiterf√ºhrende Literatur). Wir bleiben bei einem einfachen Header ohne Einr√ºckungen (Abbildung 2.2).\nAbbildung 2.2: Einen neuen Chunk hinzuf√ºgen\nText kann einfach unterhalb des Headers und au√üerhalb der Chunks getippt werden. Die wichtigsten Layoutelemente f√ºr den Text finden Sie hier. R Markdown unterst√ºtzt mathematische Notation Latex-Stil. Eine Einf√ºhrung Latex w√ºrde dieser Stelle aber zu weit f√ºhren.Das R Notebook hat den Vorteil, dass man √ºber den Button Preview oben der Leiste sofort die Ergebnisse anzeigen lassen kann. Sie m√ºssen also nicht knitten. Falls Sie es doch m√∂chten, klicken Sie auf das kleine Dreieck neben dem Preview und suchen Sie sich ein Output-Format aus. Ein einmal ‚Äúgeknittetes‚Äù Notebook ist kein Notebook mehr (kein Preview). Damit es wieder zum Nobebook wird, m√ºssen Sie im Header output: html_notebbok einstellen (Abbildung 2.2).","code":""},{"path":"reproduzieren.html","id":"wichtigste-regeln-f√ºr-reproduzierbarkeit","chapter":"Kapitel 2 R Markdown f√ºr reproduzierbare Forschung","heading":"2.6 Wichtigste Regeln f√ºr Reproduzierbarkeit","text":"Ein weiteres Video von Prof.¬†Peng widmet sich den wichtigsten Regeln f√ºr Reproduzierbarkeit.","code":""},{"path":"reproduzieren.html","id":"lesestoff-2","chapter":"Kapitel 2 R Markdown f√ºr reproduzierbare Forschung","heading":"2.7 Lesestoff","text":"Intro zu Kapitel 2 (Basics), Kapitel 3.2.1 und 3.2.2 Xie, Allaire, Grolemund (2021)","code":""},{"path":"reproduzieren.html","id":"weiterf√ºhrende-literatur","chapter":"Kapitel 2 R Markdown f√ºr reproduzierbare Forschung","heading":"2.8 Weiterf√ºhrende Literatur","text":"r4ds, Kapitel 27 (Wickham Grolemund 2021)","code":""},{"path":"reproduzieren.html","id":"aufgaben-1","chapter":"Kapitel 2 R Markdown f√ºr reproduzierbare Forschung","heading":"2.9 Aufgaben","text":"","code":""},{"path":"reproduzieren.html","id":"erstes-notebook","chapter":"Kapitel 2 R Markdown f√ºr reproduzierbare Forschung","heading":"2.9.1 Erstes Notebook","text":"Erstellen Sie ein R Notebook.F√ºgen Sie Layoutelemente hinzu:\n√úberschrift\nUnter√ºberschrift\nkursiver Text\nein Exponent: R2\nein Mathematikelement: \\(x^2\\)\neine Liste\n√úberschriftUnter√ºberschriftkursiver Textein Exponent: R2ein Mathematikelement: \\(x^2\\)eine ListeNutzen Sie die unter 2.5 verlinkte Liste der Layoutelemente.","code":""},{"path":"reproduzieren.html","id":"erste-schritte-als-notebook","chapter":"Kapitel 2 R Markdown f√ºr reproduzierbare Forschung","heading":"2.9.2 Erste Schritte als Notebook","text":"Wandeln Sie das R-Skript der ersten Session ein R Notebooks um.F√ºgen Sie mehr Erkl√§rungstext zu den einzelnen Schritten hinzu.Gliedern Sie Ihr Notebook mit passenden Layoutelementen.","code":""},{"path":"daten.html","id":"daten","chapter":"Kapitel 3 Datentypen in R","heading":"Kapitel 3 Datentypen in R","text":"\neinen Datensatz aus einem Paket mit data() laden\n\neinen Datensatz explorieren\n\ndata.frame() und tibble() erstellen\n\nDatens√§tze zusammenfassen\ndiesem Kapitel geht es um eine kurze Exploration des Datensatzes gapminder und die Vorstellung von Datenobjekten und -typen R.Zuerst laden wir die notwendigen Pakete mithilfe der Funktion library() und geben den Paketnamen (ausnahmsweise) ohne Anf√ºhrungszeichen .Man kann nur solche Pakete laden, die man bereits installiert hat. Um ein Paket zu installieren, nutzen wir die Funktion install.packages() und geben den Namen des Pakets Anf√ºhrungszeichen .","code":"\nlibrary(ggplot2)\nlibrary(gapminder)\ninstall.packages('gapminder')"},{"path":"daten.html","id":"datenobjekte","chapter":"Kapitel 3 Datentypen in R","heading":"3.1 Datenobjekte","text":"Nun laden wir den Datensatz gapminder aus dem gleichnamigen Paket mithilfe der Funktion data(). Diese Funktion l√§dt Datens√§tze, die bereits R base oder mit einem Paket installiert wurden.Um sich die Daten anzeigen zu lassen, geben wir den Namen des Datensatzes ein.Daten werden R Form von tabellenartigen Objekten abgelegt, sogen. data.frame oder tibble. Letztere sind die modernere Variante. Im Englischen w√ºrde man beides als dataframe bezeichnen.Der Typ des Objekts steht oben, wenn man sich das Objekt anzeigen l√§sst. Im Fall des Objekts gapminder handelt es sich um ein tibble. Tibbles und Dataframes sind zweidimensional: erste Dimension sind die Zeilen, zweite die Spalten. Wir k√∂nnen die Anzahl der Zeilen und Spalten direkt der ersten Zeile der Ausgabe ablesen: 1704 Zeilen und 6 Spalten. Alternativ kann man mit dem Befehl dim() die Gr√∂√üe des Objekts abfragen.","code":"\ndata(gapminder)\ngapminder## # A tibble: 1,704 √ó 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # ‚Ä¶ with 1,694 more rows\ndim(gapminder)"},{"path":"daten.html","id":"der--operator","chapter":"Kapitel 3 Datentypen in R","heading":"3.2 Der $-Operator","text":"einem tibble oder data.frame stellen Spalten verschiedene Variablen dar. ist etwa die erste Spalte country eine Variable, die L√§ndernamen enth√§lt. einer Zelle befindet sich jeweils ein Wert. jeder Zeile stehen Eintr√§ge f√ºr verschiedene Variablen, die logisch zusammengeh√∂ren: der ersten Zeile befinden sich Daten zu Afghanistan, das Asien liegt, aus dem Jahr 1952 zu Lebenserwartung, Bev√∂lkerungsgr√∂√üe und Bruttoinlandsprodukt pro Einwohner. Solche Datenstrukturen nennt man tidy. Spalten (.e.¬†Variablen) haben der Regel Namen, und k√∂nnen mit diesen direkt angesprochen werden. Dazu benutzt man das Dollarzeichen, den $-Operator. Um z.B. die Variable country anzusprechen, tippen wir:Eine einzelne Spalte eines tibble ist ein Vektor und wird nicht sch√∂n dargestellt wie das Tibble selbst. Um nur die ersten 6 Zeilen zu sehen, kann man die Funktion head() benutzen.","code":"\ngapminder$country\nhead(gapminder$country)"},{"path":"daten.html","id":"datentypen","chapter":"Kapitel 3 Datentypen in R","heading":"3.3 Datentypen","text":"Die Datentypen der einzelnen Variablen sieht man der zweiten Zeile der Ausgabe, wenn man sich das Tibble anzeigen l√§sst:Die Variablen country und continent sind sogenannte Faktoren (abgek√ºrzt mit <fct>). Es handelt sich hier um nominalskalierte Daten, .e.¬†eine endliche, abz√§hlbare Anzahl von unterschiedlichen Messwerten ist m√∂glich.Alternativ k√∂nnen wir nach dem Datentypen mithilfe der Funktion class() fragen.Die einzelnen Werte eines factor kann man sich mit der Funktion levels() anzeigen lassen.Die Variable year ist numerisch, beinhaltet aber nur ganze Zahlen und ist daher eine integer Variable (abgek√ºrzt mit <int>).Die Variable lifeExp ist numerisch und verh√§ltnisskaliert (abgek√ºrzt mit <dbl> f√ºr double). Letzteres bedeutet, dass es f√ºr dies Variable einen absoluten Nullpunkt gibt, n√§mlich Lebenserwartung von null Jahren.Um einen installierten Datensatz kennenzulernen, empfiehlt es sich, die Hilfeseiten dazu zu lesen.","code":"\ngapminder## # A tibble: 1,704 √ó 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # ‚Ä¶ with 1,694 more rows\nclass(gapminder$country)## [1] \"factor\"\nlevels(gapminder$country)\nclass(gapminder$year)## [1] \"integer\"\nclass(gapminder$lifeExp)## [1] \"numeric\"\n?gapminder"},{"path":"daten.html","id":"kurze-exploration","chapter":"Kapitel 3 Datentypen in R","heading":"3.4 Kurze Exploration","text":"Die Funktion summary() zeigt einen ersten √úberblick √ºber die Daten. Je nach Datentyp fasst diese Funktion die Daten unterschiedlich zusammen.Alternativ kann man sich einen ersten Eindruck von den Daten mithilfe der Funktion glimpse() verschaffen. Sie zeigt die ersten paar Eintr√§ge des Datensatzes.","code":"\nsummary(gapminder)##         country        continent        year         lifeExp     \n##  Afghanistan:  12   Africa  :624   Min.   :1952   Min.   :23.60  \n##  Albania    :  12   Americas:300   1st Qu.:1966   1st Qu.:48.20  \n##  Algeria    :  12   Asia    :396   Median :1980   Median :60.71  \n##  Angola     :  12   Europe  :360   Mean   :1980   Mean   :59.47  \n##  Argentina  :  12   Oceania : 24   3rd Qu.:1993   3rd Qu.:70.85  \n##  Australia  :  12                  Max.   :2007   Max.   :82.60  \n##  (Other)    :1632                                                \n##       pop              gdpPercap       \n##  Min.   :6.001e+04   Min.   :   241.2  \n##  1st Qu.:2.794e+06   1st Qu.:  1202.1  \n##  Median :7.024e+06   Median :  3531.8  \n##  Mean   :2.960e+07   Mean   :  7215.3  \n##  3rd Qu.:1.959e+07   3rd Qu.:  9325.5  \n##  Max.   :1.319e+09   Max.   :113523.1  \n## \nglimpse(gapminder)## Rows: 1,704\n## Columns: 6\n## $ country   <fct> \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", ‚Ä¶\n## $ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, ‚Ä¶\n## $ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, ‚Ä¶\n## $ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8‚Ä¶\n## $ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12‚Ä¶\n## $ gdpPercap <dbl> 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, ‚Ä¶"},{"path":"daten.html","id":"visualisieren","chapter":"Kapitel 3 Datentypen in R","heading":"3.5 Visualisieren","text":"Einen Plot des Datensatzes haben wir schon einmal erzeugt.Die Variable year ist numerisch und kein factor. Daher kann man da keine Levels sehen. Wenn man aber trotzdem die einzelnen Jahre sich anzeigen lassen m√∂chte, dann hilft die Funktion unique().Wir sehen, dass es Eintr√§ge seit 1952 gibt. Wir ver√§ndern den Code , dass das Jahr 1952 dargestellt wird.","code":"\ngapminder2007 <- gapminder %>% \n  filter(year == 2007)\n\nggplot(gapminder2007, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  geom_point() +\n  scale_x_log10() +\n  xlab('GDP per capita') +\n  ylab('Life expectancy') +\n  labs(title = 'Gapminder data for the year 2007')\nunique(gapminder$year)##  [1] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007\ngapminder1952 <- gapminder %>% \n  filter(year == 1952)\n\nggplot(gapminder1952, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  geom_point() +\n  scale_x_log10() +\n  xlab('GDP per capita') +\n  ylab('Life expectancy') +\n  labs(title = 'Gapminder data for the year 1952')"},{"path":"daten.html","id":"lesestoff-3","chapter":"Kapitel 3 Datentypen in R","heading":"3.6 Lesestoff","text":"r4ds, Kapitel 10 (Wickham Grolemund 2021)","code":""},{"path":"daten.html","id":"aufgaben-2","chapter":"Kapitel 3 Datentypen in R","heading":"3.7 Aufgaben","text":"","code":""},{"path":"daten.html","id":"plot-der-lebenserwartung","chapter":"Kapitel 3 Datentypen in R","heading":"3.7.1 Plot der Lebenserwartung","text":"Stellen Sie statt der Variablen gdpPercap, die Bev√∂lkerungsgr√∂√üe pop auf der \\(x\\)-Achse dar. Wie ver√§ndern sich die Legenden? Die Skalierung der Symbole mit der Bev√∂lkerungsgr√∂√üe ist nicht sinnvoll. Ver√§ndern Sie die Skalierung zu Bruttoinlandsprodukt, indem Sie size = pop ersetzen mit size = gdpPercap.","code":""},{"path":"daten.html","id":"abflugdaten","chapter":"Kapitel 3 Datentypen in R","heading":"3.7.2 Abflugdaten","text":"Arbeiten Sie das Kapitel 1.4 Explore first datasets Ismay Kim (2021) durch. Eventuell m√ºssen Sie das Paket nycflights13 installieren.","code":""},{"path":"daten.html","id":"pinguine","chapter":"Kapitel 3 Datentypen in R","heading":"3.7.3 Pinguine","text":"Der Datensatz penguins aus dem Paket palmerpenguins eignet sich hervorragend zum √úben der Exploration. Die Website zum Datensatz erkl√§rt, wie er entstanden ist.\nAbbildung 3.1: Artwork @allison_horst\nF√ºhren Sie eine √§hnliche Exploration durch, wie wir sie f√ºr den Datensatz gapminder gemacht haben. Zum Visualisieren der Pinguine, nutzen Sie den folgenden Code:","code":"\nggplot(data = penguins, \n       mapping = aes(x = flipper_length_mm, y = body_mass_g, col = species)) +\n  geom_point() +\n  xlab('Flipper length (mm)') +\n  ylab('Body mass (g)')"},{"path":"ggplot.html","id":"ggplot","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"Kapitel 4 Einf√ºhrung in ggplot2","text":"\nAufbau des Aufrufs der Funktion ggplot() kennen\n\nf√ºnf wichtigste Grafiktypen kennen und einsetzten\n","code":""},{"path":"ggplot.html","id":"aufbau-eines-visualisierungsbefehls","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.1 Aufbau eines Visualisierungsbefehls","text":"Das Paket ggplot2 ist ein sehr m√§chtiges Visualisierungswerkzeug. Der Name steht f√ºr ‚Äúgrammar graphics‚Äù. Das bedeutet, dass man mithilfe von verschiedenen Funktion ggplot2 seine Grafik Schritt f√ºr Schritt aufbaut, wie einen (grammatikalisch korrekten) Satz. aller K√ºrze bedeutet das:Eine statistische Grafik ist eine Zuordnung (mapping) von Variablen einem Datensatz (data) zu (√§sthetischen) Attributen (aes) von geometrischen Objekten (geom).Wir m√ºssen also f√ºr das Visualisieren Folgendes festlegen:data: der Datensatz, der die Variablen enth√§lt, die wir darstellen m√∂chten.data: der Datensatz, der die Variablen enth√§lt, die wir darstellen m√∂chten.aes: (√§sthetische) Attribute f√ºr die geometrischen Objekte, die dargestellt werden sollen. Diese Attribute sind, z.B. die x und y Koordinaten, Farbe, Form und Gr√∂√üe der geometrischen Objekteaes: (√§sthetische) Attribute f√ºr die geometrischen Objekte, die dargestellt werden sollen. Diese Attribute sind, z.B. die x und y Koordinaten, Farbe, Form und Gr√∂√üe der geometrischen Objektegeom: geometrische Objekte, die dargestellt werden sollen, z.B. Punkte, Linien, Boxen, S√§ulen etc.geom: geometrische Objekte, die dargestellt werden sollen, z.B. Punkte, Linien, Boxen, S√§ulen etc.Wir laden zun√§chst die n√∂tigen Bibliotheken und filtern den Datensatz gapminder, um nur die Daten aus dem Jahr 2007 zu visualisieren.","code":"\nlibrary(ggplot2)\nlibrary(gapminder)\n\ngapminder2007 <- gapminder %>% \n  filter(year == 2007)"},{"path":"ggplot.html","id":"punktdiagramm","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.2 Punktdiagramm","text":"Ein typischer Befehl zur Visualisierung w√ºrde also aussehen:Worten k√∂nnte man es vielleicht wie folgt umschreiben:Nimm den Datensatz (data) gapminder undNimm den Datensatz (data) gapminder undordne folgende Attribute zu:\nauf die x-Achse die Variable gdpPercap\nauf die y-Achse die Variable lifeExp\nf√§rbe ein mithilfe der Variablen continent\nbestimme die Gr√∂√üe der Symbole mithilfe der Variablen pop\nordne folgende Attribute zu:auf die x-Achse die Variable gdpPercapauf die y-Achse die Variable lifeExpf√§rbe ein mithilfe der Variablen continentbestimme die Gr√∂√üe der Symbole mithilfe der Variablen popStelle das Ganze als geometrisches Objekte Punkte dar (geom_point())Stelle das Ganze als geometrisches Objekte Punkte dar (geom_point())Sie sehen, dass diese Zuordnungen klar nach einer Legende verlangen, die dann auch automatisch, sowohl f√ºr die Farbe als auch f√ºr die Gr√∂√üe der Symbole, erstellt wird.Die Anweisungen zur Visualisierung ggplot2 werden mit einem + verbunden. Man kann (und diesem Fall soll) weitere Anweisungen geben. Z.B. sind die Beschriftungen der beiden Achsen nichtssagend und m√ºssen verbessert werden. Wir h√§ngen mit einem +-Zeichen weitere Befehle hinzu:","code":"\nggplot(data = gapminder2007, mapping = aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  geom_point()\nggplot(data = gapminder2007, mapping = aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  geom_point() + \n  labs(x = 'Bruttoinlandsprodukt pro Kopf (US$)', y = 'Lebenserwartung (Jahre)',\n       color = 'Kontinent', size = 'Bev√∂lkerung')"},{"path":"ggplot.html","id":"weitere-geoms","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.3 Weitere geoms","text":"Das geom_point() produziert eine xy-Grafik (scatter plot). Weiter wichtige Grafiktypen sindgeom_line(): Liniengeom_histogram(): Histogrammgeom_boxplot(): Boxplotgeom_bar(): S√§ulen","code":""},{"path":"ggplot.html","id":"scatter","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.4 Liniendiagramm","text":"Es ergibt wenig Sinn, die obere Grafik mit Linien darzustellen. Allerdings eignen sich Linien ausgezeichnet, um einen zeitlichen Verlauf zu visualisieren. Daher filtern wir aus dem Datensatz gapminder die Zeitreihen f√ºr Frankreich und Deutschland heraus. Weil wir jetzt zwei L√§nder haben m√∂chten, muss beim Filtern ein Vektor mit L√§ndernamen angegeben werden und statt == der Operator %%. Wir werden sp√§ter noch ausf√ºhrlich auf diese Operatoren zur√ºckkommen.","code":"\nfrance_germany <- gapminder %>% \n  filter(country %in% c('France', 'Germany'))\nggplot(data = france_germany, mapping = aes(x = year, y = gdpPercap, color = country)) +\n  geom_line()"},{"path":"ggplot.html","id":"histogramm","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.5 Histogramm","text":"Wie ist das GDP im Jahre 2007 Afrika und Europa verteilt? Dazu nutzen wir das Histogramm und filtern die Daten vorher entsprechend. Als √Ñsthetik eignet sich hier fill besser als color.","code":"\nafrica_europe <- gapminder2007 %>% \n  filter(continent %in% c('Africa', 'Europe'))\n\nggplot(africa_europe, mapping = aes(x = gdpPercap, fill = continent)) +\n  geom_histogram(bins = 20)"},{"path":"ggplot.html","id":"boxplot","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.6 Boxplot","text":"Wie ist das GDP im Jahre 2007 auf verschiedenen Kontinenten verteilt? Ein Histogramm mit allen Kontinenten w√ºrde schnell sehr un√ºbersichtlich werden. Das geht mit einem Boxplot besser.","code":"\nggplot(gapminder2007, mapping = aes(x = continent, y = gdpPercap)) +\n  geom_boxplot()"},{"path":"ggplot.html","id":"s√§ulendiagramm","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.7 S√§ulendiagramm","text":"Wie viele Eintr√§ge gibt es pro Kontinent? Das S√§ulendiagramm z√§hlt f√ºr uns die Eintr√§ge im Datensatz zusammen","code":"\nggplot(data = gapminder, mapping = aes(x = continent)) +\n  geom_bar()"},{"path":"ggplot.html","id":"lesestoff-4","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.8 Lesestoff","text":"Kapitel 2.1 Ismay Kim (2021)","code":""},{"path":"ggplot.html","id":"aufgaben-3","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.9 Aufgaben","text":"","code":""},{"path":"ggplot.html","id":"grafiken-richtig-beschriften","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.9.1 Grafiken richtig beschriften","text":"Bis auf die Grafik 4.4 fehlen bei den Grafiken oben ordentliche Achsenbeschriftungen und Titel f√ºr die Legenden. Erg√§nzen Sie den Code entsprechend.","code":""},{"path":"ggplot.html","id":"zeitreihen","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.9.2 Zeitreihen","text":"Stellen Sie den zeitlichen Verlauf der Lebenserwartung f√ºnf europ√§ischen L√§ndern Ihrer Wahl dar. F√§rben Sie die Linien nach L√§ndern.","code":""},{"path":"ggplot.html","id":"boxplots","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.9.3 Boxplots","text":"Stellen Sie die Lebenserwartung im Jahr 1952 und im Jahr 2007 pro Kontinent dar. Das sind zwei verschiedene Boxplots.","code":""},{"path":"ggplot.html","id":"ihre-arbeit-einreichen-1","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.10 Ihre Arbeit einreichen","text":"Speichern Sie Ihr Notebook ab und laden Sie nur die .Rmd Datei vom Server.Laden Sie Ihre .Rmd Datei ILIAS hoch. Beachten Sie die Deadline!Sie erhalten die Musterl√∂sung nach dem Hochladen.","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"daten-einlesen-und-visualisieren","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"Kapitel 5 Daten einlesen und visualisieren","text":"\nDaten aus Textdateien R einlesen\n\nGrafiken anpassen (nebeneinander, Facetten, Transparenz)\n\nGrafiken speichern\n","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"daten-aus-textdateien-in-r-einlesen","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.1 Daten aus Textdateien in R einlesen","text":"Um Daten aus Textdateien (z.B. aus .csv, .txt, .dat) R zu importieren (.e.¬†einzulesen) werden wir die Bibliothek readr aus tidyverse benutzen. Wir laden erst einmal tidyverse.Wir gehen davon aus, dass die Daten im Ordner data gespeichert sind. Falls Ihre Daten einem anderen Ort abgelegt sind, m√ºssen Sie den Pfad beim Einlesen entsprechend anpassen.Um die Daten zu laden, gibt es der Bibliothek readr verschiedene Funktionen, die alle mit read_ beginnen. Die allgemeinste davon ist read_delim. Darin kann man explizit einstellen, mit welchem Zeichen (z.B. Komma, Strichpunkt etc.) die einzlenen Spalten der zu importierenden Datei getrennt sind.Ein kurzer Blick auf den Datensatz. Hierbei handelt es sich um Daten zu Treibhausgasemissionen auf der EU-Ebene, die ich bei eurostat 30.4.2021 heruntergeladen und vorgefiltert habe. Die Datenbank bietet sehr viele Datens√§zte und ist als Quelle f√ºr Berichte hervorragend geeignet üòÑ.Das Ergebnis des Einlesens mit read_ Funktionen ist immer ein tibble. Kategorische Variablen werden als Text (character) eingelesen und nicht factor umgewandelt. Wenn man factor m√∂chte, muss man die Variablen per Hand umwandeln.Wir verschaffen uns einen kurzen √úberblick √ºber die Daten.Um die Anzahl der einzelnen L√§nder zu ermitteln, sehen wir uns die L√§nge der Ausgabe der Funktion unique() , die die einzelnen verschiedenen Eintr√§ge ermitteln kann. Es sind Eintr√§ge f√ºr 33 verschiedene L√§nder vorhanden.","code":"\nlibrary(tidyverse)\nemissions <- read_delim(file = 'data/emissions.csv', delim = ';')## Rows: 2871 Columns: 6\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \";\"\n## chr  (4): unit, airpol, vehicle, geo\n## dbl  (1): values\n## date (1): time\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nemissions## # A tibble: 2,871 √ó 6\n##    unit           airpol                         vehicle geo   time       values\n##    <chr>          <chr>                          <chr>   <chr> <date>      <dbl>\n##  1 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Aust‚Ä¶ 2018-01-01  14.4 \n##  2 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Belg‚Ä¶ 2018-01-01  14.4 \n##  3 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Bulg‚Ä¶ 2018-01-01   5.78\n##  4 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Swit‚Ä¶ 2018-01-01  11.0 \n##  5 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Cypr‚Ä¶ 2018-01-01   1.38\n##  6 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Czec‚Ä¶ 2018-01-01  11.9 \n##  7 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Germ‚Ä¶ 2018-01-01  97.8 \n##  8 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Denm‚Ä¶ 2018-01-01   6.85\n##  9 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Esto‚Ä¶ 2018-01-01   1.52\n## 10 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Gree‚Ä¶ 2018-01-01   7.61\n## # ‚Ä¶ with 2,861 more rows\nsummary(emissions)##      unit              airpol            vehicle              geo           \n##  Length:2871        Length:2871        Length:2871        Length:2871       \n##  Class :character   Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n##                                                                             \n##                                                                             \n##                                                                             \n##                                                                             \n##       time                values         \n##  Min.   :1990-01-01   Min.   :  0.00609  \n##  1st Qu.:1997-01-01   1st Qu.:  0.25564  \n##  Median :2004-01-01   Median :  1.92403  \n##  Mean   :2004-01-01   Mean   :  8.52836  \n##  3rd Qu.:2011-01-01   3rd Qu.:  6.93899  \n##  Max.   :2018-01-01   Max.   :119.77824  \n##                       NA's   :232\nlength(unique(emissions$geo))## [1] 33"},{"path":"daten-einlesen-und-visualisieren.html","id":"legende-verschieben-und-facetten","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.2 Legende verschieben und Facetten","text":"Wir stellen die Zeitreihen der Emissionen eingef√§rbt nach Land dar. Die L√§nder stehen der Variablen geo.Als erstes f√§llt auf, dass die Legende sehr umfangreich ist (wir haben ja 33 L√§nder im Datensatz). Daher w√§re es g√ºnstig, die Legende unterhalb der Grafik zu positionieren und den Titel der Legende oberhalb der Legende zu belassen. Das geht mit Hilfe der Funktionen theme() und guides(). Wie immer, werden sie im Plotaufbau (denken Sie grammer graphics) mit + angeh√§ngt.Die Zeitreihen sehen echt seltsam aus. Wenn wir uns die Variable vehicle ansehen, wird auch klar, warum. Wir stellen gerade Emissionen f√ºr verschiedene Fahrzeuge dar, d.h. wir mischen mehrere Zeitreihen zusammen.Die einfachste L√∂sung ist, drei verschiedene Grafiken pro Verkehrsmittel zu erstellen. Dies gelingt sehr leicht mit der Funktion facet_wrap(), die den Namen der Variablen erwartet, mit Hilfe derer die Grafiken gesplittet werden sollen. Vor der Variablen muss eine Tilde (~) stehen. unserem Fall wollen wir nach Verkehrsmittel splitten, d.h. mit Hilfe der Varialben vehicle.Da die Emissionen sehr unterschiedlich sind, macht es Sinn, die Skalierungen der y-Achsen anzupassen. Aber Achtung: Das sollten Sie Ihren Berichten unbedingt ansprechen (z.B. der Bildunterschrift), da man unterschiedliche Skalierunge sehr leicht √ºbersieht und dann die Interpretation der Daten leicht die falsche Richtung gehen kann. Der Funktionsparameter labeller = label_wrap_gen() sorgt f√ºr geschickte Zeilenumbr√ºche bei zu langen Labels. Zum Vergleich k√∂nnen Sie ihn mal weglassen und sehen, dann passiert.","code":"\nggplot(data = emissions, mapping = aes(x = time, y = values, colour = geo)) +\n  geom_line()## Warning: Removed 7 row(s) containing missing values (geom_path).\nggplot(data = emissions, mapping = aes(x = time, y = values, colour = geo)) +\n  geom_line() +\n  theme(legend.position = \"bottom\") +\n  guides(colour = guide_legend(title.position = \"top\"))## Warning: Removed 7 row(s) containing missing values (geom_path).\nunique(emissions$vehicle)## [1] \"Fuel combustion in cars\"                       \n## [2] \"Fuel combustion in heavy duty trucks and buses\"\n## [3] \"Fuel combustion in railways\"\nggplot(data = emissions, mapping = aes(x = time, y = values, colour = geo)) +\n  geom_line() +\n  facet_wrap(~vehicle) +\n  theme(legend.position = \"bottom\") +\n  guides(colour = guide_legend(title.position = \"top\"))## Warning: Removed 203 row(s) containing missing values (geom_path).\nggplot(data = emissions, mapping = aes(x = time, y = values, colour = geo)) +\n  geom_line() +\n  facet_wrap(~vehicle, scales = 'free_y', labeller = label_wrap_gen()) +\n  theme(legend.position = \"bottom\") +\n  guides(colour = guide_legend(title.position = \"top\"))## Warning: Removed 203 row(s) containing missing values (geom_path)."},{"path":"daten-einlesen-und-visualisieren.html","id":"fehlerbalken-und-co.","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.3 Fehlerbalken und Co.","text":"Um die Variabilit√§t der Daten grafisch darzustellen, bieten sich Fehlerbalken, Bereiche etc. . Daf√ºr hat ggplot2 spezielle geoms. Hier ein Beispiel inspiriert vom dem Buch ggplot2 (Wickham 2020).Sie sehen, dass man ggplot Objekte wie andere Objekte R zuweisen kann, um mit ihnen sp√§ter zu arbeiten. diesem Fall ist basis_plot ein ggplot Objekt.","code":"\ny <- c(10, 5, 23)\ndf <- tibble(x = 1:3, y = y, se = c(0.9, 1.5, 3.3))\n\nbasis_plot <- ggplot(df, aes(x, y, ymin = y - se, ymax = y + se))\nbasis_plot + geom_pointrange()\nbasis_plot + geom_errorbar()\nclass(basis_plot)## [1] \"gg\"     \"ggplot\""},{"path":"daten-einlesen-und-visualisieren.html","id":"mehrere-grafiken-nebeneinander","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.4 Mehrere Grafiken nebeneinander","text":"Um mehrere Grafiken nebeneinander zu plotten, nutzen wir die Bibliothek patchwork. Sie erlaubt verschieden Layouts. Details finden Sie hier. Die einfachsten Varianten sind zwei Grafiken nebeneinander. Daf√ºr erstellen wir zwei ggplot-Objekte und verbinden sie mit einem +.Um die Grafiken untereinander abzubilden, nutzen wir das Zeichen /.","code":"\nlibrary(patchwork)\n\np1 <- basis_plot + geom_pointrange()\np2 <- basis_plot + geom_errorbar()\n\np1 + p2\np1 / p2"},{"path":"daten-einlesen-und-visualisieren.html","id":"grafiken-abspeichern","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.5 Grafiken abspeichern","text":"Die Funktion ggsave() speichert ggplot Grafiken ab. Wenn nicht anders angegeben wird, wird der letzte aktuelle Plot gespeichert.Wir k√∂nnen aber auch explizit ein ggplot Objekt zum Speichern benennen.","code":"\nggsave(filename = 'Fehlerbalken.pdf', device = 'pdf', width = 7, height = 5)\nalles <- p1 + p2\nggsave(filename = 'Fehlerbalken_2.pdf', device = 'pdf', plot = alles, width = 7, height = 5)"},{"path":"daten-einlesen-und-visualisieren.html","id":"weitere-statistsiche-zusammenfassungen-in-grafiken","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.6 Weitere statistsiche Zusammenfassungen in Grafiken","text":"Arbeiten Sie selbst√§ndig das Kapitel 5: Statistical summaries Wickham (2020) (https://ggplot2-book.org/statistical-summaries.html).","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"lesestoff-5","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.7 Lesestoff","text":"Kapitel 2.2 bis 2.9 Ismay Kim (2021)","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"aufgaben-4","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.8 Aufgaben","text":"","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"grafiken-richtig-beschriften-1","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.8.1 Grafiken richtig beschriften","text":"Beschriften Sie die finale Grafik der Zeitreihen (Achsen, Titel, Legende).","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"bestandesaufnahme","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.8.2 Bestandesaufnahme im Wald","text":"Ar Stat arbeitet als HiWi der AG √ñkosystemforschung und soll im Nationalpark Eifel eine Bestandsaufnahme durchf√ºhren (d.h. Baumh√∂hen und -durchmesser vermessen). Er notiert den BHD (Brusth√∂hendurchmesser) und die Art der B√§ume.Lesen Sie den Datensatz BHD.txt ein und ordnen Sie ihn der Variable BHD zu.Erstellen Sie einen Vektor Nr mit durchlaufenden Baumnummern. Von welcher Art sind die Elemente des Vektors Nr? Tipp: Verwenden Sie das Zeichen :, um durchlaufende Nummern zu erzeugen.F√ºgen Sie die Datens√§tze BHD und Nr zu einem tibble zusammen und benennen Sie die Spalten dabei sinnvoll.L√∂schen Sie den Vektor Nr. Tipp: Hilfe zu Funktion rm() lesen.Lesen Sie den Datensatz Art.txt ein und ordnen Sie ihn der Variablen art zu.F√ºgen Sie die Art das tibble ein. Tipp: tibble(altes_tibble, neue_spalte = XY).Erstellen Sie eine Tabelle mit der Anzahl der jeweiligen Arten. Nutzen Sie die Funktion table().Speichern Sie die Tabelle mit write_delim() ab. Schlagen Sie der Hilfe nach, wie diese Funktion arbeitet! Tipp: Vorher muss der Datensatz zu einem data.frame umgewandelt werden. Nutzen Sie die funktion .data.frame().","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"wahlbeteiligung","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.8.3 Wahlbeteiligung bei der Bundestagswahl 2017","text":"Bauen Sie die Grafik nach (Abbildung 5.1).\nAbbildung 5.1: Wahlbeteiligung bei den Bundestagswahlen. Quelle: Der Bundeswahlleiter.\nLesen Sie den Datensatz Wahlbeteiligung.csv R ein und ordnen Sie ihn dem Objekt beteiligung zu.Sehen Sie sich den Datensatz und fassen Sie ihn zusammen.Stellen Sie die Wahlbeteiligung als Funktion der Zeit dar, wie Abbildung 5.1 gezeigt.Beschriften Sie die Grafik.Speichern Sie die Grafik als pdf ab.","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"zweitstimme","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.8.4 Zweitstimme bei der Bundestagswahl 2017","text":"Bauen Sie die Grafik nach (Abbildung 5.2).\nAbbildung 5.2: Zweitstimme bei der Bundestagswahl 2017. Quelle: Der Bundeswahlleiter.\nLesen Sie den Datensatz Zweitstimme.csv R ein und ordnen Sie ihn dem Objekt zweitstimme zu.Sehen Sie sich den Datensatz und fassen Sie ihn zusammen.Stellen Sie die die Zweitstimmen pro Partei einem S√§ulendiagramm dar. Nutzen Sie das geom geom_col() und lesen Sie den Unterschied zu geom_bar() der Hilfe nach. Tipps:\nDer Variablenname Zweitstimme 2017 enth√§lt ein Leerzeichen. Daher m√ºssen Sie es beim Aufruf zu ggplot unbedingt ‚Äú`‚Äù setzten.\nDamit die Parteien der selben Reihenfolge dargestellt werden, wie im Datensatz angegeben, wandeln Sie die Spalte Partei ein factor um: zweitstimme$Partei <- as_factor(zweitstimme$Partei).\nFarben stellen Sie direkt geom_col() ein mit fill = c('black', 'red', 'magenta', 'darkgreen', 'yellow', 'blue', 'grey')\nDer Variablenname Zweitstimme 2017 enth√§lt ein Leerzeichen. Daher m√ºssen Sie es beim Aufruf zu ggplot unbedingt ‚Äú`‚Äù setzten.Damit die Parteien der selben Reihenfolge dargestellt werden, wie im Datensatz angegeben, wandeln Sie die Spalte Partei ein factor um: zweitstimme$Partei <- as_factor(zweitstimme$Partei).Farben stellen Sie direkt geom_col() ein mit fill = c('black', 'red', 'magenta', 'darkgreen', 'yellow', 'blue', 'grey')Beschriften Sie die Grafik.Speichern Sie die Grafik als pdf ab.","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"zweigrafiken","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.8.5 Ergebnisse der Bundestagswahl in einer Grafik","text":"Stellen Sie beide Grafiken untereinander dar wie Abbildung (5.3) gezeigt.\nAbbildung 5.3: Ergebnisse der Bundestagswahl 2017. Quelle: Der Bundeswahlleiter.\n","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"ihre-arbeit-einreichen-2","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.9 Ihre Arbeit einreichen","text":"Speichern Sie Ihr Notebook ab und laden Sie nur die .Rmd Datei vom Server.Laden Sie Ihre .Rmd Datei ILIAS hoch. Beachten Sie die Deadline!Sie erhalten die Musterl√∂sung nach dem Hochladen.","code":""},{"path":"explorative-datenanalyse.html","id":"explorative-datenanalyse","chapter":"Kapitel 6 Explorative Datenanalyse mit tidyverse","heading":"Kapitel 6 Explorative Datenanalyse mit tidyverse","text":"\nKernpakete aus tidyverse benennen\n\nein einfaches Workflow (Daten einlesen, zusammenfassen, darstellen)\nmit tidyverse durchf√ºhren\n\nFunktionen des Pakets dplyr f√ºr Datentransformation\nanwenden\ntidyverse ist eine Sammlung von R-Pakete, die explizit f√ºr Datenanalyse entwickelt wurden (https://www.tidyverse.org/). tidyverse versucht durch gemeinsame Philosophie Design, Grammatik und Datenstruktur die Datenanalyse zu erleichtern (https://design.tidyverse.org/). Auch wenn tidyverse auf den ersten Blick etwas fremd erscheint, es ist ein Teil von R, kein eigenes Universum. Es ist also v√∂llig Ordnung, R-Basisfunktionen mit Funktionen aus tidyverse zu mischen.Das wichtigste Einf√ºhrungsbuch zu tidyverse ist sicherlich R4DS: ‚ÄúR Data Science‚Äù (Wickham Grolemund 2021), das Sie kostenlos online lesen k√∂nnen (https://r4ds..co.nz/).","code":""},{"path":"explorative-datenanalyse.html","id":"grundpakete","chapter":"Kapitel 6 Explorative Datenanalyse mit tidyverse","heading":"6.1 Grundpakete","text":"tidyverse enth√§lt folgende Grundpakete, die alle installiert werden, wenn Sie install.packages('tidyverse') ausf√ºhren.Jedes dieser Pakete hat ein Cheat Sheet, eine √ºbersichtliche Zusammenstellung der Funktionen des Pakets. Sie bekommen die Cheet Sheats √ºber die tidyverse-Seite (https://www.tidyverse.org/packages/), indem Sie auf das jeweilige Paket klicken und zum Abschnitt ‚ÄòCheatsheet‚Äô scrollen.","code":""},{"path":"explorative-datenanalyse.html","id":"der-explorative-workflow","chapter":"Kapitel 6 Explorative Datenanalyse mit tidyverse","heading":"6.2 Der explorative Workflow","text":"","code":""},{"path":"explorative-datenanalyse.html","id":"daten-einlesen-revisited","chapter":"Kapitel 6 Explorative Datenanalyse mit tidyverse","heading":"6.2.1 Daten einlesen, revisited","text":"Als Erstes laden wir die Bibliothek tidyverse.Sie kennen bereits die Funktion read_delim() zum Einlesen von Textdateien. Die Funktion ist die allgemeinste Funktion der read_* Familie aus readr tidyverse; read_csv() und read_csv2() sind jeweils f√ºr komma- und strichpunkt-getrennte Datens√§tze gedacht. der Basisinstallation von R (also au√üerhalb von tidyverse) gibt die sehr umfangreiche Funktion read.table(), die ebenfalls zum Einlesen von Textdateien verwendet wird. Man k√∂nnte berechtigterweise fragen, warum neue Funktion (read_*) f√ºr etwas erfinden, es schon gibt. Die Autoren von tidyverse versprechen Konsistenz und Geschwindigkeit. Ersteres war schon immer ein Problem von R, da es nicht von Computerspezialisten, sondern von Anwendern erfunden wurde. Daher ist eine Vereinheitlichung durch tidyverse mehr als willkommen. Und Geschwindigkeit ist sp√§testens bei gr√∂√üeren Datens√§tzen ein wichtiger Punkt.Wir sehen uns Daten des Deutschen Wetterdienstes , die ich 24. Mai 2020 heruntergeladen habe (https://www.dwd.de/DE/leistungen/klimadatendeutschland/klimadatendeutschland.html). Auch das ist eine tolle Datenquelle f√ºr Berichte üòÑ. Der Datensatz enth√§lt Stundenwerte f√ºr relative Luftfeuchte (%) und Lufttemperatur (¬∞C) von drei Wetterstationen, n√§mlich Hof, Frankfurt und K√∂ln-Bonn. Die Daten sind der Datei Drei_Stationen.csv gespeichert.Beim Einlesen zeigt Ihnen read_delim() bereits, welche Spalten und welche Datentypen es erkennt, mit trim_ws = T werden Leerzeichen aus Spalten entfernt.Eine weitere Kontrolle bietet die Funktion print(), die das eingelesene Ergebnis √ºbersichtlich (und im Notebook interaktiv) darstellt. Sie m√ºssen hier nicht head() verwenden, da grunds√§tzlich nur die ersten 10 Zeilen dargestellt werden.Das gleiche Ergebnis bekommen Sie auch ohne print(), wenn Sie wie gewohnt den Namen des Objekts tippen.diesem Datensatz sind folgende Variablen (Spalten) enthalten (s. Datensatzbeschreibung des DWDs)Das Objekt temp_humid ist ein tibble, ein data.frame mit ‚Äúmodernem‚Äù Verhalten. Z.B. gibt die Funktion print() nur die ersten 10 Zeilen aus, die Datentypen den Spalten werden hellgrau zwischen ‚Äò<>‚Äô mit angegeben etc. Mehr zu Tibbles finden Sie Kapitel 10 ‚ÄúTibbles‚Äù R4DS.","code":"\nlibrary(tidyverse)\ntemp_humid <- read_delim('data/Drei_Stationen.csv', delim = ';',    trim_ws = T)## Rows: 39600 Columns: 6\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \";\"\n## chr (1): eor\n## dbl (5): STATIONS_ID, MESS_DATUM, QN_9, TT_TU, RF_TU\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nprint(temp_humid)## # A tibble: 39,600 √ó 6\n##    STATIONS_ID MESS_DATUM  QN_9 TT_TU RF_TU eor  \n##          <dbl>      <dbl> <dbl> <dbl> <dbl> <chr>\n##  1        2261 2018111900     3  -2.8    99 eor  \n##  2        2261 2018111901     3  -2.5   100 eor  \n##  3        2261 2018111902     3  -2.3   100 eor  \n##  4        2261 2018111903     3  -2     100 eor  \n##  5        2261 2018111904     3  -1.9    99 eor  \n##  6        2261 2018111905     3  -2.1    99 eor  \n##  7        2261 2018111906     3  -1.8    99 eor  \n##  8        2261 2018111907     3  -1.5    99 eor  \n##  9        2261 2018111908     3  -1.1    99 eor  \n## 10        2261 2018111909     3  -0.6    97 eor  \n## # ‚Ä¶ with 39,590 more rows\ntemp_humid## # A tibble: 39,600 √ó 6\n##    STATIONS_ID MESS_DATUM  QN_9 TT_TU RF_TU eor  \n##          <dbl>      <dbl> <dbl> <dbl> <dbl> <chr>\n##  1        2261 2018111900     3  -2.8    99 eor  \n##  2        2261 2018111901     3  -2.5   100 eor  \n##  3        2261 2018111902     3  -2.3   100 eor  \n##  4        2261 2018111903     3  -2     100 eor  \n##  5        2261 2018111904     3  -1.9    99 eor  \n##  6        2261 2018111905     3  -2.1    99 eor  \n##  7        2261 2018111906     3  -1.8    99 eor  \n##  8        2261 2018111907     3  -1.5    99 eor  \n##  9        2261 2018111908     3  -1.1    99 eor  \n## 10        2261 2018111909     3  -0.6    97 eor  \n## # ‚Ä¶ with 39,590 more rows\nclass(temp_humid)## [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\""},{"path":"explorative-datenanalyse.html","id":"geschickter-umgang-mit-zeit-und-datum","chapter":"Kapitel 6 Explorative Datenanalyse mit tidyverse","heading":"6.3 Geschickter Umgang mit Zeit und Datum","text":"Ein weiteres Paket, das zwar nicht zum Kern von tidyverse geh√∂rt, jedoch trotzdem extrem n√ºtzlich ist, hei√üt lubridate. Es hilft, Text sehr einfach richtige Datums-Objekte zu transformieren (Base-R muss man sich daf√ºr kryptischen Datumsformate merken). Wir transformieren die Spalte temp_humid$MESS_DATUM ein richtiges Datum mit Uhrzeit. Die Funktion ymd_h() kann character ein richtiges Datumsformat transformieren, wenn das Datum als year, month, day, hour codiert ist. Es gibt noch weitere Varianten der Codierung, die Sie bei Bedarf der Hilfe nachschlagen sollten.","code":"\nlibrary(lubridate)\n\ntemp_humid$MESS_DATUM <- ymd_h(temp_humid$MESS_DATUM)\n\ntemp_humid## # A tibble: 39,600 √ó 6\n##    STATIONS_ID MESS_DATUM           QN_9 TT_TU RF_TU eor  \n##          <dbl> <dttm>              <dbl> <dbl> <dbl> <chr>\n##  1        2261 2018-11-19 00:00:00     3  -2.8    99 eor  \n##  2        2261 2018-11-19 01:00:00     3  -2.5   100 eor  \n##  3        2261 2018-11-19 02:00:00     3  -2.3   100 eor  \n##  4        2261 2018-11-19 03:00:00     3  -2     100 eor  \n##  5        2261 2018-11-19 04:00:00     3  -1.9    99 eor  \n##  6        2261 2018-11-19 05:00:00     3  -2.1    99 eor  \n##  7        2261 2018-11-19 06:00:00     3  -1.8    99 eor  \n##  8        2261 2018-11-19 07:00:00     3  -1.5    99 eor  \n##  9        2261 2018-11-19 08:00:00     3  -1.1    99 eor  \n## 10        2261 2018-11-19 09:00:00     3  -0.6    97 eor  \n## # ‚Ä¶ with 39,590 more rows"},{"path":"explorative-datenanalyse.html","id":"daten-zusammenfassen","chapter":"Kapitel 6 Explorative Datenanalyse mit tidyverse","heading":"6.3.1 Daten zusammenfassen","text":"Die drei Wetterstationen haben folgende IDs:Wir z√§hlen nach, wie viele Messpunkte es pro Station gibt. Dazu m√ºssen wir den Datensatz nach der Variablen STATION_ID gruppieren und dann pro Gruppe die Anzahl der Datenpunkte ermitteln:Die Zeichenkombination %>% hei√üt Pipe-Operator (pipe) und wird als ‚Äòund dann‚Äô gelesen (). Der Ausdruck temp_humid %>% group_by(STATIONS_ID) %>% count() hei√üt also: nimm das Objekt temp_humid, gruppiere es nach der Variablen STATION_ID und dann z√§hle die Eintr√§ge pro Gruppe zusammen. Der Pipe-Operator ist die Kernphilosophie von tidyverse und wird Ihnen √ºberall begegnen. Der Operator stammt aus dem Paket magrittr (https://magrittr.tidyverse.org/). Seine Hauptaufgabe ist es, den Code √ºbersichtlicher und besser lesbar zu machen (vielleicht nicht gleich zu Beginn der Lernkurve, aber schon bald üòé).","code":"\nstation_ids <-  c('2261' = 'Hof', '1420' = 'Frankfurt', '2667' = 'Koeln')\ntemp_humid %>% \n  group_by(STATIONS_ID) %>% \n  count()## # A tibble: 3 √ó 2\n## # Groups:   STATIONS_ID [3]\n##   STATIONS_ID     n\n##         <dbl> <int>\n## 1        1420 13200\n## 2        2261 13200\n## 3        2667 13200"},{"path":"explorative-datenanalyse.html","id":"die-grammatik-der-datenmanipulation-dplyr","chapter":"Kapitel 6 Explorative Datenanalyse mit tidyverse","heading":"6.4 Die Grammatik der Datenmanipulation ‚Äì dplyr","text":"Die Funktion count() geh√∂rt zum Paket dplyr, das f√ºr Datentransformationen zust√§ndig ist. Es ist mal wieder eine Grammatik. Dieses Paket enth√§lt 5 Grundfunktionen (alle nach Verben benannt, damit man gleich wei√ü, frau tut üòÑ):Wir m√∂chten nur von einer bestimmten Station die Anzahl der Messwerte wissen m√∂chten, dann filtern wir vorher.Beim Filtern l√§uft eine logische Abfrage. D.h. es wird bei jeden Eintrag STATION_ID nachgesehen, ob da der Wert 2667 steht. Wenn da 2667 steht, dann gibt == ein TRUE zur√ºck, wenn da etwas anderes steht, dann gibt == ein FALSE zur√ºck. Und die Funktion filter() beh√§lt nur die Zeilen, bei denen == ein TRUE zur√ºckgegeben hat.Weiter wichtige logische und relationale Operatoren finden Sie hier der Hilfe zu filter(). Hier ein paar einfache BeispieleZudem kann man bei filter() die Anfragen auch kombinieren. Wir wollen z.B. die Stationen K√∂ln und Hof haben. | ist der logische Operator oder. Wenn man also sowohl K√∂ln als auch Hof haben , sagt man: finde alles, entweder gleich K√∂ln oder gleich Hof ist.Das Gleiche erreicht man mit folgendem Code, indem man Frankfurt ausschlie√üt:Alternative kann man auch den Operator %% verwenden. Dieser ist sehr n√ºtzlich, wenn man anhand einer einzelnen Variablen filtert, aber unterschiedliche Eintr√§ge ausw√§hlen m√∂chte (z.B. zwei Messstationen). Es wird bei jeder Zeile der Variablen STATIONS_ID nun √ºberpr√ºft, ob hier entweder 2667 oder 2261 stehen.","code":"\ntemp_humid %>% \n  filter(STATIONS_ID == '2667') %>%\n  count()## # A tibble: 1 √ó 1\n##       n\n##   <int>\n## 1 13200\ntemp_humid %>% \n  filter(STATIONS_ID == '2667' | STATIONS_ID == '2261') %>%\n  group_by(STATIONS_ID) %>% \n  count()## # A tibble: 2 √ó 2\n## # Groups:   STATIONS_ID [2]\n##   STATIONS_ID     n\n##         <dbl> <int>\n## 1        2261 13200\n## 2        2667 13200\ntemp_humid %>% \n  filter(STATIONS_ID != '1420') %>%\n  group_by(STATIONS_ID) %>% \n  count()## # A tibble: 2 √ó 2\n## # Groups:   STATIONS_ID [2]\n##   STATIONS_ID     n\n##         <dbl> <int>\n## 1        2261 13200\n## 2        2667 13200\ntemp_humid %>% \n  filter(STATIONS_ID %in% c('2667', '2261')) %>%\n  group_by(STATIONS_ID) %>% \n  count()## # A tibble: 2 √ó 2\n## # Groups:   STATIONS_ID [2]\n##   STATIONS_ID     n\n##         <dbl> <int>\n## 1        2261 13200\n## 2        2667 13200"},{"path":"explorative-datenanalyse.html","id":"daten-plotten","chapter":"Kapitel 6 Explorative Datenanalyse mit tidyverse","heading":"6.4.1 Daten plotten","text":"Wir sehen uns die Daten erst mal , bevor wir weiter machen. Wir plotten die Temperatur. Weil es sich um Zeitreihen handelt, m√∂chten wir sie eher untereinander als nebeneinander haben. Daher setzen wir bei facet_wrap() den Parameter nrow = 3.","code":"\nggplot(data = temp_humid, aes(x = MESS_DATUM, y = TT_TU)) + \n  geom_line() +\n  facet_wrap(~STATIONS_ID, nrow = 3) +\n  labs(x = 'Zeit', y = 'Temperatur (¬∞C)')"},{"path":"explorative-datenanalyse.html","id":"neue-variablen-erstellen-mit-mutate","chapter":"Kapitel 6 Explorative Datenanalyse mit tidyverse","heading":"6.4.2 Neue Variablen erstellen mit mutate()","text":"Wir wollen die Monatsmittelwerte und die Standardabweichungen f√ºr die Temperatur berechnen und diese darstellen. Als Erstes erstellen wir zwei neue Spalten, die jeweils das Jahr und den Monat beinhalten. Die beiden neuen Spalten werden Ende von temp_humid angeh√§ngt. Um neue Spalten zu erstellen, nutzen wir die Funktion mutate(). Die Funktionen year()und month() geh√∂ren zur Bibliothek lubridate und extrahieren jeweils das Jahr und den Monat aus MESS_DATUM.Jetzt k√∂nnen wir einen neuen Datensatz mit den Mittelwerten erstellen. Daf√ºr gruppieren wir erst einmal die Daten nach STATIONS_ID, year und month. Die Mittelwerte sollen ja je Station, Jahr und Monat berechnet werden. Beim Gruppieren gibt man die Variablen ohne Anf√ºhrungszeichen und ohne einen Vektor zu bilden, einfach durch Kommas getrennt .Die Struktur von monthly_means zeigt uns, dass es sich um gruppierte Daten handelt.Da wir aber mit den Daten weiter rechnen wollen, ist es besser, die Gruppierung wieder aufzugeben. Es k√∂nnte sonst sp√§ter Fehlermeldungen geben.Um die Daten als Zeitreihen zu plotten, erstellen wir noch eine ordentliche Zeit-Spalte. Die Funktion parse_date_time() kann aus character richtige Datums-Zeitobjekte erstellen. Sie ist allgemeiner als die oben verwendete ymd_h() Funktion, da man hier das Format explizit angeben kann. unserem Fall ist das Format ‚Äòym‚Äô f√ºr Jahr und Monat.Der Code paste0(year, month) ‚Äúklebt‚Äù die Daten der Variablen year und month zusammen. Das ist n√∂tig, da die Funktion parse_date_time() einen Charaktervektor als Input erwartet und keine zwei getrennten Spalten. Da das Datum au√üer dem Jahr und dem Monat noch einen Tag braucht, hat parse_date_time() den Ersten eines jeden Monats genommen.Alternativ k√∂nnen wir die Mittelwerte mit den Standardabweichungen darstellen.Oder, weil es gerade Spa√ü macht, als halb-transparentes Band. Ich hoffe, Sie haben jetzt Lust, das Kapitel 5 im ggplot2 Buch zu lesen üòé.Ein letzter Trick. Die √úberschriften f√ºr die Teilgrafiken sind ungeschickt, da man die IDs als Mensch einfach nicht zuordnen kann. Weiter oben haben wir einen benannten Vektor definiert, der die Klarnamen enth√§lt.Diesen Vektor nutzen wir als Titel.","code":"\ntemp_humid <- temp_humid %>% \n  mutate(year = year(MESS_DATUM),\n         month = month(MESS_DATUM))\n\ntemp_humid## # A tibble: 39,600 √ó 8\n##    STATIONS_ID MESS_DATUM           QN_9 TT_TU RF_TU eor    year month\n##          <dbl> <dttm>              <dbl> <dbl> <dbl> <chr> <dbl> <dbl>\n##  1        2261 2018-11-19 00:00:00     3  -2.8    99 eor    2018    11\n##  2        2261 2018-11-19 01:00:00     3  -2.5   100 eor    2018    11\n##  3        2261 2018-11-19 02:00:00     3  -2.3   100 eor    2018    11\n##  4        2261 2018-11-19 03:00:00     3  -2     100 eor    2018    11\n##  5        2261 2018-11-19 04:00:00     3  -1.9    99 eor    2018    11\n##  6        2261 2018-11-19 05:00:00     3  -2.1    99 eor    2018    11\n##  7        2261 2018-11-19 06:00:00     3  -1.8    99 eor    2018    11\n##  8        2261 2018-11-19 07:00:00     3  -1.5    99 eor    2018    11\n##  9        2261 2018-11-19 08:00:00     3  -1.1    99 eor    2018    11\n## 10        2261 2018-11-19 09:00:00     3  -0.6    97 eor    2018    11\n## # ‚Ä¶ with 39,590 more rows\nmonthly_means <- temp_humid %>%\n  group_by(STATIONS_ID, year, month) %>% \n  summarize(mean_T = mean(TT_TU), mean_RH = mean(RF_TU),\n            sd_T = sd(TT_TU), sd_RH = sd(RF_TU))## `summarise()` has grouped output by 'STATIONS_ID', 'year'. You can override\n## using the `.groups` argument.\nmonthly_means## # A tibble: 57 √ó 7\n## # Groups:   STATIONS_ID, year [9]\n##    STATIONS_ID  year month mean_T mean_RH  sd_T sd_RH\n##          <dbl> <dbl> <dbl>  <dbl>   <dbl> <dbl> <dbl>\n##  1        1420  2018    11   4.00    79.7  1.82  9.96\n##  2        1420  2018    12   4.73    83.7  4.20 11.7 \n##  3        1420  2019     1   2.12    79.3  3.76 10.0 \n##  4        1420  2019     2   4.48    74.1  4.69 17.7 \n##  5        1420  2019     3   8.28    68.5  4.08 16.1 \n##  6        1420  2019     4  11.7     61.0  5.52 21.8 \n##  7        1420  2019     5  12.7     67.5  4.64 20.1 \n##  8        1420  2019     6  21.4     60.6  6.05 21.2 \n##  9        1420  2019     7  21.6     55.6  5.90 21.8 \n## 10        1420  2019     8  20.7     65.6  4.94 20.8 \n## # ‚Ä¶ with 47 more rows\nstr(monthly_means)## grouped_df [57 √ó 7] (S3: grouped_df/tbl_df/tbl/data.frame)\n##  $ STATIONS_ID: num [1:57] 1420 1420 1420 1420 1420 1420 1420 1420 1420 1420 ...\n##  $ year       : num [1:57] 2018 2018 2019 2019 2019 ...\n##  $ month      : num [1:57] 11 12 1 2 3 4 5 6 7 8 ...\n##  $ mean_T     : num [1:57] 4 4.73 2.12 4.48 8.28 ...\n##  $ mean_RH    : num [1:57] 79.7 83.7 79.3 74.1 68.5 ...\n##  $ sd_T       : num [1:57] 1.82 4.2 3.76 4.69 4.08 ...\n##  $ sd_RH      : num [1:57] 9.96 11.68 10.04 17.73 16.1 ...\n##  - attr(*, \"groups\")= tibble [9 √ó 3] (S3: tbl_df/tbl/data.frame)\n##   ..$ STATIONS_ID: num [1:9] 1420 1420 1420 2261 2261 ...\n##   ..$ year       : num [1:9] 2018 2019 2020 2018 2019 ...\n##   ..$ .rows      : list<int> [1:9] \n##   .. ..$ : int [1:2] 1 2\n##   .. ..$ : int [1:12] 3 4 5 6 7 8 9 10 11 12 ...\n##   .. ..$ : int [1:5] 15 16 17 18 19\n##   .. ..$ : int [1:2] 20 21\n##   .. ..$ : int [1:12] 22 23 24 25 26 27 28 29 30 31 ...\n##   .. ..$ : int [1:5] 34 35 36 37 38\n##   .. ..$ : int [1:2] 39 40\n##   .. ..$ : int [1:12] 41 42 43 44 45 46 47 48 49 50 ...\n##   .. ..$ : int [1:5] 53 54 55 56 57\n##   .. ..@ ptype: int(0) \n##   ..- attr(*, \".drop\")= logi TRUE\nmonthly_means <- ungroup(monthly_means)\nmonthly_means <- monthly_means %>%\n  mutate(year_month = parse_date_time(paste0(year, month), orders = 'ym', tz = 'CET'))\n\nmonthly_means## # A tibble: 57 √ó 8\n##    STATIONS_ID  year month mean_T mean_RH  sd_T sd_RH year_month         \n##          <dbl> <dbl> <dbl>  <dbl>   <dbl> <dbl> <dbl> <dttm>             \n##  1        1420  2018    11   4.00    79.7  1.82  9.96 2018-11-01 00:00:00\n##  2        1420  2018    12   4.73    83.7  4.20 11.7  2018-12-01 00:00:00\n##  3        1420  2019     1   2.12    79.3  3.76 10.0  2019-01-01 00:00:00\n##  4        1420  2019     2   4.48    74.1  4.69 17.7  2019-02-01 00:00:00\n##  5        1420  2019     3   8.28    68.5  4.08 16.1  2019-03-01 00:00:00\n##  6        1420  2019     4  11.7     61.0  5.52 21.8  2019-04-01 00:00:00\n##  7        1420  2019     5  12.7     67.5  4.64 20.1  2019-05-01 00:00:00\n##  8        1420  2019     6  21.4     60.6  6.05 21.2  2019-06-01 00:00:00\n##  9        1420  2019     7  21.6     55.6  5.90 21.8  2019-07-01 00:00:00\n## 10        1420  2019     8  20.7     65.6  4.94 20.8  2019-08-01 00:00:00\n## # ‚Ä¶ with 47 more rows\nggplot(data = monthly_means, aes(x = year_month, y = mean_T, col = factor(STATIONS_ID))) + \n  geom_line() + \n  labs(x = 'Zeit', y = 'Temperatur (¬∞C)', color = 'Messstation')\nggplot(monthly_means, aes(x = year_month, y = mean_T, ymin = mean_T - sd_T, ymax = mean_T + sd_T)) +\n  geom_errorbar() +\n  geom_point() +\n  facet_wrap(~STATIONS_ID, nrow = 3) + \n  labs(x = 'Zeti', y = 'Temperatur (¬∞C)')\nggplot(monthly_means, aes(x = year_month, y = mean_T, ymin = mean_T - sd_T, ymax = mean_T + sd_T)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line() +\n  facet_wrap(~STATIONS_ID, nrow = 3) + \n  labs(x = 'Zeit', y = 'Temperatur (¬∞C)')\nstation_ids##        2261        1420        2667 \n##       \"Hof\" \"Frankfurt\"     \"Koeln\"\nggplot(monthly_means, aes(x = year_month, y = mean_T, ymin = mean_T - sd_T, ymax = mean_T + sd_T)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line() +\n  facet_wrap(~STATIONS_ID, nrow = 3, labeller = labeller(STATIONS_ID = station_ids)) + \n  labs(x = 'Zeit', y = 'Temperatur (¬∞C)')"},{"path":"explorative-datenanalyse.html","id":"lesestoff-6","chapter":"Kapitel 6 Explorative Datenanalyse mit tidyverse","heading":"6.5 Lesestoff","text":"Kapitel 3 Ismay Kim (2021)","code":""},{"path":"explorative-datenanalyse.html","id":"weiterf√ºhrende-literatur-und-videos","chapter":"Kapitel 6 Explorative Datenanalyse mit tidyverse","heading":"6.6 Weiterf√ºhrende Literatur und Videos","text":"R4DS Wickham Grolemund (2021): Kapitel 5 ‚ÄúData transformation‚ÄùR4DS Wickham Grolemund (2021): Kapitel 5 ‚ÄúData transformation‚ÄùEine live Analyse des Hauptautors von tidyverse, Hadley Wickham. Empfehlenswert, auch wenn er viel zu schnell tippt üòÑ.Eine live Analyse des Hauptautors von tidyverse, Hadley Wickham. Empfehlenswert, auch wenn er viel zu schnell tippt üòÑ.","code":""},{"path":"stichproben.html","id":"stichproben","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"Kapitel 7 Stichproben und Variabilit√§t","text":"\nBegriffe Stichprobenverteilung und Standardfehler erkl√§ren\n\nZufall bei wiederholter Stichprobenerhebung erkennen\n\nStichprobenverteilung darstellen\n\nEinfluss der Stichprobengr√∂√üe auf Stichprobenverteilung\nbenennen\nMit diesem Kapitel steigen wir die schlie√üende Statistik ein. Wir beginnen damit, wie man der Statistik zu Daten kommt (Stichprobenerhebung) und welche Rolle der Zufall dabei spielt. Dabei konzentrieren wir uns auf die Erhebung von einfachen zuf√§lligen Stichproben (Zufallsstichproben), nicht auf das Designen von komplizierten Erhebungen. Das ist eine Kunst f√ºr sich und geht √ºber die Ziele dieses Kurses hinaus.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)"},{"path":"stichproben.html","id":"stichproben-1","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.1 Stichproben","text":"Wir nutzen wieder die selbst erstellten Daten aus der Aufgabe B.1. Die 12000 Studierenden sind unsere Grundgesamtheit. Da wir die Daten selbst erstellt haben, wissen wir alles √ºber sie. Das ist ein gro√üer Vorteil von Computerexperimenten üòÑ. Damit alle dieselben Daten erstellen, ist die Zeile set.seed(123) notwendig. Sie sorgt daf√ºr, dass der Generator f√ºr Zufallszahlen einen zuf√§lligen, aber reproduzierbaren Zustand versetzt wird. Die Zahl den Klammern ist nicht wichtig. Wichtig ist, dass alle dieselbe benutzen.F√ºr bessere Nachvollziehbarkeit, nummerieren wir unsere Studierenden diesmal durch.Wir wollen nun 50 Studierende befragen. Durch die zuf√§llige Auswahl Befragten erzeugen wir eine zuf√§llige Stichprobe. √úbersetzt unser Computerexperiment bedeutet es, dass wir zuf√§llig 50 Zeilen aus dem Datensatz grundgesamtheit ziehen, und zwar , dass sich diese Zeilen nicht wiederholen (d.h. niemand mehrfach befragt wird). Dazu nutzen wir die Funktion rep_sample_n(), die wiederholt (rep) n Zeilen zieht (sample), und zwar mit der Einstellung replace = FALSE, also ohne Zur√ºcklegen. Wir befragen nur einmal, daher reps = 1.Damit alle wieder dieselben Daten bekommen, setzen wir vorher den seed (Zustand des Zufallszahlengenerators). Die Variable befragung_size gibt die Stichprobengr√∂√üe .Die Variable replicate zeigt immer 1. Das bedeutet, dass wir die Befragung einmal wiederholt (repliziert) haben und alle Datenpunkte zu dieser Wiederholung geh√∂ren.Wir wollen nun wissen, wie viele Studierende unter den Befragten der Stadt oder auf dem Land wohnen.Das Ganze m√∂chten wir als Anteile ausdr√ºcken. Die Funktion n() kann innerhalb der Funktion summarise() zum Ausz√§hlen genutzt werden. Wir teilen durch die Stichprobengr√∂√üe.Es wohnen also 42% auf dem Land und 58% der Stadt.passiert, wenn wir die Befragung mehrfach wiederholen, sagen wir 33 Mal? der Realit√§t ist dieses Szenario sehr unwahrscheinlich, aber einem Computerexperiment einfach zu implementieren. Es hilft uns ein Gef√ºhl f√ºr die Variabilit√§t, die durch das zuf√§llige Ausw√§hlen der Studierenden bei der Befragung entsteht, zu entwickeln.Wir setzten erneut den seed, damit alle dieselben Ergebnisse bekommen.Jetzt wird uns angezeigt, dass es dem Datensatz befragung_reps 33 replicates (Wiederholgungen) gibt. Diese sind einfach nacheinander befragung_reps angeordnet (bl√§ttern Sie durch den Datensatz). Dementsprechend hat der Datensatz 50 \\(\\times\\) 33 = 1650 Zeilen.Wie sieht es jetzt mit den Anteilen von Stadt- und Landbewohnern aus? Wir m√ºssen nun zus√§tzlich zum wohnort auch noch nach replicate gruppieren.Erwartungsgem√§√ü bringt jede Wiederholung der Befragung, die wir ja als zuf√§lliges Herausgreifen der Studierenden ohne Mehrfachbefragung programmiert haben, etwas andere Ergebnisse. Sehen Sie sich die student_id den replicates , es werde unterschiedliche Studierende befragt! einem Histogramm sieht das Ganze aus:Die h√§ufigsten Anteile sind um die 40% f√ºr Land und um die 60% f√ºr Stadt. Wir k√∂nnen es sogar etwas genauer ablesen, da wir binwidth = 0.05 gew√§hnt haben, also Schritte von 5%. Es sind 35‚Äì40% f√ºr Land und 55‚Äì60% f√ºr Stadt.","code":"\nset.seed(123)\n\nstudent_id <- 1:12000\n  \nanreise <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n             runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngeschlecht <- sample(c('m', 'w'), size = 12000, replace = TRUE)\n\nwohnort <- sapply(anreise, function(x) {\n  if(x < 30) 'stadt'\n  else 'land'\n})\n\nverkehrsmittel <- sapply(anreise, function(x) {\n  if(x <= 10) 'zu_fuss'\n  else if(x > 10 & x <= 15) sample(c('zu_fuss', 'fahrrad'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'fahrrad', 'auto'), size = 1)\n  else sample(c('bus', 'auto'), size = 1)\n})\n\nzeit_bib <- 5 * 60 - 0.7 * anreise + rnorm(length(anreise), 0, 20)\n\ngrundgesamtheit <- tibble(student_id, geschlecht, wohnort, verkehrsmittel, anreise, zeit_bib)\n\ngrundgesamtheit## # A tibble: 12,000 √ó 6\n##    student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##         <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1          1 w          stadt   bus              15.1      294.\n##  2          2 w          land    fahrrad          32.6      254.\n##  3          3 w          stadt   fahrrad          19.3      231.\n##  4          4 m          land    auto             35.9      245.\n##  5          5 m          land    bus              37.9      234.\n##  6          6 w          stadt   zu_fuss           6.59     303.\n##  7          7 w          stadt   bus              23.5      284.\n##  8          8 m          land    auto             36.2      274.\n##  9          9 m          stadt   fahrrad          24.3      299.\n## 10         10 w          stadt   bus              21.0      282.\n## # ‚Ä¶ with 11,990 more rows\nset.seed(345)\n\nbefragung_size <- 50\n\nbefragung <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = 1)\n\nbefragung## # A tibble: 50 √ó 7\n## # Groups:   replicate [1]\n##    replicate student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##        <int>      <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1         1       1623 m          stadt   zu_fuss           7.06     299.\n##  2         1       9171 m          stadt   fahrrad          11.3      278.\n##  3         1      10207 w          land    bus             107.       199.\n##  4         1       3506 w          stadt   bus              25.0      326.\n##  5         1       8892 w          stadt   bus              28.1      259.\n##  6         1       5460 m          stadt   bus              23.6      299.\n##  7         1       6120 w          stadt   bus              20.0      268.\n##  8         1        865 w          stadt   fahrrad          26.6      290.\n##  9         1      11586 m          land    bus             114.       207.\n## 10         1       8153 w          stadt   zu_fuss           8.06     297.\n## # ‚Ä¶ with 40 more rows\nbefragung %>% \n  group_by(wohnort) %>% \n  count()## # A tibble: 2 √ó 2\n## # Groups:   wohnort [2]\n##   wohnort     n\n##   <chr>   <int>\n## 1 land       21\n## 2 stadt      29\nbefragung %>% \n  group_by(wohnort) %>% \n  summarise(prop = n()/befragung_size)## # A tibble: 2 √ó 2\n##   wohnort  prop\n##   <chr>   <dbl>\n## 1 land     0.42\n## 2 stadt    0.58\nset.seed(234)\n\nbefragung_reps <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = 33)\n\nbefragung_reps## # A tibble: 1,650 √ó 7\n## # Groups:   replicate [33]\n##    replicate student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##        <int>      <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1         1       2079 m          land    auto              38.8     262.\n##  2         1       1314 m          stadt   fahrrad           13.3     301.\n##  3         1       1710 m          stadt   auto              26.5     272.\n##  4         1       4386 w          stadt   bus               23.9     269.\n##  5         1       9490 m          land    auto              34.2     262.\n##  6         1      11757 w          land    bus              102.      227.\n##  7         1      11649 w          land    bus              111.      202.\n##  8         1       2244 m          land    bus               38.9     256.\n##  9         1       3652 w          stadt   fahrrad           10.3     254.\n## 10         1       3127 m          stadt   fahrrad           29.6     271.\n## # ‚Ä¶ with 1,640 more rows\nwohnort_props <- befragung_reps %>% \n  group_by(replicate, wohnort) %>% \n  summarise(prop = n()/befragung_size)\n\nwohnort_props## # A tibble: 66 √ó 3\n## # Groups:   replicate [33]\n##    replicate wohnort  prop\n##        <int> <chr>   <dbl>\n##  1         1 land     0.42\n##  2         1 stadt    0.58\n##  3         2 land     0.36\n##  4         2 stadt    0.64\n##  5         3 land     0.4 \n##  6         3 stadt    0.6 \n##  7         4 land     0.38\n##  8         4 stadt    0.62\n##  9         5 land     0.4 \n## 10         5 stadt    0.6 \n## # ‚Ä¶ with 56 more rows\nggplot(data = wohnort_props, aes(x = prop)) + \n  geom_histogram(binwidth = 0.05, boundary = 0.4, col = 'white') +\n  facet_wrap(~ wohnort) +\n  labs(x = 'Anteile beim Wohnort', title = 'Verteilung der Wohnorte', y = 'H√§ufigkeit')"},{"path":"stichproben.html","id":"anzahl-der-wiederholungen-und-variabilit√§t","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.2 Anzahl der Wiederholungen und Variabilit√§t","text":"passiert, wenn wir unsere Umfrage 1000 Mal wiederholen? Wir k√∂nnen den ganzen Code wiederverwenden und m√ºssen nur die reps entsprechend ver√§ndern. Wir erstellen daf√ºr eine zus√§tzliche Variable befragung_num. Beim Histogramm sollten wir die binwidth etwas heruntersetzten, da wir jetzt sehr viel mehr Daten haben und diese detaillierter anzeigen lassen k√∂nnen. Zus√§tzlich wird die x-Achse ‚Äúfrei‚Äù gegeben, .e.¬†die Skalierung wird jetzt auf jeder Achse separat bestimmt scales = 'free_x' facet_wrap(), um die Verteilungen besser zu sehen.Die h√§ufigsten Anteile beim Land liegen bei 40‚Äì42% und bei der Stadt bei 58‚Äì60%.Die Histogramme geben nun gut die Verteilung der Anteile der Stadt- und Landbewohner wieder. Solche Verteilungen nennt man Stichprobenverteilungen. Sie zeigen die Verteilung einer statistischen Kenngr√∂√üe (Statistik), unserem Fall Anteil, die aus zuf√§lligen Stichproben ausgerechnet wurde. Die Stichprobenverteilung beantwortet die Frage: Wenn ich eine zuf√§llige Menge (Stichprobengr√∂√üe) Daten (Zufallsstichprobe) aus der Grundgesamtheit herausgreife und eine Kenngr√∂√üe (z.B. Anteil) berechne, welchen Wert werde ich im Mittel erhalten und wie stark wird der Wert schwanken.","code":"\nset.seed(345)\n\nbefragung_num <- 1000\n\nbefragung_reps <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = befragung_num)\n\nwohnort_props <- befragung_reps %>% \n  group_by(replicate, wohnort) %>% \n  summarise(prop = n()/befragung_size)\n\n\nggplot(data = wohnort_props, aes(x = prop)) + \n  geom_histogram(binwidth = 0.02, boundary = 0.4, col = 'white') +\n  facet_wrap(~ wohnort, scales = 'free_x') +\n  labs(x = 'Anteile beim Wohnort', title = 'Verteilung der Wohnorte', y = 'H√§ufigkeit')"},{"path":"stichproben.html","id":"stichprobengr√∂√üe","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.3 Stichprobengr√∂√üe","text":"passiert, wenn wir die Gr√∂√üe der Stichproben variieren? Das w√ºrde Befragungen mit unterschiedlicher Anzahl von Teilnehmern entsprechen. Wir vergleichen 25, 50 und 100 Befragte und wiederholen jeweils 1000 Mal, um erneut Stichprobenverteilungen plotten zu k√∂nnen. Das ist eine repetitive Aufgabe und ich werde daf√ºr eine Funktion definieren. Wie man das macht, wird einer sp√§teren Stunde erkl√§rt.Nun wenden wir diese selbst definierte Funktion und f√ºhren die Befragungen durch.Wir stellen die drei Stichprobenverteilungen dar. Die Einstellung f√ºr binwidth ist jeweils eine andere.Wie kann man diese Stichprobenverteilungen charakterisieren? Es sind erster Linie Daten und wie bei jedem Datensatz kann man auch hier einen Mittelwert und eine Standardabweichung angeben. Die Standardabweichung der Stichprobenverteilung hei√üt Standardfehler und fasst den Einfluss der Variabilit√§t (zuf√§lliges Herausgreifen der Studierenden) zusammen.Sie sehen, dass mit steigender Stichprobengr√∂√üe der Standardfehler sinkt. Das ist intuitiv verst√§ndlich, denn je mehr Studierende wir (pro Wiederholung!) befragen, desto repr√§sentativer ist die Stichprobe.Da dieses Kapitel wichtig ist, gibt es ausnahmsweise vorformulierte Take-Home-Messages üòÑ:\nEine zuf√§llige Stichprobe ist (meistens\\(^*\\)) der K√∂nigsweg, um repr√§sentative\nInformationen √ºber die Grundgesamtheit zu bekommen.\n\nDie Verteilung einer statistischen Kenngr√∂√üe, die aus\nZufallsstichproben ausgerechnet wurde, hei√üt Stichprobenverteilung. Um\ndiese Verteilung zu bekommen, muss man wiederholt Stichproben erheben.\nJe mehr Stichproben man erhebt, desto genauer kann man die\nStichprobenverteilung beschreiben.\n\nDie Standardabweichung der Kenngr√∂√üe, die durch die\nStichprobenverteilung dargestellt wird, hei√üt Standardfehler.\n\nDer Zufall macht sich durch eine Streuung (erfasst durch den\nStandardfehler) der Stichprobenverteilung bemerkbar. Je gr√∂√üer die\neinzelnen Stichproben, desto kleiner der Standardfehler.\n\\(^*\\): Manchmal ist die interessierte Gr√∂√üe unterschiedlichen Untergruppen der Grundgesamtheit unterschiedlich verteilt. Z.B. k√∂nnten bestimmte Haltungen der Bev√∂lkerung gegen√ºber irgendwelchen Sachverhalten von Alter oder Bildungsstand oder Wohnort (Stadt vs.¬†Land) abh√§ngen. Dann sollte man sich √ºberlegen, ob man statt einer zuf√§lligen Stichprobe lieber eine geschichtete Zufallsstichprobe zieht, d.h. innerhalb dieser Kategorien zuf√§llig beprobt.","code":"\ncalculate_props <- function(grund_data = grundgesamtheit, befragung_size, befragung_reps = 1000) {\n  \n  befragung <- rep_sample_n(grund_data, size = befragung_size, replace = FALSE, reps = befragung_reps)\n\nwohnort_props <- befragung %>% \n  group_by(replicate, wohnort) %>% \n  summarise(prop = n()/befragung_size)\n\nwohnort_props\n}\nset.seed(123)\n\n# Stichprobengr√∂√üe 25\nwohnort_props_25 <- calculate_props(grund_data = grundgesamtheit, befragung_size = 25, befragung_reps = 1000)\n  \n# Stichprobengr√∂√üe 50\nwohnort_props_50 <- calculate_props(grund_data = grundgesamtheit, befragung_size = 50, befragung_reps = 1000)\n\n# Stichprobengr√∂√üe 100\nwohnort_props_100 <- calculate_props(grund_data = grundgesamtheit, befragung_size = 100, befragung_reps = 1000)\nggplot(data = wohnort_props_25, aes(x = prop)) + \n  geom_histogram(binwidth = 0.04, boundary = 0.4, col = 'white') +\n  facet_wrap(~ wohnort, scales = 'free_x') +\n  labs(x = 'Anteile beim Wohnort', title = 'Verteilung der Wohnorte, Stichprobengr√∂√üe = 25', y = 'H√§ufigkeit')\nggplot(data = wohnort_props_50, aes(x = prop)) + \n  geom_histogram(binwidth = 0.02, boundary = 0.4, col = 'white') +\n  facet_wrap(~ wohnort, scales = 'free_x') +\n  labs(x = 'Anteile beim Wohnort', title = 'Verteilung der Wohnorte, Stichprobengr√∂√üe = 50', y = 'H√§ufigkeit')\nggplot(data = wohnort_props_100, aes(x = prop)) + \n  geom_histogram(binwidth = 0.02, boundary = 0.4, col = 'white') +\n  facet_wrap(~ wohnort, scales = 'free_x') +\n  labs(x = 'Anteile beim Wohnort', title = 'Verteilung der Wohnorte, Stichprobengr√∂√üe = 100', y = 'H√§ufigkeit')\nwohnort_props_25 %>% \n  group_by(wohnort) %>% \n  summarise(prop_sd = sd(prop))## # A tibble: 2 √ó 2\n##   wohnort prop_sd\n##   <chr>     <dbl>\n## 1 land      0.102\n## 2 stadt     0.102\nwohnort_props_50 %>% \n  group_by(wohnort) %>% \n  summarise(prop_sd = sd(prop))## # A tibble: 2 √ó 2\n##   wohnort prop_sd\n##   <chr>     <dbl>\n## 1 land     0.0679\n## 2 stadt    0.0679\nwohnort_props_100 %>% \n  group_by(wohnort) %>% \n  summarise(prop_sd = sd(prop))## # A tibble: 2 √ó 2\n##   wohnort prop_sd\n##   <chr>     <dbl>\n## 1 land     0.0458\n## 2 stadt    0.0458"},{"path":"stichproben.html","id":"lesestoff-7","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.4 Lesestoff","text":"Kapitel 7 Ismay Kim (2021)","code":""},{"path":"stichproben.html","id":"aufgaben-5","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.5 Aufgaben","text":"","code":""},{"path":"stichproben.html","id":"wahrer-wert-in-der-grundgesamtheit","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.5.1 Wahrer Wert in der Grundgesamtheit","text":"Berechnen Sie die Anteile von Studierenden der Grundgesamtheit, die der Stadt bzw. auf dem Land leben. Wie gut waren die Sch√§tzungen im Vergleich zum wahren Wert der Grundgesamtheit?","code":""},{"path":"stichproben.html","id":"lohnt-eine-station-zum-ausleihen-von-fahrr√§dern","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.5.2 Lohnt eine Station zum Ausleihen von Fahrr√§dern?","text":"unserem fiktiven Beispiel der Uni Werdeschlau geht es eigentlich darum, ob sich eine Station zum Parken und Reparieren (also eine Servicestation) von Fahrr√§dern lohnen w√ºrde. Daher ist die Frage interessant, wie viele Studierende mit dem Fahrrad zur Uni kommen. Wiederholen Sie die obige Analyse und ermitteln Sie statt der Anteile von Stadt- und Landbewohnern nun die Anteile der unterschiedlichen Verkehrsmittel. Die selbst definierte Funktion m√ºssen Sie durch die folgende ersetzen.der L√∂sung zu dieser Aufgabe erhalten Sie auch weitere Tipps zur Darstellung von Histogrammen und Dichtefunktionen üòé.","code":"\ncalculate_props_verkehr <- function(grund_data = grundgesamtheit, befragung_size, befragung_reps = 1000) {\n  \n  befragung <- rep_sample_n(grund_data, size = befragung_size, replace = FALSE, reps = befragung_reps)\n\nverkehrsmittel_props <- befragung %>% \n  group_by(replicate, verkehrsmittel) %>% \n  summarise(prop = n()/befragung_size)\n\nverkehrsmittel_props\n}"},{"path":"stichproben.html","id":"ihre-arbeit-einreichen-3","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.6 Ihre Arbeit einreichen","text":"Speichern Sie Ihr Notebook ab und laden Sie nur die .Rmd Datei vom Server.Laden Sie Ihre .Rmd Datei ILIAS hoch. Beachten Sie die Frist!Sie erhalten die Musterl√∂sung nach dem Hochladen.","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"bootstrapping-und-konfidenzintervalle","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"Kapitel 8 Bootstrapping und Konfidenzintervalle","text":"\nFunktionsweise von Bootstrap erkl√§ren\n\nBootstrap-Konfidenzintervalle f√ºr Mittelwert berechnen\nIm Kapitel 7 haben Sie gesehen, dass Statistiken aus zuf√§llig gezogenen Stichproben, dem Zufall unterliegen. Sie sind Zufallsvariablen. Diesen Zufall haben wir mithilfe der Stichprobenverteilung dieser Statistiken quantifiziert, dem wir den Standardfehler, die Standardabweichung aus der Stichprobenverteilung, berechnet haben.Die Statistik, die wir im Kapitel 7 berechnet haben, war der Anteil von Studierenden, die entweder der Stadt oder auf dem Land wohnen. Mit jeder Stichprobe, aus der wir diesen Anteil berechnet haben, haben wir eigentlich gesch√§tzt, wie gro√ü der wahre Anteil der Stadt- und Landbewohner der Grundgesamtheit (allen 12000 Studierenden von Werdeschlau) ist. Der aus der Stichprobe berechnete Anteil ist also ein Sch√§tzer f√ºr den wahren Anteil der Grundgesamtheit. Dieser Sch√§tzer ist eine Zufallsvariable (s.o.) und wie jede andere Zufallsvariable ist er durch seine Verteilung, n√§mlich die Stichprobenverteilung charakterisiert.Wir k√∂nnen jetzt also eine Menge statistischer Begriffe mithilfe unseres Beispiels mit Leben f√ºllen:\nGrundgesamtheit: alle Studierenden der Universit√§t\nWerdeschlau\n\nzuf√§llige Stichprobe: eine zuf√§llig ausgesuchte\nGruppe von Studierenden\n\nParameter der Grundgesamtheit: z.B. der wahre\nAnteil von Studierenden, die der Stadt oder auf dem Land leben\n\nSch√§tzer f√ºr diesen Parameter der Grundgesamtheit:\nAnteil der Studierenden, die der Stadt oder auf dem Land leben,\nberechnet aus der zuf√§lligen Stichprobe. Da die Stichprobe zuf√§llig ist,\nkann man davon ausgehen, dass sie repr√§sentativ f√ºr die Grundgesamtheit\nist und der Sch√§tzer unverzerrt (unbiased, d.h. ohne einen\nsystematischen Fehler).\n\nInferenz: schlie√üen auf die Grundgesamtheit darf\nman, wenn die Stichprobe zuf√§llig erhoben wurde und repr√§sentativ f√ºr\ndie Fragestellung ist.\nDie Begriffe Statistik, Sch√§tzer, Sch√§tzfunktion und Stichprobenfunktion werden als Synonyme verwendet. Die Statistik ist ja auch eine Funktion, da sie mit einer Formel eine Zahl aus Daten (Stichprobe) berechnet. Sie fasst die Stichprobe also zusammen.Im echten Leben werden Sie kaum wiederholt befragen (Stichproben ziehen) k√∂nnen. Das ist vollkommen unrealistisch und nur f√ºr Computerexperimente ein tolles Werkzeug. diesem Kapitel wird es darum gehen, wie man nun im richtigen Leben mit einer Stichprobe einen Parameter der Grundgesamtheit sch√§tzen kann und dabei seine Variabilit√§t quantifiziert.","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"bootstrapping-die-m√ºnchhausenmethode","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.1 Bootstrapping, die M√ºnchhausenmethode","text":"Wenn wir nur eine Stichprobe haben, werden wir den Parameter der Grundgesamtheit daraus sch√§tzen. Der Sch√§tzer kann der Mittelwert oder eben auch der Anteil sein, wie bei unseren vorherigen Beispielen mit dem Mittelwert der Anreisezeit oder dem Anteil der Studierenden aus der Stadt bzw. vom Land.Da jede Stichprobe dem Zufall unterliegt und der Sch√§tzer somit eine Zufallsvariable darstellt, w√ºrden wir gerne wissen, wie gut wir sch√§tzen.Gibt es einen plausiblen Bereich f√ºr den Mittelwert der Anreisezeit? Plausibel meint, dass wenn wir sehr oft verschiedene zuf√§llige Stichprobe ziehen, dieser Bereich sagen wir mal 95% der F√§lle den wahren Mittelwert einschlie√üt. Solche Plausibilit√§tsbereiche nennt man Konfidenzintervalle.Es gibt mehrere Methoden, solche Konfidenzintervalle zu berechnen. Wenn man die Verteilung des Sch√§tzers kennt, wie beim Mittelwert (Normalverteilung), dann kann man daraus den Standardfehler berechnen (nennen wir diesen \\(SE\\)). Ein 95%-Konfidenzintervall w√§re dann \\(\\hat{\\mu} \\pm 1.96 \\cdot SE\\), wobei \\(\\hat{\\mu}\\) der gesch√§tzte Mittelwert ist. Das H√ºtchen steht f√ºr gesch√§tzt. Diese Formel haben Sie bestimmt schon der Grundvorlesung Statistik gesehen.Es gibt aber Sch√§tzer, f√ºr die keine theoretische Verteilung bekannt ist. Da muss man eine andere Methode anwenden. Eine bekannte Methode hei√üt Bootstrapping (Bootstrap-Verfahren), manchmal auch M√ºnchhausenmethode. Sie klingt auf den ersten Blick wie ein Selbstbetrug, als ob man sich selbst den Haaren aus dem Sumpf zieht (Abblildung 8.1), hat aber fundierte mathematische Wurzeln. Bootstrap wurde von Efron (1979) Ende der 70er vorgestellt und hat sich seitdem als eine der wichtigsten Resampling-Strategien etabliert.\nAbbildung 8.1: M√ºnchhausen zieht sich aus dem Sumpf (Theodor Hosemann (1807-1875), Public domain, via Wikimedia Commons), Link zum Bild\nDas Prinzip beim Bootstrap ist, dass die Stichprobe die Rolle der Grundgesamtheit √ºbernimmt. Abbildung 8.2 zeigt das Vorgehen aus dem Kapitel 7. Wir ziehen mehrere echte Stichproben aus einer Grundgesamtheit, erhalten eine Stichprobenverteilung und k√∂nnen den Parameter der Grundgesamtheit sch√§tzen.\nAbbildung 8.2: Berechnen einer Stichprobenverteilung durch wiederholtes Stichproben ziehen. Abbildung aus (Hesterberg 2015), dort Figure 4. Die Publikation ist open-access und darf f√ºr nicht-kommerzielle Zwecke verwendet werden. Link zur Lizenz\nAbbildung 8.3 haben wir nur eine Stichprobe. Wir gehen davon aus, dass diese Stichprobe eine Miniatur der Grundgesamtheit ist, also zuf√§llig gezogen wurde und repr√§sentative ist. Mit dieser Grundidee im Kopf, ersetzten wir die Grundgesamtheit durch diese Stichprobe und verfahren (fast) genauso, um die Stichprobenverteilung zu ermitteln. Der einzige Unterschied ist, dass wir aus dieser einen Stichprobe mit Zur√ºcklegen neue Stichproben (Bootstrap-Stichproben) ziehen.\nAbbildung 8.3: Berechnen einer Stichprobenverteilung durch wiederholtes Ziehen aus einer Stichprobe mit Zur√ºcklegen. Abbildung aus (Hesterberg 2015), dort Figure 5. Die Publikation ist open-access und darf f√ºr nicht-kommerzielle Zwecke verwendet werden. Link zur Lizenz\nDie Konfidenzintervalle berechnet man aus der Stichprobenverteilung der Bootstrap-Stichproben, indem man z.B. das 2.5% und das 97.5% Quantil berechnet. Zwischen diesen beiden Quantilen sind 95% der Werte enthalten. Dieses Intervall nennt man das 95%-Konfidenzintervall.","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"konfidenzintervall-f√ºr-den-mittelwert-der-anreisezeit","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.2 Konfidenzintervall f√ºr den Mittelwert der Anreisezeit","text":"Wir sehen uns das Ganze anhand des Beispiels der Studierenden aus Werdeschlau . Zun√§chst wieder der Code f√ºr die Simulation der Grundgesamtheit.Nun befragen wir 200 Studierende.Wir berechnen den wahren Mittelwert der Anreisezeit und den Mittelwert aus der Befragung.Wir ziehen jetzt unsere Bootstrap-Stichproben aus der einen Stichprobe, n√§mlich der befragung. Achten Sie darauf, wie √§hnlich der Code zum Ziehen von echten Stichproben ist. Wir √§ndern nur replace = TRUE.Studierende kommen jetzt mehrfach vor, da wir ja mit Zur√ºcklegen gezogen haben. Wir sehen uns das den erste 50 Bootstrap-Stichproben .Jetzt berechnen wir die Mittelwerte aus den Bootstrap-Stichproben.Der Standardfehler des Bootstraps und das 95%-Konfidenzintervall basierend auf Quantilen berechnen sich wie folgt:Wir stellen die Stichprobenverteilung mit dem Standardfehler des Bootstraps und dem 95%-Konfidenzintervall dar. Das ist eine umfangreiche Grafik und wir gehen der √úbung Schritt f√ºr Schritt vor. Die Funktion scale_color_manual erlaubt es uns, die Legende anzupassen.Der Standardfehler des Bootstraps stimmt gut mit dem Fehler \\(s/\\sqrt(n)\\) f√ºr den Sch√§tzer des Mittelwerts √ºberein. Diese Formel nutzt die Normalverteilung des Sch√§tzers aus. Da wir normalerweise die wahre Standardabweichung der Grundgesamtheit nicht kennen, wird der Formel \\(s/\\sqrt(n)\\) die Standardabweichung der Stichprobe als Sch√§tzung verwendet.Standardfehler des Bootstraps.Und der wahre Standardfehler, wenn man die Standardabweichung der Grundgesamtheit kennt.","code":"\nlibrary(tidyverse)\nlibrary(infer)\nset.seed(123)\n\nstudent_id <- 1:12000\n  \nanreise <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n             runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngeschlecht <- sample(c('m', 'w'), size = 12000, replace = TRUE)\n\nwohnort <- sapply(anreise, function(x) {\n  if(x < 30) 'stadt'\n  else 'land'\n})\n\nverkehrsmittel <- sapply(anreise, function(x) {\n  if(x <= 10) 'zu_fuss'\n  else if(x > 10 & x <= 15) sample(c('zu_fuss', 'fahrrad'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'fahrrad', 'auto'), size = 1)\n  else sample(c('bus', 'auto'), size = 1)\n})\n\nzeit_bib <- 5 * 60 - 0.7 * anreise + rnorm(length(anreise), 0, 20)\n\ngrundgesamtheit <- tibble(student_id, geschlecht, wohnort, verkehrsmittel, anreise, zeit_bib)\n\ngrundgesamtheit## # A tibble: 12,000 √ó 6\n##    student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##         <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1          1 w          stadt   bus              15.1      294.\n##  2          2 w          land    fahrrad          32.6      254.\n##  3          3 w          stadt   fahrrad          19.3      231.\n##  4          4 m          land    auto             35.9      245.\n##  5          5 m          land    bus              37.9      234.\n##  6          6 w          stadt   zu_fuss           6.59     303.\n##  7          7 w          stadt   bus              23.5      284.\n##  8          8 m          land    auto             36.2      274.\n##  9          9 m          stadt   fahrrad          24.3      299.\n## 10         10 w          stadt   bus              21.0      282.\n## # ‚Ä¶ with 11,990 more rows\nset.seed(345)\n\nbefragung_size <- 200\n\nbefragung <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = 1)\n\nbefragung## # A tibble: 200 √ó 7\n## # Groups:   replicate [1]\n##    replicate student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##        <int>      <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1         1       1623 m          stadt   zu_fuss           7.06     299.\n##  2         1       9171 m          stadt   fahrrad          11.3      278.\n##  3         1      10207 w          land    bus             107.       199.\n##  4         1       3506 w          stadt   bus              25.0      326.\n##  5         1       8892 w          stadt   bus              28.1      259.\n##  6         1       5460 m          stadt   bus              23.6      299.\n##  7         1       6120 w          stadt   bus              20.0      268.\n##  8         1        865 w          stadt   fahrrad          26.6      290.\n##  9         1      11586 m          land    bus             114.       207.\n## 10         1       8153 w          stadt   zu_fuss           8.06     297.\n## # ‚Ä¶ with 190 more rows\nmean_grundgesamtheit <- grundgesamtheit %>% \n  summarise(Mittelwert = mean(anreise))\n\nmean_grundgesamtheit## # A tibble: 1 √ó 1\n##   Mittelwert\n##        <dbl>\n## 1       36.0\nmean_befragung <- befragung %>% \n  summarise(Mittelwert = mean(anreise))\n\nmean_befragung## # A tibble: 1 √ó 2\n##   replicate Mittelwert\n##       <int>      <dbl>\n## 1         1       34.1\nset.seed(345)\n\nbefragung_size <- 200\nnumber_reps <- 10000\n\nbefragung_reps_bootstrap <- rep_sample_n(befragung, size = befragung_size, replace = TRUE, reps = number_reps)\n\nbefragung_reps_bootstrap## # A tibble: 2,000,000 √ó 7\n## # Groups:   replicate [10,000]\n##    replicate student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##        <int>      <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1         1       7038 m          stadt   bus              29.4      243.\n##  2         1        493 m          stadt   bus              21.1      303.\n##  3         1       3442 m          land    auto             30.3      311.\n##  4         1       4920 w          stadt   bus              21.1      290.\n##  5         1       3178 w          stadt   zu_fuss           8.07     325.\n##  6         1       7694 w          land    auto             31.3      295.\n##  7         1       9479 m          land    bus              39.5      302.\n##  8         1       3367 m          land    auto             36.4      272.\n##  9         1       5985 w          stadt   fahrrad          15.9      303.\n## 10         1        843 m          stadt   fahrrad          26.4      300.\n## # ‚Ä¶ with 1,999,990 more rows\nbefragung_reps_bootstrap %>% \n  filter(replicate %in% (1:50)) %>% \n  group_by(replicate, student_id) %>% \n  tally() %>% \n  filter(n != 1) %>% \n  arrange(desc(n))## # A tibble: 2,657 √ó 3\n## # Groups:   replicate [50]\n##    replicate student_id     n\n##        <int>      <int> <int>\n##  1        43       4787     8\n##  2        20       5985     6\n##  3        34       7749     6\n##  4        38       5641     6\n##  5        41       8456     6\n##  6         3       7083     5\n##  7         3      10943     5\n##  8         5       8994     5\n##  9         6      10118     5\n## 10         8      11425     5\n## # ‚Ä¶ with 2,647 more rows\nres_means_bootstrap <- befragung_reps_bootstrap %>%\n  group_by(replicate) %>% \n  summarise(Mittelwert = mean(anreise))\n\nres_means_bootstrap## # A tibble: 10,000 √ó 2\n##    replicate Mittelwert\n##        <int>      <dbl>\n##  1         1       32.6\n##  2         2       31.9\n##  3         3       38.9\n##  4         4       33.5\n##  5         5       35.4\n##  6         6       37.2\n##  7         7       34.4\n##  8         8       33.4\n##  9         9       35.5\n## 10        10       31.0\n## # ‚Ä¶ with 9,990 more rows\nstat_bootstrap <- res_means_bootstrap %>% \n  summarize(mean = mean(Mittelwert), sd = sd(Mittelwert), ci_2.5 = quantile(Mittelwert, probs = 0.025), ci_97.5 = quantile(Mittelwert, probs = 0.975))\nggplot(res_means_bootstrap, aes(Mittelwert)) + \n  geom_histogram(bins = 50 , color=\"white\") +\n  labs(y = 'H√§ufigkeit', x = 'Mittelwerte der Anreise (min)') +\n  geom_vline(aes(xintercept = mean_grundgesamtheit$Mittelwert, col = 'grundgesamtheit'), linetype = \"dashed\", size = 2) + \n  geom_vline(aes(xintercept = stat_bootstrap$mean, col = 'boot')) + \n  geom_vline(aes(xintercept = mean_befragung$Mittelwert, col = 'stichprobe'), linetype = 'dashed', size = 2) +\n  geom_vline(aes(xintercept = stat_bootstrap$mean + stat_bootstrap$sd, col = 'sd')) + \n  geom_vline(xintercept = stat_bootstrap$mean - stat_bootstrap$sd, col = 'orange') + \n  geom_vline(aes(xintercept = stat_bootstrap$ci_2.5, col = 'ci')) +\n  geom_vline(xintercept = stat_bootstrap$ci_97.5, col = 'brown') + \n  scale_color_manual(name = \"Statistik\", values = c(grundgesamtheit = 'black', sd = 'orange', boot = 'red', ci = 'brown', stichprobe = 'gray90'), breaks = c('stichprobe', 'boot', 'sd', 'ci', 'grundgesamtheit'), label = c('Mittelwert Stichprobe', 'Mittelwert Bootstrap', 'Standardfehler Bootstrap', '95% Konfidenzintervall Bootstrap', 'Mittelwert Population'))\nbefragung %>% \n  summarize(sd_error = sd(anreise)/sqrt(length(anreise)))## # A tibble: 1 √ó 2\n##   replicate sd_error\n##       <int>    <dbl>\n## 1         1     2.04\nres_means_bootstrap %>% \n  summarize(sd_error = sd(Mittelwert))## # A tibble: 1 √ó 1\n##   sd_error\n##      <dbl>\n## 1     2.06\ngrundgesamtheit %>% \n  summarize(sd_error = sd(anreise)/sqrt(length(befragung$anreise)))## # A tibble: 1 √ó 1\n##   sd_error\n##      <dbl>\n## 1     2.09"},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"bootstrap-konfidenzintervalle-mit-infer","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.3 Bootstrap-Konfidenzintervalle mit infer","text":"Das Paket infer bietet eine sehr bequeme M√∂glichkeit, Konfidenzintervalle mit Bootstrap zu berechnen und zu visualisieren. Das Vorgehen aus dem vorherigen Abschnitt √ºbertragen wir nun den Workflow mit infer.Berechnen der Bootstrap-Stichproben.Visualisieren der Stichprobenverteilung.Berechnen KonfidenzintervalleVisualisieren der Stichprobenverteilung mit den KonfidenzintervallenDie Konfidenzintervalle, die wir als Quantile aus der Bootstrap-Stichprobenverteilung berechnet haben, und die, die man mit der Formel \\(\\hat{\\mu} \\pm 1.96 \\cdot s/\\sqrt(n)\\) berechnen kann, sind √§hnlich. Das werden Sie einer der Aufgaben (s.u.) nachrechnen. Das liegt daran, dass der Zentrale Grenzwertsatz garantiert, dass die Stichprobenverteilung des Sch√§tzers des Mittelwerts eine Normalverteilung ist. Es gibt aber Sch√§tzer, wie den des Medians, f√ºr den es keine theoretische Verteilung gibt. Daher gilt als Take-Home-Message, dass man Bootstrap zum Berechnen der Konfidenzintervalle gut einsetzen kann, egal ob es eine theoretische Verteilung des Sch√§tzers gibt. Die Quantilmethode zur Berechnung der Bootstrap-Konfidenzintervalle liefert gute Ergebnisse. Wir werden im sp√§teren Kapitel lernen, dass man Bootstrap auch f√ºr die Regressionsanalyse nutzen kann.","code":"\nset.seed(345)\n\nbootstrap_distribution <- befragung %>%\n  specify(response = anreise) %>% \n  generate(reps = 10000, type = 'bootstrap') %>% \n  calculate(stat = 'mean')\nvisualize(bootstrap_distribution)\npercentile_ci <- bootstrap_distribution %>% \n  get_confidence_interval(level = 0.95, type = \"percentile\")\npercentile_ci## # A tibble: 1 √ó 2\n##   lower_ci upper_ci\n##      <dbl>    <dbl>\n## 1     30.1     38.3\nvisualize(bootstrap_distribution) + \n  shade_confidence_interval(endpoints = percentile_ci, color = \"orange\", fill = \"khaki\") +\n  geom_vline(xintercept = mean_grundgesamtheit$Mittelwert, linetype = 'dashed')"},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"bedeutung-der-konfidenzintervalle","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.4 Bedeutung der Konfidenzintervalle","text":"Ein Konfidenzintervall h√§ngt von der Stichprobe ab, ist also vom Zufall betroffen. Man kann also sagen, dass die Grenzen des Konfidenzintervalls Zufallsvariablen sind. Das Konfidenzintervall wird nicht immer den wahren Parameter der Grundgesamtheit einschlie√üen. Die Definition eines 95%-Konfidenzintervalls kann wie folgt formuliert werden:Wenn wir sehr oft die Stichproben neu ziehen und jedes Mal ein 95%-Konfidenzintervall berechnen, dann erwarten wir, dass 95% der F√§lle diese Konfidenzintervalle den wahren Parameter der Grundgesamtheit enthalten.Das Konfidenzintervall ist unsere Absch√§tzung der Lage des wahren Parameters der Grundgesamtheit. Die Interpretation wird oft abgek√ºrzt, dass man sagt, man sei zu 95% sicher, dass das 95%-Konfidenzintervall den wahren Parameter enth√§lt. Das ist nicht richtig (s. Definition oben). Es ist besser zu sagen, dass 95% der F√§lle, das Konfidenzintervall den wahren Parameter der Grundgesamtheit enth√§lt. mit 95% der F√§lle gemeint ist, wissen Sie ja, wenn Sie sich die genaue Definition erinnern üòÑ.","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"lesestoff-8","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.5 Lesestoff","text":"Kapitel 8 Ismay Kim (2021)","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"aufgaben-6","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.6 Aufgaben","text":"","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"konfidenzintervall-aus-dem-zentralen-grenzwertsatz","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.6.1 Konfidenzintervall aus dem Zentralen Grenzwertsatz","text":"Der Zentrale Grenzwertsatz besagt, dass die Stichprobenverteilung des Sch√§tzers des Mittelwerts sich asymptotisch (also bei vielen Stichproben) der Normalverteilung n√§hert. Daher kann man f√ºr die Konfidenzintervalle auch die folgende Formel nutzen: \\(\\hat{\\mu} \\pm 1.96 \\cdot SE\\), wobei \\(\\hat{\\mu}\\) der gesch√§tzte Mittelwert ist. Das H√ºtchen steht f√ºr gesch√§tzt.Berechnen Sie die Konfidenzintervalle mit dieser Formel f√ºr die Stichprobenverteilung aus dem Bootstrap f√ºr den Mittelwert der Anreisezeit. Dazu passen Sie der Funktion get_confidence_interval den Typ des Konfidenzintervalls type = \"se\" und geben Sie den Mittelwert der Befragung : get_confidence_interval(type = \"se\", point_estimate = mean_befragung$Mittelwert).Stellen Sie die Stichprobenverteilung mit diesem Konfidenzintervall dar und vergleichen Sie mit dem Konfidenzintervall, das wir mit der Quantil-Methode berechnet haben.","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"anteile-an-stadt--und-landbewohnern","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.6.2 Anteile an Stadt- und Landbewohnern","text":"Wiederholen Sie die Analyse, die wir mit dem Paket infer f√ºr die Sch√§tzung des Mittelwerts der Anreisezeit gemacht haben, nun f√ºr die Anteile Stadt- und Landbewohnern. Tipp: specify(response = wohnort, success = 'stadt').","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"wie-h√§ngt-das-konfidenzintervall-von-der-stichprobengr√∂√üe-ab","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.6.3 Wie h√§ngt das Konfidenzintervall von der Stichprobengr√∂√üe ab?","text":"Wiederholen Sie die Analyse f√ºr den Mittelwert der Anreisezeit f√ºr eine Stichprobe von 30 Studierenden. Wie ver√§ndert sich das Konfidenzintervall?","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"ihre-arbeit-einreichen-4","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.7 Ihre Arbeit einreichen","text":"Speichern Sie Ihr Notebook ab und laden Sie nur die .Rmd Datei vom Server.Laden Sie Ihre .Rmd Datei ILIAS hoch. Beachten Sie die Frist!Sie erhalten die Musterl√∂sung nach dem Hochladen.","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"weiterf√ºhrende-literatur-1","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.8 Weiterf√ºhrende Literatur","text":"Kapitel 17.3 Sauer (2019)","code":""},{"path":"hypothesentsts.html","id":"hypothesentsts","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"Kapitel 9 Hypothesentests mit dem Paket infer","text":"\nIdee hinter simulationsbasierten Tests erkl√§ren\n\nTests mit dem Paket infer durchf√ºhren\n","code":""},{"path":"hypothesentsts.html","id":"es-gibt-nur-einen-test","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.1 Es gibt nur einen Test","text":"Ich wei√ü nicht, wie es Ihnen ergangen ist, aber ich habe meiner Grundausbildung Statistik verschiedene Tests kennengelernt: \\(t\\)-Test f√ºr gepaarte und ungepaarte Stichproben, \\(F\\)-Test f√ºr die Varianz, \\(z\\)-Test etc. Ende war ich verwirrt und wusste nicht mehr, welchen ich wann nehmen soll. Ende des Tages musste ich, wenn ich einen neuen Datensatz hatte und einen Test brauchte, lange nachdenken, welchen ich nutzen soll.Diese vielen Tests stammen noch aus der Urzeit der Statistik, als Rechenzeit, wenn √ºberhaupt vorhanden, unbezahlbar war. Daher haben die V√§ter der Statistik (war damals √ºberwiegend eine reine M√§nnerclique üò†) viele N√§herungsverfahren entwickelt. Diese N√§herungsverfahren leiten eine theoretische Verteilung f√ºr verschiedene Teststatistiken ab. Tests, die auf solchen N√§herungen basieren, haben h√§ufig starke Annahmen √ºber die Daten, wie dass die Daten normalverteilt sein m√ºssen, oder dass der Datensatz gro√ü sein muss, damit die N√§herung stimmt.Heute ist f√ºr die meisten unserer Analysen ausreichend Rechenkapazit√§t vorhanden. Daher m√ºssen wir nicht mehr auf solche N√§herungen zur√ºckgreifen, sondern k√∂nnen Computersimulationen benutzen. Diese Computersimulationen setzt man ein, um bei Hypothesentests Daten unter der Nullhypothese zu generieren. Abbildung 9.1 zeigt das allgemeine Vorgehen, wie es f√ºr jeden beliebigen Test g√ºltig ist. Dieses Vorgehen kann man auf folgende Schritte ‚Äúrunterkochen‚Äù:Teststatistik aus Stichprobe berechnen\nWir haben eine Stichprobe (data der Abbildung 9.1), die wir mithilfe einer Teststatistik zusammen fassen. Wir nennen diese Teststatistik \\(\\sigma^*\\). Sie kann z.B. der Mittelwert der Differenzen zwischen einer Behandlung und einer Kontrolle einem Experiment sein.Nullhypothese formulieren\nWir denken gut √ºber unsere Forschungsfrage nach und √ºberlegen uns, welches Modell besten die Nullhypothese \\(H_0\\), also eine Situation ohne jeglichen Effekt, wiedergibt. Im Falle des Mittelwerts ist es eine Welt, der der besagte Mittelwert Null ist, es also keinen Behandlungseffekt unserem Experiment gibt. Das Modell f√ºr \\(H_0\\) kann eine Permutation der Daten sein (Permutationstests) oder aus einer theoretischen Verteilung stammen (z.B. Normalverteilung). Es kann auch ein richtig kompliziertes Modell sein. Letzteres ist nicht Bestandteil dieses Kurses.Simulation der Daten unter der Nullhypothese\nWir simulieren Daten aus diesem Modell ohne Effekt, d.h. Daten unter der Nullhypothese, und berechnen aus jedem simulierten Datensatz dieselbe Teststatistik wie aus der echten Stichprobe.Berechnen der Stichprobenverteilung\nDie vielen simulierten Teststatistiken ergeben eine Stichprobenverteilung.Vergleich der beobachteten Teststatistik mit der Stichprobenverteilung ‚Äì Entscheidung\nNun k√∂nnen wir die aus unseren Daten berechnete Teststatistik \\(\\sigma^*\\) mit der Stichprobenverteilung der Teststatistik der simulieren Daten vergleichen. Wir entscheiden, ob \\(\\sigma^*\\) der Welt ohne Effekt, also unter der Nullhypothese, h√§ufig oder eher selten vorkommt. Der \\(p\\)-Wert gibt , wie wahrscheinlich es ist, unter der Nullhypothese einen Effekt zu beobachten, der mindestens extrem ist, wie derjenige, den wir durch unsere Daten errechnet haben, sprich mindestens extrem wie \\(\\sigma^*\\). Wenn \\(\\sigma^*\\) selten vorkommt, der \\(p\\)-Wert also klein ist, verwerfen wir die Nullhypothese und sagen, dass es einen Effekt gibt. Mit anderen Worten, es ist dann unwahrscheinlich, dass das beobachtete \\(\\sigma^*\\) auf den Zufall zur√ºckzuf√ºhren ist.Diese Schritte gelten wirklich f√ºr jeden beliebigen Test. Daher kann man verallgemeinert sagen, dass es nur einen Test (ein Testframework) gibt.\nAbbildung 9.1: Logik hinter den Hypothesentests aus der Sicht der modernen Datenanlyse (Quelle: http://allendowney.blogspot.com/2016/06/--still--one-test.html, Nutzung mit Erlaubnis des Autors Prof.¬†Allen Downey).\n","code":""},{"path":"hypothesentsts.html","id":"workflow-in-infer","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.2 Workflow in infer","text":"Das Paket infer bietet ein einheitliches Framework f√ºr Hypothesentests (Abbildung 9.2). Es hat 4 Verben, die den oben beschriebenen Prozess der Hypothesentests vereinheitlichen und ein Verb f√ºr die Visualisierung der Ergebnisse:specify() Variablen festlegenhypothesize() Nullhypothese definierengenerate() Daten unter der Nullhypothese generierencalculate() Stichprobenverteilung (d.h. Verteilung der Teststatistik) berechnenvisualize() Stichprobenverteilung darstellenMit get_p_value kann man den \\(p\\)-Wert berechnen und mit shade_p_value diesen darstellen lassen.\nAbbildung 9.2: Verallgemeinertes Vorgehen bei Hypothesentests (Quelle: https://infer.netlify.app/).\n","code":""},{"path":"hypothesentsts.html","id":"stadt--und-landbewohner-in-werdeschlau","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.2.1 Stadt- und Landbewohner in Werdeschlau","text":"Wir m√∂chten gerne wissen, ob sich unter den Studierenden Werdeschlau genauso viele Stadt- wie Landbewohner gibt oder ob sich die Anteile der Stadt- und Landbewohner unterscheiden.Zun√§chst laden wir die n√∂tigen Bibliotheken.Wir simulieren erneut unsere Grundgesamtheit.Nun befragen wir 200 Studierende.Wir berechnen den Anteil der Stadtbewohner der Befragung.Die Nullhypothese und die Alternativhypothese lauten:\\(H_0\\): es gibt keinen Unterschied der Anzahl der Stadt- und Landbewohner, d.h. Anteil der Stadtbewohner \\(p = 0.5\\).\\(H_A\\): Anteil der Stadtbewohner \\(p \\neq 0.5\\)Wir setzten das Ganz jetzt mit infer um.Das Paket infer setzt generate() die Art der Simulation automatisch (bootstrap, simulate oder permute). F√ºr sogen. Punkthypothesen null = \"point\" bei kategoriellen Variablen, z.B. \\(H_0\\): Anteil der Stadtbewohner = 0.5, simuliert generate() neue Daten mit Hilfe der Funktion sample() und nutzt die hypothesize() definierten Anteil p als Wahrscheinlichket f√ºr success. D.h. unserem Fall simuliert generate() 10000 neue Stichproben mit der Wahrscheinlichkeit von 0.5 f√ºr Stadtbewohner (success = 'stadt') wie von \\(H_0\\) verlangt üòÑ.Wir sehen uns die Stichprobenverteilung .Ende berechnen wir den \\(p\\)-Wert.Der \\(p\\)-Wert ist sehr klein, es ist also unwahrscheinlich, den Anteil von Stadtbewohnern von 0.62 zu beobachten, wenn wirklich gleich viele Stad- und Landbewohner sind, der Anteil also 0.5 betr√§gt. Daher schlie√üen wir, dass das Verh√§ltnis von Stadt- zu Landbewohnern ist nicht eins zu eins ist.","code":"\nlibrary(tidyverse)\nlibrary(infer)\nset.seed(123)\n\nstudent_id <- 1:12000\n  \nanreise <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n             runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngeschlecht <- sample(c('m', 'w'), size = 12000, replace = TRUE)\n\nwohnort <- sapply(anreise, function(x) {\n  if(x < 30) 'stadt'\n  else 'land'\n})\n\nverkehrsmittel <- sapply(anreise, function(x) {\n  if(x <= 10) 'zu_fuss'\n  else if(x > 10 & x <= 15) sample(c('zu_fuss', 'fahrrad'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'fahrrad', 'auto'), size = 1)\n  else sample(c('bus', 'auto'), size = 1)\n})\n\nzeit_bib <- 5 * 60 - 0.7 * anreise + rnorm(length(anreise), 0, 20)\n\ngrundgesamtheit <- tibble(student_id, geschlecht, wohnort, verkehrsmittel, anreise, zeit_bib)\n\ndatatable(grundgesamtheit, options = list(scrollX = T)) %>%\n  formatRound(c('zeit_bib', 'anreise'), 1)\nset.seed(345)\n\nbefragung_size <- 200\n\nbefragung <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = 1)\n\ndatatable(befragung, options = list(scrollX = T)) %>%\n  formatRound(c('zeit_bib', 'anreise'), 1)\nprop_hat <- befragung %>% \n  specify(response = wohnort, success = \"stadt\") %>%\n  calculate(stat = \"prop\")\n\nprop_hat## Response: wohnort (factor)\n## # A tibble: 1 √ó 1\n##    stat\n##   <dbl>\n## 1  0.62\nset.seed(123)\n\nnull_distn <- befragung %>%\n  specify(response = wohnort, success = \"stadt\") %>%\n  hypothesize(null = \"point\", p = .5) %>%\n  generate(reps = 10000) %>%\n  calculate(stat = \"prop\")## Setting `type = \"draw\"` in `generate()`.\nvisualize(null_distn) +\n  shade_p_value(obs_stat = prop_hat, direction = \"two-sided\")\nnull_distn %>%\n  get_p_value(obs_stat = prop_hat, direction = \"two-sided\")## # A tibble: 1 √ó 1\n##   p_value\n##     <dbl>\n## 1  0.0008"},{"path":"hypothesentsts.html","id":"pr√§ferenzen-f√ºr-den-wohnort","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.2.2 Pr√§ferenzen f√ºr den Wohnort","text":"Haben weibliche und m√§nnliche Studierende unterschiedliche Pr√§ferenzen f√ºr den Wohnort? Das ist ein √§hnlicher Fall wie der Einf√ºhrung zu Hypothesentests, als wir uns mit den Bef√∂rderungschancen von Frauen und M√§nnern den 70ern befasst haben. Wir haben hier zwei kategoriale Variablen, n√§mlich wohnort und geschlecht und wollen wissen, ob die beiden miteinander zusammenh√§ngen. Daher lauten unsere Nullhypothese und Alternativhypothese:\\(H_0\\): wohnort und geschlecht sind unabh√§ngig.\\(H_A\\): wohnort und geschlecht h√§ngen zusammen.Wir berechnen zun√§chst den Unterschied zwischen Stadtbewohnern nach Geschlecht.Wir basieren unseren Test auf Permutation, d.h. wir permutieren mehrfach eine der Variablen und berechnen die Differenzen den Anteilen der Stadtbewohner je nach Geschlecht f√ºr jede Permutation. Die Permutation wird von generate() automatisch richtig gew√§hlt.Nun plotten wir die Stichprobenverteilung, die auf Permutation basiert, und f√§rben den \\(p\\)-Wert ein. Der rote Balken zeigt der Wert der Statistik den Daten, d_hat.Der \\(p\\)-Wert betr√§gtDer \\(p\\)-Wert ist auch diesem Fall klein. Wir w√ºrden also wie im oberen Beispiel schlie√üen, dass es unwahrscheinlich ist, dass die Unterschiede zwischen M√§nnern und Frauen bei der Pr√§ferenz des Wohnorts zuf√§llig sind.dieser Stelle ein kleiner Ausflug die Welt der statistischen Signfikanz. Sehr h√§ufig wird f√ºr die Entscheidung, ob eine Nullhypothese abgelehnt wird, ein willk√ºrlicher Schwellenwert, das Signifikanzniveau \\(\\alpha\\) (meistens 5%) genutzt. Wenn der \\(p\\)-Wert darunter liegt, wird die Nullhypothese verworfen, ansonsten wird sie beibehalten. Es gibt also eine Dichotomisierung, eine Klassifizierung der \\(p\\)-Werte statistisch signifikant und statistisch nicht signifikant. Dieses Vorgehen wird seit vielen Jahren stark kritisiert (Wasserstein, Schirm, Lazar 2019). Und unserem Beispiel wird auch klar, warum. Hier ist der \\(p\\)-Wert etwas gr√∂√üer als das Signifikanzniveau und wir m√ºssten die Nullhypothese beibehalten, wegen einer winzigen √úberschreitung von 0.42%. Das macht wissenschaftlich keinen Sinn und ist extrem unbefriedigend. Zudem sorgt die ‚ÄúJagd‚Äù nach signifikanten Ergebnissen f√ºr allerlei Missbrauch. Man k√∂nnte z.B. im Nachhinein das Signifikanzniveau h√∂her setzten. Das nennt man \\(p\\)-Hacking. Um diesem Missbrauch vorzubeugen und um mehr Informationen aus einem Hypothesentest zu ziehen, berichten Sie einfach den \\(p\\)-Wert und ordenen ihn ein. Ist ein Effekt, wie er den Daten beobachtet wurde, selten oder h√§ufig unter der Annahme der Nullhypothese?BTW, der wahre Unterschied aus der Grundgesamtheit zwischen den M√§nnern und Frauen ist extrem klein. Nur weil etwas signifikant oder nicht signifikant ist, ist es noch lange nicht relevant. Das zu beurteilen, braucht es Fachwissen (domain knowledge) und eine richtige Einordnung der Ergebnisse. Daher ist die Berechnung von Effektgr√∂√üen (s. n√§chstes Kapitel) sehr viel spannender als das Durchf√ºhren von Hypothesentests.\\(p\\)-Werte werden h√§ufig miss- oder √ºberinterpretiert. Es geht weit, dass nur wissenschaftliche Ergebnisse mit signifikanten Ausg√§ngen bei Hypothesentests als wertvoll und publizierbar angesehen werden. Davor kann man nur dringend warnen. Diese Einstellung f√ºhrt zur Verzerrung der wissenschaftlichen Ergebnisse. Ich lade Sie ein, mehr dazu bei Wasserstein Lazar (2016) und im Kapitel 9.6 Ismay Kim (2021) nachzulesen.","code":"\nd_hat <- befragung %>% \n  specify(wohnort ~ geschlecht, success = \"stadt\") %>%\n  calculate(stat = \"diff in props\", order = c(\"w\", \"m\"))\n\nd_hat## Response: wohnort (factor)\n## Explanatory: geschlecht (factor)\n## # A tibble: 1 √ó 1\n##    stat\n##   <dbl>\n## 1 0.142\nset.seed(123)\n\nnull_distn <- befragung %>%\n  specify(wohnort ~ geschlecht, success = \"stadt\") %>%\n  hypothesize(null = \"independence\") %>% \n  generate(reps = 10000) %>% \n  calculate(stat = \"diff in props\", order = c(\"w\", \"m\"))## Setting `type = \"permute\"` in `generate()`.\nnull_distn## Response: wohnort (factor)\n## Explanatory: geschlecht (factor)\n## Null Hypothesis: independence\n## # A tibble: 10,000 √ó 2\n##    replicate     stat\n##        <int>    <dbl>\n##  1         1 -0.0381 \n##  2         2 -0.0180 \n##  3         3  0.0221 \n##  4         4  0.00201\n##  5         5 -0.0982 \n##  6         6  0.0221 \n##  7         7  0.0622 \n##  8         8  0.0421 \n##  9         9  0.00201\n## 10        10 -0.0581 \n## # ‚Ä¶ with 9,990 more rows\nvisualize(null_distn) +\n  shade_p_value(obs_stat = d_hat, direction = \"two-sided\")\nnull_distn %>%\n  get_p_value(obs_stat = d_hat, direction = \"two-sided\")## # A tibble: 1 √ó 1\n##   p_value\n##     <dbl>\n## 1  0.0542\ngrundgesamtheit %>% \n  specify(wohnort ~ geschlecht, success = \"stadt\") %>%\n  calculate(stat = \"diff in props\", order = c(\"w\", \"m\"))## Response: wohnort (factor)\n## Explanatory: geschlecht (factor)\n## # A tibble: 1 √ó 1\n##       stat\n##      <dbl>\n## 1 -0.00930"},{"path":"hypothesentsts.html","id":"lesestoff-9","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.3 Lesestoff","text":"Kapitel 9.3 bis 9.6 Ismay Kim (2021)","code":""},{"path":"hypothesentsts.html","id":"aufgaben-7","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.4 Aufgaben","text":"","code":""},{"path":"hypothesentsts.html","id":"sind-anreisezeit-und-zeit-in-der-bibliothek-korreliert","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.4.1 Sind Anreisezeit und Zeit in der Bibliothek korreliert?","text":"F√ºhren Sie einen Hypothesentest durch, ob die Anreisezeit und die Zeit der Bibliothek (zeit_bib) korreliert sind. Formulieren Sie die Null- und die Alternativhypothese mit Ihren eigenen Worten, bevor Sie den Test durchf√ºhren.Tipps: specify(anreise ~ zeit_bib) und calculate(stat = \"correlation\"). Sehr hilfreich dazu ist auch die Webseite von infer (s.u.).","code":""},{"path":"hypothesentsts.html","id":"h√§ngt-die-wahl-des-verkehrsmittels-vom-wohnort-ab","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.4.2 H√§ngt die Wahl des Verkehrsmittels vom Wohnort ab?","text":"Vielleicht k√∂nnen Sie sich noch erinnern, dass wir einer der fr√ºheren Stunden die Kontingenztabelle durchgenommen haben (s. Vorlesung Zusammenhangsma√üe). einer solchen Tabelle fasst man die H√§ufigkeiten von zwei kategoriellen Variablen zusammen. Anhand von einer Tabelle kann man entscheiden, ob es einen Zusammenhang zwischen diesen beiden Variablen gibt. Die Statistik, die man f√ºr eine Tabelle ausrechnet, hei√üt Kontingenzkoeffizient. einem Hypothesentest, genannt \\(\\chi^2\\)-Test, kann man √ºberpr√ºfen, ob dieser Zusammenhang zuf√§llig ist.√úberpr√ºfen Sie nun, ob es einen Zusammenhang gibt zwischen der Wahl des Verkehrsmittels und dem Wohnort der befragten Studierenden. Tipps: specify(formula = wohnort ~ verkehrsmittel) und calculate(stat = \"Chisq\"). Auch hier lohnt ein Blick auf die Webseite von infer (s.u.).","code":""},{"path":"hypothesentsts.html","id":"ihre-arbeit-einreichen-5","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.5 Ihre Arbeit einreichen","text":"Speichern Sie Ihr Notebook ab und laden Sie nur die .Rmd Datei vom Server.Laden Sie Ihre .Rmd Datei ILIAS hoch. Beachten Sie die Deadline!Sie erhalten die Musterl√∂sung nach dem Hochladen.","code":""},{"path":"hypothesentsts.html","id":"weiterf√ºhrende-literatur-und-videos-1","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.6 Weiterf√ºhrende Literatur und Videos","text":"Webseite von infer: https://infer.netlify.app/Vortrag des Autors von infer.","code":""},{"path":"effekte.html","id":"effekte","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","text":"\nUnterschied zwischen einem Hypothesentest und der Sch√§tzung von\nEffekten erkl√§ren\nBei Hypothesentests wird h√§ufig ein willk√ºrlich festgelegter Schwellenwert, das Signifikanzniveau, h√§ufig 5%, verwendet. Liegt der \\(p\\)-Wert darunter, wird die Nullhypothese verworfen. Liegt er dar√ºber, wird sie beibehalten.Dieses starre Konzept der statistischen Signifikanz, bei dem ein willk√ºrlich gesetzter Grenzwert \\(\\alpha\\) dar√ºber entscheidet, ob ein Ergebnis weiter beachtet wird oder nicht, wird seit Jahrzehnten kritisiert (Wasserstein, Schirm, Lazar 2019). Es ist schlie√ülich auch wirklich schwer zu verstehen, warum \\(p = 0.049\\) qualitativ etwas anderes aussagen soll als \\(p = 0.051\\). Und machen wir mit \\(p = 0.05\\)? Die beste L√∂sung ist, wie von der ASA (American Statistical Association) vorgeschlagen, den Begriff ‚Äúsignifikant‚Äù nicht mehr zu verwenden. Stattdessen berichten Sie den berechneten \\(p\\)-Wert und interpretieren ihn im wissenschaftlichen Kontext (Wasserstein, Schirm, Lazar 2019).Ich w√ºrde nicht weit gehen, statistische Tests abschaffen zu wollen. Sie sind manchmal n√ºtzlich, aber auch nicht h√§ufiger als manchmal. Allerdings kann ich auch aus eigener Praxis sagen, dass h√§ufig statistische Signifikanz mit wissenschaftlicher Relevanz verwechselt wird. Daher werden wir uns diesem Kapitel damit besch√§ftigen, wie man die relevanten Aspekte, n√§mlich die Gr√∂√üe des Effekts, aus den beobachteten Daten sch√§tzen kann.","code":""},{"path":"effekte.html","id":"gr√∂√üe-des-effekts","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.1 Gr√∂√üe des Effekts","text":"Daten sind immer mit Unsicherheiten behaftet. Modelle und Tests haben Annahmen, z.B. dass die gew√§hlte Statistik oder das Modell der Nullhypothese f√ºr die Forschungsfrage geeignet sind. Diese Unsicherheit gilt es richtig zu erfassen und zu berichten. Daher ist es wichtig, f√ºr jede Sch√§tzung entweder ein Konfidenzintervall anzugeben oder den Standardfehler (Wasserstein, Schirm, Lazar 2019).Wir werden zum Sch√§tzen von Effekten f√ºr einfache statistische Analysen das Paket dabestr (https://github.com/ACCLAB/dabestr) benutzen. Es bietet die M√∂glichkeit, Effekte zu sch√§tzen, Bootstrap-Konfidenzintervalle zu berechnen und das Ganze sehr ansprechenden Grafiken zu visualisieren. Zu dem Paket gibt es eine Publikation, Ho et al. (2019), die ich sehr empfehlen kann. Konfidenzintervalle und die (einige einfache) Effekte k√∂nnen wir auch mit infer berechnen. Der gr√∂√üte Vorteil von dabestr sind die tollen Grafiken.Effekte ist ein sehr allgemeiner Begriff. unterschiedlichen Kontexten und F√§chern werden unterschiedliche Formeln daf√ºr verwendet. dabestrbietet eine Reihe davon (s.u.). Im Zweifelsfall lesen Sie nach, wie ‚ÄúEffekte‚Äù f√ºr Ihre Fragestellung definiert werden.","code":""},{"path":"effekte.html","id":"sch√§tzen-des-effekts-mit-dem-bootstrap-dabestr","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.2 Sch√§tzen des Effekts mit dem Bootstrap ‚Äì dabestr","text":"Wir laden die n√∂tigen Bibliotheken.und holen uns wieder unsere Lieblingsgrundgesamtheit und Befragung üòÑ.Wir w√ºrden gerne den Unterschied der Arbeitszeit der Bibliothek zwischen M√§nnern und Frauen kennen. D.h. wir m√∂chten diesen Unterschied sch√§tzen und auch die Unsicherheit der Sch√§tzung quantifizieren. Ich hoffe, der Wortlaut kommt Ihnen bekannt vor ü§ì.Die wichtigste Funktion dabestr hei√üt dabest(). Sie erstellt einen tidy tibble im richtigen Format f√ºr die Sch√§tzung und das Bootstrap. Aktuell k√∂nnen Sie mit dabestr folgende Effekte sch√§tzen:Differenz zwischen Mittelwerten von Gruppen mit mean_diff().Differenz zwischen Medianen von Gruppen mit median_diff().Cohen‚Äôs d mit cohens_d(). Cohen‚Äôs d ist die Differenz zwischen Mittelwerten geteilt durch die Standardabweichung der Daten.Hedges‚Äô g mit hedges_g(). √Ñhnlich wie Cohen‚Äôs d, aber etwas besser f√ºr kleine Datens√§tze.Cliff‚Äôs delta mit cliffs_delta(). Entwickelt f√ºr ordinalskalierte Daten, √ºberpr√ºft, wie oft Datenpunkte einer Gruppe gr√∂√üer sind als Datenpunkte einer Vergleichsgruppe.Wir stellen unsere Daten mithilfe von dabest() zusammen. Der Parameter idx gibt , wie die Differenz berechnet wird, n√§mlich M√§nner - Frauen.Wir sehen uns mit dabest() erstellten Datensatz .Wir berechnen die Unterschiede Mittelwerten von zeit_bib zwischen den Geschlechtern und bootstrappen einem Schritt.Sie bekommen die R√ºckmeldung, dass ungepaarte Differenzen zwischen den Gruppen ausgerechnet wurden. Ungepaart bedeutet, dass die Frauen und M√§nner statistisch nicht verbunden sind. Etwas anderes w√§re es, wenn man dieselben Studierenden wiederholt befragen w√ºrde (und dazwischen z.B. etwas unternehmen w√ºrde, um deren Meinung zu beeinflussen). Weiterhin zeigt Ihnen dabest auch gleich die Bootstrap-Konfidenzintervalle.Die Sch√§tzung des Unterschieds stellen wir einem Gardner-Altman-Plot dar (Gardner Altman 1986). Wir beschriften die Grafik gleich sinnvoll und schalten die Legende ab, da sie diesem Fall redundant ist.der Abbildung k√∂nnen Sie mehrere Dinge sehen:alle gemessenen Punkte (Zeiten der Bibliothek). D.h. Sie k√∂nnen die Streuung der Daten erkennen. Das ist sehr informativ.die Sch√§tzung der mittleren Differenz (dicker schwarzer Punkt) und als graue Fl√§che die Stichprobenverteilung aus dem Bootstrap.das 95%-Konfidenzintervall f√ºr die Sch√§tzung (dicker schwarzer vertikaler Strich)Wir k√∂nnen nun erkennen, dass die Sch√§tzung recht genau ist, da das Konfidenzintervall schmal ist. Frauen scheinen mehr Zeit der Bibliothek zu verbringen, da die gesch√§tzte Differenz negativ ist. Das k√∂nnen wir genau abfragen, mit mean_diff_zeit_bib$result$difference. Das sind -4.4 Minuten. Dieser Unterschied ist f√ºr alle praktischen Belange irrelevant. Das Konfidenzintervall umfasst auch die Null, sodass kein Unterschied auch eine plausible Sch√§tzung ist. Somit w√ºrden wir schlie√üen, dass es nicht genug Evidenz f√ºr einen relevanten Unterschied der Arbeitszeit der Bibliothek zwischen M√§nnern und Frauen gibt.","code":"\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(dabestr)\nset.seed(123)\n\nstudent_id <- 1:12000\n  \nanreise <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n             runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngeschlecht <- sample(c('m', 'w'), size = 12000, replace = TRUE)\n\nwohnort <- sapply(anreise, function(x) {\n  if(x < 30) 'stadt'\n  else 'land'\n})\n\nverkehrsmittel <- sapply(anreise, function(x) {\n  if(x <= 10) 'zu_fuss'\n  else if(x > 10 & x <= 15) sample(c('zu_fuss', 'fahrrad'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'fahrrad', 'auto'), size = 1)\n  else sample(c('bus', 'auto'), size = 1)\n})\n\nzeit_bib <- 5 * 60 - 0.7 * anreise + rnorm(length(anreise), 0, 20)\n\ngrundgesamtheit <- tibble(student_id, geschlecht, wohnort, verkehrsmittel, anreise, zeit_bib)\n\ndatatable(grundgesamtheit, options = list(scrollX = T)) %>%\n  formatRound(c('zeit_bib', 'anreise'), 1)\nset.seed(345)\n\nbefragung_size <- 200\n\nbefragung <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = 1)\n\ndatatable(befragung, options = list(scrollX = T)) %>%\n  formatRound(c('zeit_bib', 'anreise'), 1)\ndiff_zeit_bib <- befragung %>%\n  dabest(x = geschlecht, y = zeit_bib, \n         idx = c('w', 'm'), \n         paired = FALSE)\ndiff_zeit_bib## dabestr (Data Analysis with Bootstrap Estimation in R) v0.3.0\n## =============================================================\n## \n## Good morning!\n## The current time is 07:42  on Dienstag Mai 31, 2022.\n## \n## Dataset    :  .\n## The first five rows are:\n## # A tibble: 5 √ó 7\n## # Groups:   replicate [1]\n##   replicate student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##       <int>      <int> <fct>      <chr>   <chr>            <dbl>    <dbl>\n## 1         1       1623 m          stadt   zu_fuss           7.06     299.\n## 2         1       9171 m          stadt   fahrrad          11.3      278.\n## 3         1      10207 w          land    bus             107.       199.\n## 4         1       3506 w          stadt   bus              25.0      326.\n## 5         1       8892 w          stadt   bus              28.1      259.\n## \n## X Variable :  geschlecht\n## Y Variable :  zeit_bib\n## \n## Effect sizes(s) will be computed for:\n##   1. m minus w\nset.seed(123)\n\nmean_diff_zeit_bib <- diff_zeit_bib %>%\n  mean_diff(reps = 10000)\n\nmean_diff_zeit_bib## dabestr (Data Analysis with Bootstrap Estimation in R) v0.3.0\n## =============================================================\n## \n## Good morning!\n## The current time is 07:42  on Dienstag Mai 31, 2022.\n## \n## Dataset    :  .\n## X Variable :  geschlecht\n## Y Variable :  zeit_bib\n## \n## Unpaired mean difference of m (n = 105) minus w (n = 95)\n##  -4.41 [95CI  -12.9; 4.2]\n## \n## \n## 10000 bootstrap resamples.\n## All confidence intervals are bias-corrected and accelerated.\nplot(mean_diff_zeit_bib, color.column = geschlecht, rawplot.ylabel = 'Zeit in der Bibliothek (min)', effsize.ylabel = 'Ungepaarte mittlere Differenz', show.legend = F)"},{"path":"effekte.html","id":"welche-fragen-soll-ich-mir-bei-der-analyse-stellen","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.3 Welche Fragen soll ich mir bei der Analyse stellen?","text":"Bei der statistischen Analyse Ihrer Daten sollten Sie sich vom ‚Äústatistischen Denken‚Äù leiten lassen. Wasserstein, Schirm, Lazar (2019) beschreibt es alsAccept uncertainty. thoughtful, open, modest.Denken Sie immer daran, dass Unsicherheit jedem Forschungsunterfangen innewohnt. Wenn Sie Ihre Daten interpretieren, stellen Sie sich folgende Fragen (Anderson 2019):Welche praktische Bedeutung hat meine Sch√§tzung des Effekts? Ist er √ºberhaupt relevant?Wie pr√§zise ist die Sch√§tzung?Passt das Modell zu meiner Forschungsfrage? Ist es korrekt formuliert?","code":""},{"path":"effekte.html","id":"was-dabest-nicht-kann-besorgt-infer","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.4 Was dabest nicht kann, besorgt infer","text":"Aktuell kann dabestr nicht mit Anteilen (proportions) arbeiten. Daher werden wir bei solchen Aufgaben auf unser bew√§hrtes Framework infer zur√ºckgreifen.","code":""},{"path":"effekte.html","id":"lesestoff-10","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.5 Lesestoff","text":"Ho et al. (2019)","code":""},{"path":"effekte.html","id":"aufgaben-8","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.6 Aufgaben","text":"","code":""},{"path":"effekte.html","id":"bodenverdichtung-revisited","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.6.1 Bodenverdichtung, revisited","text":"Wir kommen zur√ºck zu Daten aus Aufgabe B.3.3. Wie stark hat sich die Lagerungsdichte auf den befahrenen Feldern ver√§ndert? Sch√§tzen Sie den Effekt und geben Sie 95%-Konfidenzintervalle . Vergleichen Sie die Aussagen, die Sie mit dieser L√∂sung treffen k√∂nnen, mit der Aussage aus dem Hypothesentest.","code":""},{"path":"effekte.html","id":"anteil-von-besch√§ftigten-frauen-im-privaten-und-√∂ffentlichen-sektor-revisited","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.6.2 Anteil von besch√§ftigten Frauen im privaten und √∂ffentlichen Sektor, revisited","text":"Wir sehen uns erneut die Daten der Weltbank aus der Aufgabe B.3.4 . Diesmal interessiert und die Sch√§tzung des Anteils und ihr 95%-Bootstrap-Konfidenzintervall. Vergleichen Sie die Aussagen, die Sie mit dieser L√∂sung treffen k√∂nnen, mit der Aussage aus dem Hypothesentest.","code":""},{"path":"effekte.html","id":"ihre-arbeit-einreichen-6","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.7 Ihre Arbeit einreichen","text":"Speichern Sie Ihr Notebook ab und laden Sie nur die .Rmd Datei vom Server.Laden Sie Ihre .Rmd Datei ILIAS hoch. Beachten Sie die Deadline!Sie erhalten die Musterl√∂sung nach dem Hochladen.","code":""},{"path":"effekte.html","id":"weitere-infos","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.8 Weitere Infos","text":"Vignette von dabestr: https://cran.r-project.org/web/packages/dabestr/vignettes/using-dabestr.html","code":""},{"path":"regression.html","id":"regression","chapter":"Kapitel 11 Lineare Regression","heading":"Kapitel 11 Lineare Regression","text":"\nallgemeinen Aufbau eines Regressionsmodells erkl√§ren\n\nAnnahmen der linearen Normalregression benennen\n\neinfache lineare Regression selbst R durchf√ºhren\ndiesem Kapitel werden wir die statistische Modellierung einsteigen. Bisher haben Sie gelernt, wie man mithilfe von modernen Resamplingverfahren oder Simulationen Hypothesentests durchf√ºhrt und Konfidenzintervalle berechnet. Wir werden sp√§teren Kapiteln auch f√ºr die Modellierung Bootstrap verwenden.Es gibt im Wesentlichen zwei Gr√ºnde, warum man modelliert.Wir vermuten einen Zusammenhang zwischen Variablen und wollen diesen √ºberpr√ºfen (explikatives Modellieren).Wir wollen ein Modell zur Vorhersage entwickeln (pr√§diktives Modellieren).Wir werden uns diesem Kurs nur mit dem explikativen (erkl√§renden) Modellieren besch√§ftigen.","code":""},{"path":"regression.html","id":"begriff-regression","chapter":"Kapitel 11 Lineare Regression","heading":"11.1 Begriff Regression","text":"Woher kommt der Begriff Regression? Diesen pr√§gte Sir Francis Galton (1822-1911) (Fahrmeir, Kneib, Lang 2009). Galton interessierte sich unter anderem f√ºr den Zusammenhang zwischen der durchschnittlichen K√∂rpergr√∂√üe der Eltern und der K√∂rpergr√∂√üe ihrer erwachsenen Kinder. Leider war er nicht nur einer der V√§ter der Statistik, sondern auch ein Rassist.Galton stellte fest, dass Kinder von unterdurchschnittlich kleinen Eltern eher gr√∂√üer waren und umgekehrt, Kinder von √ºberdurchschnittlich gro√üen Eltern eher kleiner waren. Diesen Effekt nannte er Regression (R√ºckkehr) zur Mitte.","code":""},{"path":"regression.html","id":"idee-der-regression","chapter":"Kapitel 11 Lineare Regression","heading":"11.2 Idee der Regression","text":"Die Regression ist ein Modell, dass einen Zusammenhang zwischen Variablen analysiert. Wenn dieser Zusammenhang linear ist, dann nennt man das Modell lineare Regression. Wir werden uns ausschlie√ülich mit solchen linearen Modellen besch√§ftigen.Die lineare Regression untersucht also den linearen Zusammenhang zwischen den sogen. erkl√§renden Variablen und der Zielvariablen. Im historischen Beispiel von Galton gab es nur eine erkl√§rende Variable, n√§mlich die Durchschnittsgr√∂√üe der Eltern. Die Zielvariable war die zu erwartende Gr√∂√üe der Kinder. Es ging also nicht darum, die exakte Gr√∂√üe eines bestimmten Kindes zu berechnen, sondern den Einfluss der Durchschnittsgr√∂√üe der Eltern auf die zu erwartende Gr√∂√üe der Kinder. Es ging also um den systematischen Einfluss, nicht um bestimmte Eltern-Kind-Paare. Diese waren nur Stichproben. Sp√§testens hier sollte es klingeln, denn die Gr√∂√üe der Kinder ist somit eine Zufallsvariable.Die Zielvariable muss nicht immer stetig wie die K√∂rpergr√∂√üe sein. Sie kann bin√§r, kategorial oder eine Z√§hlvariable sein. Auch die erkl√§renden Variablen k√∂nnen stetig, bin√§r oder kategorial sein. Das macht die Regessionsmodelle sehr divers. Wir werden uns im Wesentlichen mit numerischen Zielvariablen besch√§ftigen.Wir k√∂nnen somit die Regression zusammenfassen:\nDie Regression ist ein Modell der Form\n\n\\[y = f(X) + \\varepsilon\\]\n\n\\(y\\): Zielvariable\n\n\\(f\\): Art des Zusammenhangs\n\n\\(X\\): Pr√§diktoren (erkl√§rende\nVariablen auch Kovariablen)\n\n\\(\\varepsilon\\): Fehlerterm\n\nWenn:\n\n\\(f\\) linear ist (Einfluss der\nPr√§diktoren addiert sich), spricht man von linearer Regression\n\n\\(X\\) nur ein Pr√§diktor ist,\nspricht man von einfacher Regression, sonst von multipler\nRegression\n\nModellkomponenten:\n\n\\(f(X)\\): systematische oder\ndeterministische Komponente\n\n\\(\\varepsilon\\): stochastische\nKomponente (St√∂rgr√∂√üe, Fehlerterm)\nEs geht bei der Regression also darum, die systematische Komponente zu modellieren. Der Zusammenhang zwischen Pr√§diktoren und der Zielvariablen ist nie exakt, es gibt also einen Fehlerterm. Die Zielgr√∂√üe ist eine Zufallsvariable, deren Verteilung von den Pr√§diktoren abh√§ngt.","code":""},{"path":"regression.html","id":"einfache-lineare-regression","chapter":"Kapitel 11 Lineare Regression","heading":"11.3 Einfache lineare Regression","text":"Bei einer einfachen linearen Regression gibt es nur einen Pr√§diktor. Der Zusammenhang zwischen der Zielvariablen und diesem Pr√§diktor ist linear. Somit hat das Model die Form einer Geraden. Eine Gerade kann man ja mithilfe des \\(y\\)-Achsenabschnitts und der Steigung beschreiben. Und genauso sieht das einfache lineare Regressionsmodell aus.\nGegeben sind Datenpaare: \\((y_i,x_i), \\quad =1,\\dots,n\\) zu\nmetrischen Variablen \\(y\\) und \\(x\\).\n\nDas Modell \\[y_i=\\beta_0 + \\beta_1x_i +\n\\varepsilon_i, \\qquad =1,\\dots,n.\\] hei√üt einfaches\nlineares Regressionsmodell, wenn die Fehler \\(\\varepsilon_1,\\dots, \\varepsilon_n\\)\nunabh√§ngig und identisch verteilt sind (iid) mit\n\n\\[\\mathrm{E}(\\varepsilon_i) = 0, \\qquad\n\\mathrm{Var}(\\varepsilon_i)=\\sigma^2.\\] Wenn zus√§tzlich gilt\n\\[\\varepsilon_i \\sim N(0,\\sigma^2)\\]\nd.h. die Residuen normalverteilt sind, sprechen wir von klassischer\nNormalregression.\n\n\\(\\beta_0\\) hei√üt \\(y\\)-Achsenabschnitt und \\(\\beta_1\\) Steigung des Modells.\n\\(\\varepsilon_i\\) steht f√ºr Fehler, die wir im Modell machen. Das sind die Unterschiede, genannt Residuen, zwischen dem, das Modell dem systematischen Teil (Geradengleichung) \\(\\beta_0 + \\beta_1x_i\\) ausrechnet und dem tats√§chlich gemessenen Wert \\(y_i\\).\\(\\mathrm{E}\\) steht f√ºr Erwartungswert. nennt man den theoretischen Mittelwert einer Zufallsvariablen. Und \\(\\mathrm{Var}\\) steht f√ºr Varianz. Beim einfachen linearen Regressionsmodell nimmt man also , dass die Fehler im Mittel Null sind. Sie werden nat√ºrlich nie alle Null sein, sondern sie werden variieren. Der Fehlerterm ist also eine Zufallsvariable, dessen Varianz fest sein soll. Eine feste Varianz nennt man Homoskedastizit√§t und die Fehler entsprechend homoskedastisch. Eine Varianz, die schwankt, bezeichnet man als Heteroskedastizit√§t. Das bedeutet unter anderem, dass die Fehler f√ºr kleine und gro√üe Werte im Modell √§hnlich sein m√ºssen. Um auf Galtons Beispiel zur√ºckzukommen, das Modell soll sowohl die Gr√∂√üe der gro√üen als auch der kleinen Kinder gleich gut erkl√§ren.","code":""},{"path":"regression.html","id":"beispiel-zusammenhang-zwischen-der-anreisezeit-und-der-arbeitszeit-in-der-bibliothek","chapter":"Kapitel 11 Lineare Regression","heading":"11.4 Beispiel: Zusammenhang zwischen der Anreisezeit und der Arbeitszeit in der Bibliothek","text":"Als Beispiel f√ºr eine einfache lineare Regression nutzen wir wieder unsere simulierten Daten. Zun√§chst laden wir die Bibliotheken. Die Bibliothek kntir brauchen wir f√ºr das Layouten der Tabellen.Wir generieren wieder unsere Lieblingsgrundgesamtheit.Und befragen 200 Studierende.","code":"\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(moderndive)\nlibrary(knitr)\nset.seed(123)\n\nstudent_id <- 1:12000\n  \nanreise <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n             runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngeschlecht <- sample(c('m', 'w'), size = 12000, replace = TRUE)\n\nwohnort <- sapply(anreise, function(x) {\n  if(x < 30) 'stadt'\n  else 'land'\n})\n\nverkehrsmittel <- sapply(anreise, function(x) {\n  if(x <= 10) 'zu_fuss'\n  else if(x > 10 & x <= 15) sample(c('zu_fuss', 'fahrrad'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'fahrrad', 'auto'), size = 1)\n  else sample(c('bus', 'auto'), size = 1)\n})\n\nzeit_bib <- 5 * 60 - 0.7 * anreise + rnorm(length(anreise), 0, 20)\n\ngrundgesamtheit <- tibble(student_id, geschlecht, wohnort, verkehrsmittel, anreise, zeit_bib)\n\ndatatable(grundgesamtheit, options = list(scrollX = T)) %>%\n  formatRound(c('zeit_bib', 'anreise'), 1)\nset.seed(345)\n\nbefragung_size <- 200\n\nbefragung <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = 1)\n\ndatatable(befragung, options = list(scrollX = T)) %>%\n  formatRound(c('zeit_bib', 'anreise'), 1)"},{"path":"regression.html","id":"modell-anpassen","chapter":"Kapitel 11 Lineare Regression","heading":"11.4.1 Modell anpassen","text":"Wir unterstellen einen linearen Zusammenhang zwischen zeit_bib und anreise und passen ein lineares Modell . Dazu nutzen wir die Funktion lm(). Sie braucht die Zielvariable und den Pr√§diktor, die Sie mit Tilde verbunden angeben. Das ist die sogen. formula (Formel, √§hnlich wie eine Matheformel). Die Tilde hatten wir schon √§hnlich bei der Bibliothek infer benutzt. Au√üerdem m√ºssen wir noch den Datensatz, dem die Variablen zu finden sind, benennen.","code":"\nlin_mod <- lm(zeit_bib ~ anreise, data = befragung)"},{"path":"regression.html","id":"modellergebnisse-ansehen","chapter":"Kapitel 11 Lineare Regression","heading":"11.4.2 Modellergebnisse ansehen","text":"Die Struktur eines solchen linearen Modellobjekts ist richtig kompliziert. Daher gibt es verschiedene Methoden, um aus diesem Objekt sinnvolle Information zu entnehmen.Als Erstes sehen wir uns die Zusammenfassung des Modells . Die nicht tidy-Form enth√§lt sehr viel Information, die man Anfang gar nicht braucht. Und sie gl√§nzt mit vielen Signifikanz-Sternchen, die wir liebsten gleich verbannen w√ºrden.Daher empfehle ich die tidy-Form. Wir nutzen die Funktion get_regression_table aus der Bibliothek moderndive, die intern auf die Bibliothek broom zugreift (https://cran.r-project.org/web/packages/broom/vignettes/broom.html). broom hilft, die Ausgabe des linearen Modells eine tidy-Form zu konvertieren. Die Funktion kable() aus der Bibliothek knitr layoutet die Tabelle.Sie sehen der ersten Spalte (Intercept) und anreise. Das sind der \\(y\\)-Achsenabschnitt \\(\\beta_0\\) und die Steigung \\(\\beta_1\\) des Modells. Das hei√üt, unser Modell lautet ausgeschrieben:\\[\\widehat{\\text{zeit_bib}_i} = 302.094 - 0.766 \\cdot \\text{anreise_i}\\]\nDer Index \\(\\) steht hier f√ºr die unterschiedlichen Studierenden, denn jede(r) hat nat√ºrlich eine eigene Anreise- und Arbeitszeit der Bibliothek. Wir lernen also, dass mit steigender Anreisezeit, die Arbeitszeit der Bibliothek sinkt. Und zwar k√∂nnen wir es sogar noch genauer sagen: mit jeder zus√§tzlichen Minute Anreisezeit, sinkt die Arbeitszeit der Bibliothek um 0.766 Minuten. Auf die √ºbrigen Spalten kommen wir sp√§ter zu sprechen.Welche Werte hat das Modell berechnet? Diese nennt man angepasste Werte (fitted) und man kann sie mit der Funktion fitted() abfragen. Wir sehen uns nur die ersten Eintr√§ge .Wir w√ºrden gerne diese angepassten Werte mit den echten gemessenen Werten, n√§mlich der tats√§chlichen Arbeitszeit der Bibliothek, vergleichen. Daf√ºr f√ºgen wir die gemessenen und die angepassten Werte einem tibble zusammen. Wir erstellen eine neue Spalte mit angepassten Werten und den Residuen, d.h. den Differenzen zwischen den gemessenen und den angepassten Werten.Nun k√∂nnen wir die Werte gegeneinander plotten und uns die Residuen ansehen. Diese sind als graue vertikale Linien zwischen den gemessenen und den angepassten Werten dargestellt. Die angepassten Werte liegen alle auf einer Geraden, das ist ja die Quintessenz eines linearen Modells. Zu jedem gemessenen Wert gibt es einen angepassten, modellierten Wert auf der Geraden. Das geom geom_abline zeichnet unsere Gerade.","code":"\nstr(lin_mod)## List of 12\n##  $ coefficients : Named num [1:2] 302.094 -0.766\n##   ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"anreise\"\n##  $ residuals    : Named num [1:200] 2.55 -15.31 -21.32 42.64 -21.19 ...\n##   ..- attr(*, \"names\")= chr [1:200] \"1\" \"2\" \"3\" \"4\" ...\n##  $ effects      : Named num [1:200] -3903 -312.5 -24.2 42.7 -21.2 ...\n##   ..- attr(*, \"names\")= chr [1:200] \"(Intercept)\" \"anreise\" \"\" \"\" ...\n##  $ rank         : int 2\n##  $ fitted.values: Named num [1:200] 297 293 220 283 281 ...\n##   ..- attr(*, \"names\")= chr [1:200] \"1\" \"2\" \"3\" \"4\" ...\n##  $ assign       : int [1:2] 0 1\n##  $ qr           :List of 5\n##   ..$ qr   : num [1:200, 1:2] -14.1421 0.0707 0.0707 0.0707 0.0707 ...\n##   .. ..- attr(*, \"dimnames\")=List of 2\n##   .. .. ..$ : chr [1:200] \"1\" \"2\" \"3\" \"4\" ...\n##   .. .. ..$ : chr [1:2] \"(Intercept)\" \"anreise\"\n##   .. ..- attr(*, \"assign\")= int [1:2] 0 1\n##   ..$ qraux: num [1:2] 1.07 1.05\n##   ..$ pivot: int [1:2] 1 2\n##   ..$ tol  : num 1e-07\n##   ..$ rank : int 2\n##   ..- attr(*, \"class\")= chr \"qr\"\n##  $ df.residual  : int 198\n##  $ xlevels      : Named list()\n##  $ call         : language lm(formula = zeit_bib ~ anreise, data = befragung)\n##  $ terms        :Classes 'terms', 'formula'  language zeit_bib ~ anreise\n##   .. ..- attr(*, \"variables\")= language list(zeit_bib, anreise)\n##   .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n##   .. .. ..- attr(*, \"dimnames\")=List of 2\n##   .. .. .. ..$ : chr [1:2] \"zeit_bib\" \"anreise\"\n##   .. .. .. ..$ : chr \"anreise\"\n##   .. ..- attr(*, \"term.labels\")= chr \"anreise\"\n##   .. ..- attr(*, \"order\")= int 1\n##   .. ..- attr(*, \"intercept\")= int 1\n##   .. ..- attr(*, \"response\")= int 1\n##   .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n##   .. ..- attr(*, \"predvars\")= language list(zeit_bib, anreise)\n##   .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n##   .. .. ..- attr(*, \"names\")= chr [1:2] \"zeit_bib\" \"anreise\"\n##  $ model        :'data.frame':   200 obs. of  2 variables:\n##   ..$ zeit_bib: num [1:200] 299 278 199 326 259 ...\n##   ..$ anreise : num [1:200] 7.06 11.34 106.81 24.97 28.11 ...\n##   ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language zeit_bib ~ anreise\n##   .. .. ..- attr(*, \"variables\")= language list(zeit_bib, anreise)\n##   .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n##   .. .. .. ..- attr(*, \"dimnames\")=List of 2\n##   .. .. .. .. ..$ : chr [1:2] \"zeit_bib\" \"anreise\"\n##   .. .. .. .. ..$ : chr \"anreise\"\n##   .. .. ..- attr(*, \"term.labels\")= chr \"anreise\"\n##   .. .. ..- attr(*, \"order\")= int 1\n##   .. .. ..- attr(*, \"intercept\")= int 1\n##   .. .. ..- attr(*, \"response\")= int 1\n##   .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n##   .. .. ..- attr(*, \"predvars\")= language list(zeit_bib, anreise)\n##   .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n##   .. .. .. ..- attr(*, \"names\")= chr [1:2] \"zeit_bib\" \"anreise\"\n##  - attr(*, \"class\")= chr \"lm\"\nsummary(lin_mod)## \n## Call:\n## lm(formula = zeit_bib ~ anreise, data = befragung)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -60.060 -14.063   0.836  15.662  64.151 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 302.09377    2.28190  132.39   <2e-16 ***\n## anreise      -0.76627    0.05112  -14.99   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 20.85 on 198 degrees of freedom\n## Multiple R-squared:  0.5316, Adjusted R-squared:  0.5292 \n## F-statistic: 224.7 on 1 and 198 DF,  p-value: < 2.2e-16\nget_regression_table(lin_mod) %>% kable()\nhead(fitted(lin_mod))##        1        2        3        4        5        6 \n## 296.6866 293.4026 220.2491 282.9618 280.5521 284.0451\nmodel_res <- befragung %>%\n  mutate(fitted = fitted(lin_mod), residuals = residuals(lin_mod)) \nggplot(model_res, aes(x = anreise, y = zeit_bib)) +\n  geom_segment(aes(xend = anreise, yend = fitted, lty = 'Residuen'), alpha = 0.2)  + \n  geom_abline(intercept = coef(lin_mod)[1], slope = coef(lin_mod)[2], color = \"lightblue\") +\n  geom_point(aes(col = 'observed')) +\n  geom_point(aes(y = fitted, col = 'fitted'), shape = 1, size = 2) +\n  labs(x = 'Anreisezeit (min)', y = 'Arbeitszeit in der Bibliothek (min)') +\n  scale_color_manual(name = '', values = c(observed = 'black', fitted = 'blue'), breaks = c('observed', 'fitted'), label = c('Gemessene Werte', 'Angepasste Werte')) +\n  scale_linetype_manual(name = '', values = ('Residuen' = 'solid')) +\n  theme(legend.position = 'bottom')"},{"path":"regression.html","id":"modellannahmen-√ºberpr√ºfen","chapter":"Kapitel 11 Lineare Regression","heading":"11.4.3 Modellannahmen √ºberpr√ºfen","text":"Bei der linearen Regression nehmen wir , dass der Zusammenhang zwischen der Zielvariablen und dem Pr√§diktor linear ist. Das k√∂nnen wir der oberen Grafik bereits erkennen. Weiterhin nehmen wir , dass die Residuen im Mittel um die Null schwanken und homoskedastisch sind, d.h. ihre Varianz ist gleich. Diese Annahmen m√ºssen wir √ºberpr√ºfen, bevor es ans Interpretieren der Modellparameter geht. Das macht man haupts√§chlich grafisch. Wir plotten die Residuen gegen die angepassten Werte.Diese Darstellung der Residuen gegen angepasste Werte nennt man Residualplot. Darin k√∂nnen wir erkennen, dass unsere Residuen um die Null schwanken. Die erste Annahme ist also schon einmal erf√ºllt. Die Homoskedastizit√§t ist nicht ganz erf√ºllt. Bei gr√∂√üeren angepassten Werten scheinen die Residuen st√§rker zu schwanken. Allerdings gibt es nur sehr wenige Werte < 250 Minuten. Daher ist es schwierig, daraus eine echte Heteroskedastizit√§t abzuleiten. Wichtig ist, dass wir keine systematische Zunahme oder Abnahme der Variabilit√§t beobachten und auch keine sonstigen Muster den Residuen. Die zweite Modellannahme k√∂nnen wir auch als erf√ºllt abhaken. Wir k√∂nnen plausibel davon ausgehen, dass die Residuen unabh√§ngig sind. Denn wir haben weder eine Zeitreihe gemessen, noch Leute mehrfach befragt.Die Zusammenfassung des linearen Modells bietet noch weiter Informationen.Hier gibt es noch die Spaltenstd_error: Standardfehler der Sch√§tzung des jeweiligen Modellparameters (intercept oder anreise)statistics: Wert der \\(t\\)-Statistik f√ºr einen Hypothesentest, der √ºberf√ºft, ob der jeweilige Modellparameter Null ist. Die Nullhypothese lautet \\(\\beta_0 = 0\\) f√ºr den \\(y\\)-Achsenabschnitt bzw. \\(\\beta_1 = 0\\) f√ºr die Steigung. Die Alternativhypothese lautet, dass der jeweilige Modellparameter ungleich Null ist.p-value: p-Wert des Hypothesentestslower_ci, upper_ci: unterer und oberer Wert des Konfidenzintervalls (Standardeinstellung 95%)F√ºr die Berechnung des Standardfehlers, des Hypothesentests und der Konfidenzintervalle wird, zus√§tzlich zu den oben genannten Annahmen, vorausgesetzt, dass die Residuen normalverteilt sind. Nur wenn sie es tats√§chlich sind, sind diese Berechnungen und der Hypothesentest korrekt, sonst m√∂glicherweise nicht. Deswegen m√ºssen wir, bevor wir diese Spalten interpretieren, √ºberpr√ºfen, ob die Residuen normalverteilt sind. Das machen wir mit einem QQ-Plot. Sie d√ºrfen auch noch einen formalen shapiro.test() machen, wenn Sie m√∂chten.Wenn die Residuen normalverteilt sind, dann liegen sie nahe der Geraden im QQ-Plot. Das ist hier der Fall. Die Annahme der Normalverteilung der Residuen ist also erf√ºllt und wir d√ºrfen den Standardfehler, den p-Wert und die Konfidenzintervalle interpretieren.","code":"\nggplot(data = model_res, aes(x = fitted, y = residuals)) + \n  geom_point() +\n  geom_hline(yintercept=0, col = 'red') + \n  labs(x = 'Angepasste Werte', y = 'Residuen')\nget_regression_table(lin_mod) %>% kable()\nggplot(model_res, aes(sample = residuals)) + \n  stat_qq() +\n  stat_qq_line(col = 'blue') +\n  labs(x = 'Quantile aus der Normalverteilung',\n       y = 'Qunatile der Daten')"},{"path":"regression.html","id":"modell-interpretieren","chapter":"Kapitel 11 Lineare Regression","heading":"11.4.4 Modell interpretieren","text":"unserem fiktiven Datensatz nimmt die Arbeitszeit der Bibliothek mit der Anreisezeit ab. Wir haben bereits die Sch√§tzungen des \\(y\\)-Achsenabschnitts (intercept) und der Steigung (anreise) interpretiert. Jetzt k√∂nnen wir zus√§tzlich sagen, wie gut wir gesch√§tzt haben. Der \\(y\\)-Achsenabschnitt gibt die Anreisezeit der Bibliothek , wenn die Anreisezeit Null w√§re. Das ist zwar die korrekte Interpretation, aber eine Extrapolation au√üerhalb unseres Messbereichs der Anreisezeit. Daher sollte man den \\(y\\)-Achsenabschnitt nicht √ºberinterpretieren. Er ist erst einmal ein Modellparameter, der gut gesch√§tzt wurde, mit einem Konfidenzintervall von [297.594, 306.594]. Ob die Studierenden tats√§chlich diese Zeit der Bibliothek verbringen w√ºrden, wenn sie gewisserma√üen auf dem Campus wohnen w√ºrden, ist eine Spekulation.Einfacher und sinnvoller ist die Interpretation der Steigung. Ihr Konfidenzintervall lautet [-0.867, -0.665]. Es ist eine gute Sch√§tzung, da das Konfidenzintervall schmal ist. Pro zus√§tzlicher Anreiseminute sinkt die Arbeitszeit der Bibliothek um einen Wert diesem Konfidenzintervall.Die Hypothesentests k√∂nnen Sie interpretieren, wie gewohnt, m√ºssen Sie aber nicht. Wie den vorherigen Kapiteln erw√§hnt, kann man auf den Begriff signifikant getrost verzichten. Die Sch√§tzung der Parameter und deren Konfidenzintervalle bringt sehr viel mehr Information.\nRegression beinhaltet eine gro√üe Familie Modellen.\n\nLineare Regression: linearer Einfluss der Kovariablen auf \\(y\\).\n\nAnnahmen: Residuen \\(\\varepsilon_i\\) sind iid mit \\[\\mathrm{E}(\\varepsilon_i) = 0, \\qquad\n\\mathrm{Var}(\\varepsilon_i)=\\sigma^2.\\]\n\nNormalregression zus√§tzlich: \\(\\varepsilon_i \\sim N(0,\\sigma^2)\\).\n\n√úberpr√ºfung der Annahmen (vor allem) grafisch.\n\nKonfidenzintervalle und Hypothesentests der Normalregression d√ºrfen\nnur dann interpretiert werden, wenn die Annahmen erf√ºllt sind.\n","code":""},{"path":"regression.html","id":"lesestoff-11","chapter":"Kapitel 11 Lineare Regression","heading":"11.5 Lesestoff","text":"Kapitel 5 Ismay Kim (2021)","code":""},{"path":"regression.html","id":"aufgaben-9","chapter":"Kapitel 11 Lineare Regression","heading":"11.6 Aufgaben","text":"","code":""},{"path":"regression.html","id":"kategorielle-variable-als-pr√§diktor","chapter":"Kapitel 11 Lineare Regression","heading":"11.6.1 Kategorielle Variable als Pr√§diktor","text":"Arbeiten Sie selbstst√§ndig das Beispiel Kapitel 5.2 Ismay Kim (2021) durch.","code":""},{"path":"regression-inferenz.html","id":"regression-inferenz","chapter":"Kapitel 12 Inferenz in linearer Regression","heading":"Kapitel 12 Inferenz in linearer Regression","text":"\nKonfidenzintervalle f√ºr Modellparameter berechnen\n\nZusammenfassung eines linearen Modells R interpretieren\n\nBestimmtheitsma√ü \\(R^2\\)\nerkl√§ren\n\nKonfidenzintervalle f√ºr nicht normalverteilte Residuen\nberechnen\nIm letzten Kapitel haben wir die einfache Normalregression kennen gelernt. Bevor wir die gesch√§tzten Parameter und deren Konfidenzintervalle interpretieren, m√ºssen wir die Annahmen der Normalregression √ºberpr√ºfen (vor allem grafisch). Diese Annahmen sind:Der Zusammenhang zwischen der abh√§ngigen und den erkl√§renden Variablen ist linear.Residuen sind unabh√§ngig.Residuen haben einen Mittelwert von Null und sind homoskedastisch (gleiche Varianz).Residuen sind normalverteiltIn diesem Kapitel besch√§ftigen wir uns mit folgenden Fragen:Wie sch√§tzt man die Parameter?Wie gut sind diese Sch√§tzungen?Wie gut ist das Modell?macht man, wenn die Residuen nicht normalverteilt sind?","code":""},{"path":"regression-inferenz.html","id":"zur√ºck-zum-beispiel-anreisezeit-und-arbeitszeit-in-der-bilbiothek","chapter":"Kapitel 12 Inferenz in linearer Regression","heading":"12.1 Zur√ºck zum Beispiel: Anreisezeit und Arbeitszeit in der Bilbiothek","text":"","code":""},{"path":"regression-inferenz.html","id":"parameter-und-kofidenzinetervalle-sch√§tzen","chapter":"Kapitel 12 Inferenz in linearer Regression","heading":"12.1.1 Parameter und Kofidenzinetervalle sch√§tzen","text":"Woher kommen die Sch√§tzungen der Modellparameter und deren Konfidenzintervalle unserem Modell? Dieses Kapitel basiert auf den theoretischen Herleitungen aus Fahrmeir, Kneib, Lang (2009).Eine sehr oft verwendete Methode, die Modellparameter zu sch√§tzen, hei√üt Methode der kleinsten Quadrate (KQ). Sie beruht auf der Idee, dass diejenigen Modellparameter, \\(y\\)-Achsenabschnitt und die Steigung, die besten sind, bei denen die Summe der quadrierten Residuen die kleinste ist. Formal schreibt man:\nGegeben sei das lineare Regressionsmodell\n\n\\[y_i=\\beta_0 + \\beta_1 x_i +\n\\varepsilon_i\\]\n\nUm die unbekannten Parameter \\(\\beta_0\\) und \\(\\beta_1\\) zu bestimmen, minimiere die Summe\nder quadrierten Abweichungen\n\n\\[\\mathrm{KQ}(\\beta_0,\n\\beta_1)=\\sum_{=1}^n (y_i - (\\beta_0 + \\beta_1 x_i))^2=\\sum_{=n}^n\n\\varepsilon_i^2  \\rightarrow \\operatorname*{min}_{\\beta_0,\\,\n\\beta_1}\\]\n\nDann hei√üt \\[\n\\boldsymbol{\\hat{\\beta}}=(\\hat{\\beta}_0, \\hat{\\beta}_1)\\]\n\nder Kleinste-Quadrate-Sch√§tzer f√ºr den Parametervektor \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)\\).\nEs geht also darum, die Residuen zu minimieren, d.h. die Abst√§nde zwischen den beobachteten und den im Modell vorhergesagten Werten klein wie m√∂glich zu machen.Man quadriert die Residuen, damit sowohl die positiven als auch die negativen gleich wichtig sind und sich der Summe nicht gegenseitig aufheben.Die Methode der kleinsten Quadrate ergibt folgende Formeln f√ºr die Sch√§tzung der Modellparameter einer Einfachregression:\\[\n\\begin{align*}\n\\hat{\\sigma}^2 &= \\frac{1}{n-2} \\sum_{=n}^n \\varepsilon_i^2 \\\\\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\\\\n\\hat{\\beta}_1 &= \\frac{\\sum^n_{=1} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum^n_{=1} (x_i - \\bar{x})^2}\n\\end{align*}\n\\]\nDie Residuen \\(\\hat{\\varepsilon}_i\\) berechnen sich als die Differenz zwischen den beobachteten und den im Modell berechneten Werten:\\[\\hat{\\varepsilon}_i = y_i - \\hat{y}_i\\]\nund die angepassten Werte als Punkte auf der Regressionsgeraden:\\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\]Die gesch√§tzten Modellparameter sind Zufallsvariablen. Denn wenn die Daten etwas anders ausfallen (Zufallsstichproben!), bekommen wir andere Sch√§tzungen f√ºr den \\(y\\)-Achsenabschnitt und die Steigung. Unter der Annahme, dass die Residuen normalverteilt sind, d.h.:\\[\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\]\ngilt f√ºr diese Zufallsvariablen, dass sie selbst normalverteilt sind:\\[\\hat{\\beta}_0 \\sim \\mathcal{N}(\\beta_0, \\sigma^2_{\\hat{\\beta}_0}),\n\\qquad \\hat{\\beta}_1 \\sim \\mathcal{N}(\\beta_1, \\sigma^2_{\\hat{\\beta}_1})\\]Ihre Varianzen \\(\\sigma^2_{\\hat{\\beta}_0}\\) und \\(\\sigma^2_{\\hat{\\beta}_1}\\) m√ºssen gesch√§tzt werden:\\[\\hat{\\sigma}_{\\hat{\\beta}_0} = \\hat{\\sigma} \\frac{\\sqrt{\\textstyle \\sum_{=1}^n x_i^2}}{\\sqrt{n \\textstyle \\sum_{=1}^n (x_i-\\bar{x})^2}}, \\quad\n\\hat{\\sigma}_{\\hat{\\beta}_1} = \\frac{\\hat{\\sigma}}{\\sqrt{\\textstyle \\sum_{=1}^n (x_i-\\bar{x})^2}}\\]Wenn die Residuen normalverteilt sind, kann mann zeigen, dass die sogen. standardisierten Sch√§tzer der \\(t\\)-Verteilung folgen:\\[\\frac{\\hat{\\beta}_0 - \\beta_0}{\\hat{\\sigma}_{\\hat{\\beta}_0}} \\sim t_{n-2}, \\quad \\frac{\\hat{\\beta}_1 - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta}_1}} \\sim t_{n-2}\\]\n\\(n-2\\) steht hier f√ºr die Anzahl der Freiheitsgrade der \\(t\\)-Verteilung. \\(n\\) ist die Anzahl der Datenpunkte im Modell.Somit wissen wir nun endlich, wie die Sch√§tzungen f√ºr den \\(y\\)-Achsenabschnitt und die Steigung verteilt sind. Mit diesem Wissen k√∂nnen wir Konfidenzintervalle f√ºr diese Modellparameter berechnen:\\[\\hat{\\beta}_0 \\pm \\hat{\\sigma}_{\\hat{\\beta}_0} t_{1-\\alpha/2, n-2}, \\qquad \\hat{\\beta}_1 \\pm \\hat{\\sigma}_{\\hat{\\beta}_1} t_{1-\\alpha/2, n-2}\\]F√ºr \\(\\alpha\\) setzt man passendes Konfidenzlevel ein, z.B. 5%, um das 95%-Konfidenzintervall zu erhalten.","code":"\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(knitr)\nlibrary(moderndive)\nlin_mod <- lm(zeit_bib ~ anreise, data = befragung)\n\nget_regression_table(lin_mod) %>% kable()"},{"path":"regression-inferenz.html","id":"wie-gut-ist-das-modell","chapter":"Kapitel 12 Inferenz in linearer Regression","heading":"12.1.2 Wie gut ist das Modell?","text":"Welcher Anteil der Streuung der Daten l√§sst sich durch die Regression erkl√§ren?Die Streuung im Modell besteht aus:\\[\n\\begin{align*}\n\\mathit{SQT} &= \\mathit{SQE} + \\mathit{SQR}\\\\\n\\sum^{n}_{= 1} (y_i-\\bar{y})^2 &= \\sum^{n}_{=1} (\\hat{y}_i - \\bar{y})^2 + \\sum^{n}_{=1} (y_i - \\hat{y}_i)^2\\\\\n\\end{align*}\n\\]mit \\(y_i\\): Messwerte, \\(\\bar{y}\\): Mittelwert, \\(\\hat{y}_i\\): angepasste Werte\\(\\mathit{SQT}\\) Sum squares total: Gesamtstreuung der Daten\\(\\mathit{SQE}\\) Sum squares explained: Streuung erkl√§rt vom Modell\\(\\mathit{SQR}\\) Sum sqaures residual: Residualstreuung (vom Modell nicht erkl√§rt)Die \\(\\mathit{SQE}\\) beschreibt die Variation der angepassten Werte um den Mittelwert der beobachteten Daten und \\(\\mathit{SQR}\\) entspricht dem nicht erkl√§rten Teil der Streuung. Das hei√üt, je kleiner die Residualstreuung, desto besser das Modell, weil es dann einen gr√∂√üeren Anteil der Streuung der Daten erkl√§ren kann. Das Verh√§ltnis der Gesamtstreuung zur Residualstreuung nennt man das Bestimmtheitsma√ü (oder auch Determinationskoeffizient). Dieser wird meistens mit \\(R^2\\) bezeichnet und ist definiert als:\\[R^2 = \\frac{\\mathit{SQE}}{\\mathit{SQT}} = 1- \\frac{\\sum^{n}_{=1} (y_i - \\hat{y}_i)^2}{\\sum^{n}_{= 1} (y_i - \\bar{y}_i)^2}\\]\\(R^2\\) liegt (normalerweise) zwischen 0 (schlechtes Modell) und 1 (perfekter Fit). \\(R^2 < 0\\) zeigt eine falsche Modellwahl (z.B. kein Achsenabschnitt, wenn dieser aber n√∂tig ist).Das \\(R^2\\) hat die unangenehme Eigenschaft, bei einer multiplen Regression immer zu steigen, wenn man zus√§tzliche Pr√§diktoren hinzunimmt. Diese Pr√§diktoren k√∂nnen auch ohne jeden Zusammenhang zur abh√§ngigen Variablen stehen. Um dieses Problem zu korrigieren, hat man das adjustierte Bestimmtheitsma√ü \\(R^2_\\text{ajd}\\) eingef√ºhrt. Es ‚Äúbestraft‚Äù f√ºr zus√§tzliche Pr√§diktoren\n\\[R^2_\\text{ajd} = 1 - (1 - R^2) \\frac{n-1}{n - p - 1}\\]mit \\(n\\): Anzahl der Datenpunkte, \\(p\\): Anzahl der Pr√§diktoren (ohne \\(y\\)-Achsenabschnitt). \\(R^2_\\text{ajd}\\) ist aussagekr√§ftiger als \\(R^2\\) bei multipler Regression.Zur√ºck zu unserem Beispiel. Wie viel Streuung erkl√§rt nun unser Modell? Wir sehen uns die tidy Zusammenfassung .r_squared: Bestimmtheimtsma√ü \\(R^2\\)adj_r_squared: adjustiertes Bestimmtheitsma√ü \\(R^2_\\text{ajd}\\)mse: mittlerer quadratischer Fehler, berechnet als mean(residuals(lin_mod)^2)rmse: Wurzel aus msesimga: Standardabweichung (.e.¬†Standardfehler) des Fehlerterms \\(\\varepsilon\\)statistic: Wert der \\(F\\)-Statistik f√ºr den Hypothesentest mit H\\(_0\\): alle Modellparameter sind Nullp-value: \\(p\\)-Wert zum Hypothesentestdf: Freiheitsgrade, hier Anzahl der Pr√§diktorennobst: Anzahl der DatenpunkteSomit erkl√§rt unser Modell 53% der Varianz der Daten.","code":"\nget_regression_summaries(lin_mod) %>% kable()"},{"path":"regression-inferenz.html","id":"bootstrap-mit-infer-konfidenzintervall-f√ºr-steigung","chapter":"Kapitel 12 Inferenz in linearer Regression","heading":"12.2 Bootstrap mit infer: Konfidenzintervall f√ºr Steigung","text":"Die \\(t\\)-Verteilung der Sch√§tzungen gilt asymptotisch, d.h. bei gro√üen Datens√§zten, auch f√ºr nicht-normalverteilte Residuen (unter bestimmten Bedingungen, s. Fahrmeir, Kneib, Lang (2009)). Allerdings erfordern\nnicht-normalverteilte Residuen gro√üe Stichproben oder aber alternative Methoden zum Berechnen der Konfidenzintervalle. Auch heteroskedastische Residuen f√ºhren zu falschen Konfidenzintervallen und Hypothesentests. Bootstrap ist robust gegen die Verletzung beider Annahmen (Normalvereilung und Homoskedastizit√§t der Residuen). Allerdings m√ºssen die Daten nach wie vor unabh√§ngig sein (keine wiederholten Messungen, keine Zeitreihen!), damit Bootstrap korrekt arbeitet.Bei einer Einfachregression, d.h. wenn es nur einen Pr√§diktor gibt, ist die Steigung h√§ufig der interessante Parameter. Wenn wir uns also nicht f√ºr den \\(y\\)-Achsenabschnitt interessieren, dann k√∂nnen wir das altbekannte Framework von infer f√ºr das Bootstrap-Konfidenzintervall f√ºr die Steigung verwenden. F√ºr unser Beispiel ginge es :Schritt 1: Bootstrap-Stichproben generieren und Statistik ‚ÄúSteigung‚Äù berechnenSchritt 2: Konfidenzintervall berechnenSchritt 3: Ergebnisse darstellenVerglichen mit dem Konfidenzintervall basierend auf der Normalverteilungsannahme, ist Bootstrap hier sehr √§hnlich. Das liegt daran, dass die Normalverteilungsannahme und die Homoskedastizit√§tsannahme ja erf√ºllt sind. einem Fall sind die Standardkonfidenzintervalle basierend auf der Normalverteilungsannahme und dem Bootstrap sehr √§hnlich. Wir w√ºrden also im Normalfall einfach die Standardkonfidenzintervalle benutzten.Wir k√∂nnen mit infer auch einen Hypothesentest durchf√ºhren, der untersucht, ob die Steigung von Null verschieden ist.H\\(_0\\): \\(\\beta_1 = 0\\)H\\(_1\\): \\(\\beta_1 \\neq 0\\)F√ºr die Darstellung brauchen wir noch die berechnete Steigung:Und nun die Visualisierung:","code":"\nbootstrap_distn_slope <- befragung %>% \n  specify(formula = zeit_bib ~ anreise) %>%\n  generate(reps = 10000, type = \"bootstrap\") %>% \n  calculate(stat = \"slope\")\npercentile_ci <- bootstrap_distn_slope %>% \n  get_confidence_interval(type = \"percentile\", level = 0.95)\n\npercentile_ci## # A tibble: 1 √ó 2\n##   lower_ci upper_ci\n##      <dbl>    <dbl>\n## 1   -0.848   -0.687\nvisualize(bootstrap_distn_slope) +\n    shade_confidence_interval(endpoints = percentile_ci) \nget_regression_table(lin_mod) %>% \nfilter(term == 'anreise') %>%\n  kable()\nnull_distn_slope <- befragung %>% \n  specify(formula = zeit_bib ~ anreise) %>%\n  hypothesize(null = \"independence\") %>% \n  generate(reps = 10000) %>% \n  calculate(stat = \"slope\")## Setting `type = \"permute\"` in `generate()`.\nobserved_slope <- befragung %>% \n  specify(formula = zeit_bib ~ anreise) %>% \n  calculate(stat = \"slope\")\n\nobserved_slope## Response: zeit_bib (numeric)\n## Explanatory: anreise (numeric)\n## # A tibble: 1 √ó 1\n##     stat\n##    <dbl>\n## 1 -0.766\nvisualize(null_distn_slope) +\n  shade_p_value(obs_stat = observed_slope, direction = \"both\")\nnull_distn_slope %>% \n  get_p_value(obs_stat = observed_slope, direction = \"both\")## Warning: Please be cautious in reporting a p-value of 0. This result is an\n## approximation based on the number of `reps` chosen in the `generate()` step. See\n## `?get_p_value()` for more information.## # A tibble: 1 √ó 1\n##   p_value\n##     <dbl>\n## 1       0"},{"path":"regression-inferenz.html","id":"bootstrap-mit-rsample-konfidenzintervalle-f√ºr-steigung-und-y-achsenabschnitt","chapter":"Kapitel 12 Inferenz in linearer Regression","heading":"12.3 Bootstrap mit rsample: Konfidenzintervalle f√ºr Steigung und \\(y\\)-Achsenabschnitt","text":"Falls wir doch Konfidenzintervalle f√ºr beide Modellparameter brauchen (und die Annahmen Normalverteilung und/oder Homoskedastizit√§t verletzt sind) oder aber ein lineares Modell mit mehreren Pr√§diktoren anpassen, k√∂nnen wir nicht mehr mit infer arbeiten. Es gibt aber ein ganzes Modelluniversum, zusammengestellt der Paketsammlung tidymodels (https://www.tidymodels.org/). Darin gibt es nicht nur alle m√∂glichen Modelle, sondern vor allem eine tidy und einheitliche Herangehensweise ans Modellieren. tidymodels ist jenseits dessen, wir diesem Kurs machen werden. Wir werden aber die Bibliothek rsample nutzen, die uns beim Bootstrappen hilft.Um Konfidenzintervalle sowohl f√ºr den \\(y\\)-Achsenabschnitt als auch f√ºr die Steigung zu bekommen, generieren wir Bootstrap-Stichproben aus der befragung, passen dann jede solche Stichprobe unser lineares Modell . Jedes dieser Modelle liefert uns einen \\(y\\)-Achsenabschnitt und eine Steigung. Dadurch bekommen wir Bootstrapverteilungen dieser Modellparameter, wie sonst auch, wenn wir Bootstrap benutzt haben. Aus diesen Bootstrapverteilungen berechnen wir dann mit der Methode der Quantile unsere Konfidenzintervalle.Schritt 1: Bootstrapstichproben aus befragung generierenWir generieren 10000 Bootstrap-Stichproben aus dem Datensatz befragung mit Hilfe der Funktion bootstraps() aus der Bibliothek rsample.Das Objekt my_bootstraps ist ein ‚Äúverpacktes‚Äù (nested) Objekt. Jeder split ist eine Bootstrap-Stichprobe.Wir sehen uns eine Bootstrap-Stichprobe mal . Dazu nutzen wir die Funktion analysis(), die solche splits richtig behandelt. Jede Bootstrap-Stichprobe ist durch Ziehen mit Zur√ºcklegen aus dem Datensatz befragung hervorgegangen, das ist ja beim Bootstrap immer .Wie viele Studierende wurden mehrfach gezogen? Wir sehen uns als Beispiel die erste Bootstrap-Stichproben .Schritt 2: Modell auf den Bootstrap-Stichproben anpassenUm unser urspr√ºngliches lineares Modell anzupassen, benutzen wir eine selbstgeschriebene Funktion, inspiriert von der Hilfe zu rsample (?int_pctl). Die Funktion lm_est() passt auf einer Bootstrap-Stichprobe das lineare Modell zeit_bib ~ anreise . Um die Berechnung auf allen Bootstrap-Stichproben effizient zu machen, nutze ich die Funktion map(), die sich eine Bootstrap-Stichprobe nach der anderen vornimmt und das lineare Modell anpasst. Wir kommen einer sp√§tere Stunde auf solche effizienten Funktionen zur√ºck.Wir sehen uns das entstandene Objekt mit allen angepassten Modellen . Und \\(\\dots\\) wir sehen nichts üòÑ.Die Ergebnisse m√ºssen wir noch ‚Äúauspacken.‚ÄùJetzt k√∂nnen wir sehen, dass das Objekt model_coeffs die beiden Modellparameter, \\(y\\)-Achsenabschnitt und die Steigung, enth√§lt und das jeweils h√§ufig, wie wir eben Bootstrap-Stichproben generiert haben. Somit haben wir jetzt eine Bootstrapverteilung dieser Modellparameter, die wir nun plotten k√∂nnen.Schritt 3: Konfidenzintervalle berechnenDie Verteilungen sind symmetrisch, weil ja die Annahmen der Normalregression erf√ºllt sind. Jetzt fehlen uns noch die Konfidenzintervalle, von denen wir erwarten, dass sie den Standardkonfidenzintervallen √§hnlich sein werden.Wir plotten alle Ergebnisse nun zusammen. Das m√ºssen wir ‚Äúh√§ndisch‚Äù machen, da uns ja nicht die tolle Funktion visualize() aus infer zur Verf√ºgung steht. Wir f√ºgen die Konfidenzintervalle als vertikale Linien zu den Histogrammen der Bootstrapverteilungen hinzu.diesem Beispiel √§hneln sich die Konfidenzintervalle, da die Annahmen der Normalregression erf√ºllt sind. Das wird den Aufgaben (s.u.), die Sie selbst√§ndig bearbeiten werden, nicht mehr der Fall sein. Selbstverst√§ndlich sollen Sie im Falle von erf√ºllten Annahmen einfach die Standardkonfidenzintervalle nutzen und brauchen kein Bootstrap. Sie werden aber sehen, dass beim Modellieren mit richtigen Daten die Annahmen der Normalverteilung und der Homoskedastizit√§t h√§ufig verletzt sind. Mit dem Bootstrap sind Sie jetzt daf√ºr ger√ºstet üòÑ.","code":"\nlibrary(rsample)\nset.seed(123)\n\nbootstrap_reps <- 10000\n\nmy_bootstraps <- bootstraps(befragung, times = bootstrap_reps)\nmy_bootstraps## # Bootstrap sampling \n## # A tibble: 10,000 √ó 2\n##    splits           id            \n##    <list>           <chr>         \n##  1 <split [200/72]> Bootstrap00001\n##  2 <split [200/72]> Bootstrap00002\n##  3 <split [200/72]> Bootstrap00003\n##  4 <split [200/70]> Bootstrap00004\n##  5 <split [200/68]> Bootstrap00005\n##  6 <split [200/68]> Bootstrap00006\n##  7 <split [200/74]> Bootstrap00007\n##  8 <split [200/69]> Bootstrap00008\n##  9 <split [200/79]> Bootstrap00009\n## 10 <split [200/73]> Bootstrap00010\n## # ‚Ä¶ with 9,990 more rows\nclass(my_bootstraps)## [1] \"bootstraps\" \"rset\"       \"tbl_df\"     \"tbl\"        \"data.frame\"\nanalysis(my_bootstraps$splits[[1]])## # A tibble: 200 √ó 7\n## # Groups:   replicate [1]\n##    replicate student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##        <int>      <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1         1       5641 w          stadt   fahrrad          21.0      304.\n##  2         1       4887 w          stadt   zu_fuss          12.9      303.\n##  3         1       2672 w          stadt   zu_fuss          10.8      313.\n##  4         1       2771 m          stadt   zu_fuss           5.86     286.\n##  5         1       8221 w          stadt   auto             18.2      310.\n##  6         1       1236 w          stadt   zu_fuss           6.51     330.\n##  7         1       5395 w          stadt   bus              25.9      297.\n##  8         1      11681 m          land    bus             109.       245.\n##  9         1       2672 w          stadt   zu_fuss          10.8      313.\n## 10         1       5395 w          stadt   bus              25.9      297.\n## # ‚Ä¶ with 190 more rows\nanalysis(my_bootstraps$splits[[1]]) %>%\n  group_by(student_id) %>%\n  summarise(n = n()) %>% \n  arrange(desc(n))## # A tibble: 128 √ó 2\n##    student_id     n\n##         <int> <int>\n##  1       4148     4\n##  2       4384     4\n##  3       5342     4\n##  4       2043     3\n##  5       2416     3\n##  6       4045     3\n##  7       4418     3\n##  8       5395     3\n##  9       5641     3\n## 10       5971     3\n## # ‚Ä¶ with 118 more rows\nlm_est <- function(split, ...) {\n  lm(zeit_bib ~ anreise, data = analysis(split)) %>%\n    tidy()\n}\n\nmodel_res <- my_bootstraps %>%\n  mutate(results = map(splits, .f = lm_est))\nmodel_res## # Bootstrap sampling \n## # A tibble: 10,000 √ó 3\n##    splits           id             results         \n##    <list>           <chr>          <list>          \n##  1 <split [200/72]> Bootstrap00001 <tibble [2 √ó 5]>\n##  2 <split [200/72]> Bootstrap00002 <tibble [2 √ó 5]>\n##  3 <split [200/72]> Bootstrap00003 <tibble [2 √ó 5]>\n##  4 <split [200/70]> Bootstrap00004 <tibble [2 √ó 5]>\n##  5 <split [200/68]> Bootstrap00005 <tibble [2 √ó 5]>\n##  6 <split [200/68]> Bootstrap00006 <tibble [2 √ó 5]>\n##  7 <split [200/74]> Bootstrap00007 <tibble [2 √ó 5]>\n##  8 <split [200/69]> Bootstrap00008 <tibble [2 √ó 5]>\n##  9 <split [200/79]> Bootstrap00009 <tibble [2 √ó 5]>\n## 10 <split [200/73]> Bootstrap00010 <tibble [2 √ó 5]>\n## # ‚Ä¶ with 9,990 more rows\nmodel_coeffs <- model_res %>%\n  # Die Spalte splits los werden.\n  select(-splits) %>%\n  # Und das Ergebnis in ein tibble umwandeln.\n  unnest(results)\n\nmodel_coeffs## # A tibble: 20,000 √ó 6\n##    id             term        estimate std.error statistic   p.value\n##    <chr>          <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n##  1 Bootstrap00001 (Intercept)  301.       2.18       138.  1.29e-198\n##  2 Bootstrap00001 anreise       -0.724    0.0531     -13.7 2.47e- 30\n##  3 Bootstrap00002 (Intercept)  302.       2.40       126.  5.25e-191\n##  4 Bootstrap00002 anreise       -0.781    0.0585     -13.3 2.11e- 29\n##  5 Bootstrap00003 (Intercept)  302.       2.21       136.  8.71e-198\n##  6 Bootstrap00003 anreise       -0.813    0.0506     -16.1 8.94e- 38\n##  7 Bootstrap00004 (Intercept)  300.       2.22       135.  5.73e-197\n##  8 Bootstrap00004 anreise       -0.751    0.0554     -13.6 4.93e- 30\n##  9 Bootstrap00005 (Intercept)  299.       2.18       137.  2.89e-198\n## 10 Bootstrap00005 anreise       -0.742    0.0493     -15.1 1.19e- 34\n## # ‚Ä¶ with 19,990 more rows\nggplot(model_coeffs, aes(x = estimate, group = term)) + \n  geom_histogram(col = \"white\", nbins = 30) +\n  facet_wrap(~ term, scales = \"free_x\")## Warning: Ignoring unknown parameters: nbins## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n# Konfidenzintervalle mit Bootstrap\npercentile_ci <- int_pctl(model_res, results)\n\n# Standard-Konfidenzintervalle mit der Annahme der Normalverteilung und Homoskedastizit√§t der Residuen\nstandard_ci <- tidy(lin_mod, conf.int = TRUE)\n\npercentile_ci %>% kable()\nstandard_ci %>% kable()\nggplot(model_coeffs, aes(x = estimate, group = term)) + \n  geom_histogram(col = \"white\", nbins = 30) +\n  geom_vline(data = percentile_ci, aes(xintercept = .lower, group = term, col = 'percentile_ci_lower')) +\n  geom_vline(data = percentile_ci, aes(xintercept = .upper, group = term), col = 'blue') +\n  geom_vline(data = standard_ci, aes(xintercept = conf.low, group = term, col = 'conf.low')) +\n  geom_vline(data = standard_ci, aes(xintercept = conf.high, group = term), col = 'orange') +\n  scale_color_manual(name = \"Confidence intervals\", values = c(percentile_ci_lower = \"blue\", conf.low = 'orange'), labels = c('standard', 'bootstrap')) +\n  facet_wrap(~ term, scales = \"free_x\") +\n  theme(legend.position = 'bottom')## Warning: Ignoring unknown parameters: nbins## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"regression-inferenz.html","id":"lesestoff-12","chapter":"Kapitel 12 Inferenz in linearer Regression","heading":"12.4 Lesestoff","text":"Kapitel 10 Ismay Kim (2021)","code":""},{"path":"regression-inferenz.html","id":"aufgaben-10","chapter":"Kapitel 12 Inferenz in linearer Regression","heading":"12.5 Aufgaben","text":"","code":""},{"path":"regression-inferenz.html","id":"artenreichtum-auf-den-galapagosinseln","chapter":"Kapitel 12 Inferenz in linearer Regression","heading":"12.5.1 Artenreichtum auf den Galapagosinseln","text":"Wir besch√§ftigen uns mit dem Datensatz gala aus der Bibliothek faraway.Laden Sie den Datensatz und lesen Sie der Hilfe nach, um es sich dabei handelt.Untersuchen Sie die Hypothese, dass die Anzahl der endemischen Arten linear von der Gesamtartenzahl abh√§ngt.√úberpr√ºfen Sie die Annahmen des linearen Modells.Vergleichen Sie das Konfidenzintervall, das auf der Normalverteilungsannahme beruht mit dem Bootstrap-Konfidenzintervall.F√ºhren Sie den Hypothesentest analog zum Beispiel oben mit infer durch.Interpretieren Sie das Modell.","code":""},{"path":"regression-inferenz.html","id":"artenreichtum-auf-den-galapagosinseln-revisited","chapter":"Kapitel 12 Inferenz in linearer Regression","heading":"12.5.2 Artenreichtum auf den Galapagosinseln, revisited","text":"Wiederholen Sie die obere Aufgabe nun mit Hilfe der Bibliothek rasample. Tip: Wandeln Sie die Inselnamen, die nur als Zeilennamen existieren, eine richtige Spalte um gala <- gala %>% rownames_to_column(var = \"island\").","code":""},{"path":"regression-inferenz.html","id":"ihre-arbeit-einreichen-7","chapter":"Kapitel 12 Inferenz in linearer Regression","heading":"12.6 Ihre Arbeit einreichen","text":"Speichern Sie Ihr Notebook ab und laden Sie nur die .Rmd Datei vom Server.Laden Sie Ihre .Rmd Datei ILIAS hoch. Beachten Sie die Deadline!Sie erhalten die Musterl√∂sung nach dem Hochladen.","code":""},{"path":"daten-und-bericht.html","id":"daten-und-bericht","chapter":"A Datenquellen und Struktur des Abschlussberichts","heading":"A Datenquellen und Struktur des Abschlussberichts","text":"\nVerschiedene Datenquellen kennen\n\nM√∂gliche R-Pakete f√ºr direkten Datenbankzugang kennen\n","code":""},{"path":"daten-und-bericht.html","id":"datenquellen","chapter":"A Datenquellen und Struktur des Abschlussberichts","heading":"A.1 Datenquellen","text":"diesem Abschnitt beschreibe ich einige m√∂gliche Datenquellen, die Sie f√ºr Ihre Abschlussberichte nutzen k√∂nnen. Technische Unterst√ºtzung kann ich nur beim Paket eurostat anbieten. Andere Pakete m√ºssen Sie selbstst√§ndig entdecken (Challenge üòé).","code":""},{"path":"daten-und-bericht.html","id":"statistsiches-bundesamt","chapter":"A Datenquellen und Struktur des Abschlussberichts","heading":"A.1.1 Statistsiches Bundesamt","text":"Das Statistische Bundesamt bietet Daten zu und √ºber Deutschland. deren Datenbank GENESIS. Es ist wichtig, dass Sie beim Herunterladen (Werteabruf) das Format flat w√§hlen. Dann bekommen Sie einen tidy Datensatz.","code":""},{"path":"daten-und-bericht.html","id":"eurostat","chapter":"A Datenquellen und Struktur des Abschlussberichts","heading":"A.1.2 eurostat","text":"eurostat ist das statistische Amt der Europ√§ischen Union. Auf ihrer Seite finden Sie viele Informationen und statistische Daten √ºber und zu Europa.Die Daten von eurostat k√∂nnen direkt mit dem Paket eurostat tidy heruntergeladen werden. Das Paket hat eine sehr gute Homepage und Tutorien Unter dem Reiter Articles finden Sie auch ein Tutorium, das die Darstellung der Daten als Karten (auch interaktiv) zeigt.","code":""},{"path":"daten-und-bericht.html","id":"gapminder","chapter":"A Datenquellen und Struktur des Abschlussberichts","heading":"A.1.3 gapminder","text":"Sie haben einen Auszug aus den Daten von gapminder bereits kennen gelernt. Es gibt aber noch viel mehr dort zu entdecken. Die Daten k√∂nnen Sie per Hand hier herunter laden. Besser ist jedoch, sich mit dem DDF Format (data description format) auseinander zu setzten. Dieses bietet tidy .csv-Dateien . Der vollst√§ndige Datensatz von gapminder kann hier herunter geladen werden.","code":""},{"path":"daten-und-bericht.html","id":"national-oceanic-and-atmospheric-administration-noaa","chapter":"A Datenquellen und Struktur des Abschlussberichts","heading":"A.1.4 National Oceanic and Atmospheric Administration (NOAA)","text":"NOAA bietet zahlreiche Datens√§tze zu Ozeanen, Wetter und Klima . Sie k√∂nnen die Daten mit dem Paket rnoaa direkt herunterladen.","code":""},{"path":"daten-und-bericht.html","id":"weitere-datenquellen","chapter":"A Datenquellen und Struktur des Abschlussberichts","heading":"A.1.5 Weitere Datenquellen","text":"World Bank Open DataWorld Happiness ReportGlobal Carbon Budget 2020\nPublikation und Datensatz\nGlobal Carbon Project\nPublikation und DatensatzGlobal Carbon ProjectGehalte organischem Kohlenstoff B√∂den unter mehrj√§hrigen Kulturen\nPublikation\nDatensatz\nPublikationDatensatzPANGAEA: eine der gr√∂√üten Datenbanken f√ºr Umweltdaten√úberblick √ºber open data und Pakete f√ºr direkten Download auf ROpenScie:\nDaten\nPakete","code":""},{"path":"daten-und-bericht.html","id":"forschungsplan","chapter":"A Datenquellen und Struktur des Abschlussberichts","heading":"A.2 Forschungsplan","text":"F√ºr Ihren Bericht erstellen Sie bitte einen Forschungsplan, den Sie ILAS hochladen. Nutzen Sie f√ºr den Forschungsplan das zur Verf√ºgung gestellte Template. Beachten Sie die Deadline ILIAS. Sie bekommen Feedback zu diesem Forschungsplan, bevor Sie Ihren eigentlichen Bericht erarbeiten. Damit sollten Missverst√§ndnisse bez√ºglich Inhalt und Schwere der Aufgabe vermieden werden.","code":""},{"path":"daten-und-bericht.html","id":"struktur-des-abschlussberichts","chapter":"A Datenquellen und Struktur des Abschlussberichts","heading":"A.3 Struktur des Abschlussberichts","text":"","code":""},{"path":"daten-und-bericht.html","id":"struktur-des-arbeitsverzeichnisses","chapter":"A Datenquellen und Struktur des Abschlussberichts","heading":"A.3.1 Struktur des Arbeitsverzeichnisses","text":"F√ºr Ihren Bericht, legen Sie bitte ein eigenes R-Projekt . Eine ausf√ºhrliche Anleitung finden sie hier. Ein Projekt hilft Ihnen, Ihre Arbeit gut zu organisieren und nicht den √úberblick √ºber verschiedene Dateien zu verlieren.Ihrem R-Projekt-Ordner legen Sie einen Ordner f√ºr Daten, gegebenenfalls einen f√ºr Hilfsskripte und gegebenenfalls einen f√ºr Abbildungen, die Sie zus√§tzlich im Bericht zeigen m√∂chten. Ihr R-Notebook verbleibt im Wurzelverzeichnis des Projekts.","code":""},{"path":"daten-und-bericht.html","id":"daten-herunterladen-und-speichern","chapter":"A Datenquellen und Struktur des Abschlussberichts","heading":"A.3.2 Daten herunterladen und speichern","text":"Falls Sie Daten mit Hilfe eines R-Pakets herunterladen, dann legen Sie daf√ºr ein gesondertes R-Notebook und beschreiben Sie den Vorgang: Wann und woher wurde der Datensatz heruntergeladen. Speichern Sie den Datensatz im Ordner Daten. Analysieren Sie den gespeicherten Datensatz. kann ich sp√§ter beim Korrigieren auf denselben (abgespeicherten) Datensatz zugreifen wie Sie. Denn Datenbanken k√∂nnen zwischen Abgabe und Korrektur aktualisiert werden.","code":""},{"path":"daten-und-bericht.html","id":"struktur-des-berichts","chapter":"A Datenquellen und Struktur des Abschlussberichts","heading":"A.3.3 Struktur des Berichts","text":"Strukturieren Sie Ihren Bericht wie folgt:Einf√ºhrung (Ende muss die Forschungsfrage stehen)Material und Methoden: Datenbeschreibung, Beschreibung der Methode mit Literaturangaben, gegebenenfalls Beschreibung des UntersuchungsgebietsErgebnisse: Explorative Datenanalyse, weitere AnalysenDiskussion mit Einbezug von weiterer FachliteraturSchlussfolgerungLiteraturSie k√∂nnen Ergebnisse und Diskussion zu einem Abschnitt vereinen.Jeder Bericht muss eine Challenge beinhalten. Das kann z.B. das Vorbereiten (tidy wrangle) eines besonders komplizierten Datensatzes sein. Oder die Einarbeitung ein spannendes Paket (z.B. r√§umliche Darstellung eurostat) etc.Sie k√∂nnen den Bericht Zweiergruppen anfertigen. Dazu rate ich sogar. diesem Fall m√ºssen Sie im Bericht Ihren eigenen Anteil klar mit Ihrem Namen ausweisen. Es m√ºssen sowohl Text als auch Analyse (Code) von jedem Gruppenmitglied bearbeitet werden.Es gibt keine L√§ngenvorgabe f√ºr Ihren Bericht. Seien Sie concise: viel wie n√∂tig aber nicht mehr.Ende kniten Sie den Bericht zu einem html-Dokument. √úberlegen Sie, welche Code-Chunks im Bericht zu sehen sein m√ºssen und vermeiden Sie Redundanz. Wichtig: das Notebook muss bei mir lauff√§hig sein, d.h. Sie m√ºssen alle Zusatzskripte, Daten etc. mit richtigen relativen Pfaden im Projekt ansprechen.\nLaden Sie den gesamten Projektordner als zip-Datei auf ILIAS hoch.\nDie Deadline steht auf ILIAS.\n","code":""},{"path":"aufgabensammlung.html","id":"aufgabensammlung","chapter":"B Aufgabensammlung","heading":"B Aufgabensammlung","text":"Diese Aufgabensammlung enth√§lt zus√§tzlich Aufgaben, die z.B. im Seminar bearbeitet werden. Aufgaben zu den einzelnen Kapiteln finden Sie Ende des jeweiligen Kapitels.","code":""},{"path":"aufgabensammlung.html","id":"werdeschlau","chapter":"B Aufgabensammlung","heading":"B.1 Tutorial Zusammenhangsma√üe","text":"Begleitend zu diesem Tutorial sind die Kapitel 2 und 3 Mittag (2017) ‚ÄúStatistik‚Äù zu lesen.","code":""},{"path":"aufgabensammlung.html","id":"befragung-der-studierenden-der-uni-werdeschlau","chapter":"B Aufgabensammlung","heading":"Befragung der Studierenden der Uni Werdeschlau","text":"Wir besch√§ftigen uns mit einem fiktiven Beispiel.der (kleinen) Universit√§t Werdeschlau m√∂chte die Studierendenvertretung wissen, ob sich eine Station zum Ausleihen von Fahrr√§dern lohnen w√ºrde. Dazu befragen sie die Studierenden, wie lange sie zur Uni fahren. Zudem wollen Sie wissen, ob die Anreisezeit und die Zeit, die die Studierenden pro Woche der Bibliothek verbringen, zusammen h√§ngen.","code":""},{"path":"aufgabensammlung.html","id":"grundgesamtheit-generieren","chapter":"B Aufgabensammlung","heading":"B.1.1 Grundgesamtheit generieren","text":"Unsere Grundgesamtheit sind alle 12000 Studierende von der Uni Werdeschlau. Wir erstellen uns diesem Beispiel selbst unsere Grundgesamtheit aus der Gleichverteilung. Die Regeln dazu sind absolut frei erfunden. Die meisten Studierenden sind zwischen 5 und 40 Minuten unterwegs; 20% jedoch haben eine l√§ngere Anreise zwischen 60 und 120 Minuten.Lassen Sie den Code laufen.Wir setzen geschlecht, wohnort, verkehrsmittel, anreise und zeit_bib zu einer Datenmatrix (tibble) zusammen und nennen das Objekt grundgesamtheit.Lassen Sie den folgenden Code laufen.","code":"\nlibrary(tidyverse)\nset.seed(123)\nanreise <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n                     runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngeschlecht <- sample(c('m', 'w'), size = 12000, replace = TRUE)\nwohnort <- sapply(anreise, function(x) {\n  if(x < 30) 'stadt'\n  else 'land'\n})\nverkehrsmittel <- sapply(anreise, function(x) {\n  if(x <= 10) 'zu_fuss'\n  else if(x > 10 & x <= 15) sample(c('zu_fuss', 'fahrrad'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'fahrrad', 'auto'), size = 1)\n  else sample(c('bus', 'auto'), size = 1)\n})\n\nzeit_bib <- 5 * 60 - 0.7 * anreise + rnorm(length(anreise), 0, 20)\ngrundgesamtheit <- tibble(geschlecht, wohnort, verkehrsmittel, anreise, zeit_bib)"},{"path":"aufgabensammlung.html","id":"befragung-simulieren","chapter":"B Aufgabensammlung","heading":"B.1.2 Befragung simulieren","text":"der Realit√§t werden nat√ºrlich nicht alle 12000 Studierende befragt (wer hat schon viele Kapazit√§ten?), sondern eine zuf√§llige Stichprobe erhoben, also eine Teilmenge der Grundgesamtheit.Frage: Wie nennt man die StudierendenMerkmalstr√§gerBefragte oderModalwerte?Falls Sie nicht sicher sind: Lesen Sie Kapitel 2 Mittag (2017) ‚ÄúStatistik‚Äù!Um unsere Stichprobe zu erstellen, ziehen wir ohne Zur√ºcklegen aus unserer Grundgesamtheit. Sagen wir mal, die Kapazit√§ten der Befragenden reichen f√ºr 200 Befragungen.Lassen Sie den folgenden Code laufen.","code":"\nbefragung <- grundgesamtheit[sample(1:dim(grundgesamtheit)[1], size = 200, replace = FALSE), ]"},{"path":"aufgabensammlung.html","id":"kurze-explorative-datenanalyse","chapter":"B Aufgabensammlung","heading":"B.1.3 Kurze explorative Datenanalyse","text":"Von hier arbeiten wir mit unserer Stichprobe befragung. Sehen wir uns mal die empirische Verteilung, d.h. die Verteilung der Stichprobe .Plotten Sie ein Histogramm der Anreisezeiten unserer Stichprobe. Lassen Sie den Code laufen und passen Sie die bins sinnvoll .","code":"\nggplot(data = befragung, aes(x = anreise)) +\n  geom_histogram(bins = 5) +\n  xlab('Tagliche Anreise (min)') +\n  ylab('H√§ufigkeit')"},{"path":"aufgabensammlung.html","id":"lage--und-streuungsma√üe","chapter":"B Aufgabensammlung","heading":"B.1.4 Lage- und Streuungsma√üe","text":"Erstellen Sie die F√ºnf-Punkte-Zusammenfassung der Stichprobe. Lassen Sie den Code laufen.Wie Sie sehen k√∂nnen, behandelt R die kategorischen Daten anders als numerische. Kategorische Variablen werden als absolute H√§ufigkeiten pro Kategorie dargestellt (d.h. ausgez√§hlt).Wir vergleichen nun gew√∂hnliche und robuste Lage- und Streuungsma√üe. Zu gew√∂hnlichen geh√∂ren der Mittelwert (genauer arithmetisches Mittel) und die Standardabwechung.Berechnen Sie den Mittelwert und die Standardabweichung der Anreise.Zu den robusten Ma√üen geh√∂ren der Median, der Interquartilabstand und die mittlere absolute Abweichung vom Median (MAD). MAD beschreibt die durchschnittlich Abweichung der Stichprobe von ihrem Median.Berechnen Sie den Median der Stichprobe.Den Interquartilabstand bekommt man mit Hilfe der Funtion quantile, die die Quantile berechnet.Lassen Sie den folgenden Code laufen.Der Interquartilabstand kann auch direkt mit der Funktion IQR() berechnet werden und MAD mit mad().Berechnen Sie den Interquartilabstand und MAD der Stichprobe.Vergleichen Sie die gew√∂hnlichen und die robusten Ma√üe. Es gilt: je gr√∂√üer die au√üergew√∂hnlichen Datenpunkte, d.h. je h√∂her die h√∂chsten Anfahrtszeiten, desto st√§rker reagieren der Mittelwert und die Standardabweichung. Die robusten Ma√üe bleiben (solange der Anteil der hohen Anfahrtszeiten nicht zu gro√ü ist) relativ konstant.Abgesehen vom Aspekt der Robustheit, ist das arithmetische Mittel nicht immer ein geeigneter Durchschnittswert. F√ºr Wachstumsfaktoren (Preissteigerung, Zinsen) ist das geometrische Mittel die korrekte Mittelung und f√ºr die Berechnung von Durchschnittsgeschwindigkeiten das harmonische Mittel (Details z.B. bei Fahrmeir et al.¬†(2016). Statistik. Der Weg zur Datenanalyse).","code":"\nsummary(befragung)\nquantile(x = befragung$anreise, probs = 0.75) - quantile(x = befragung$anreise, probs = 0.25)"},{"path":"aufgabensammlung.html","id":"zufall-in-der-befragung","chapter":"B Aufgabensammlung","heading":"B.1.5 Zufall in der Befragung","text":"Wir haben unsere Befragten rein zuf√§llig ausgew√§hlt. D.h. die Kenngr√∂√üen h√§ngen von dieser zuf√§lligen Auswahl ab. Wie w√ºrden sie sich ver√§ndern, wenn wir eine andere Stichprobe ausgesucht h√§tten?Wir simulieren eine wiederholte Befragung und sehen uns , wie sich der Mittelwert und der Median entwickeln. Lassen Sie den Code laufen.Stellen Sie die Mittelwerte mean_all als Histogramm dar. Lassen Sie den Code laufen und passen Sie die bins sinnvoll .Stellen Sie die Mediane median_all als Histogramm dar. Lassen Sie den Code laufen und passen Sie die bins sinnvoll .","code":"\nanzahl_der_befragungen <- 100\n\n# Leere Vektoren erstellen, in die sp√§ter die Ergebnisse geschrieben werden.\nmean_all <- vector()\nmedian_all <- vector()\n\n\nfor (i in 1:anzahl_der_befragungen){\n  dat <- grundgesamtheit[sample(1:dim(grundgesamtheit)[1], size = 200, replace = FALSE), ]\n  mean_all[i] <- mean(dat$anreise)\n  median_all[i] <- median(dat$anreise)\n}\n\nkennzahlen <- tibble(mean_all, median_all)\nggplot(data = kennzahlen, aes(x = mean_all)) +\n  geom_histogram(bins = 5) +\n  xlab('Mittelwerte der taglichen Anreise (min)') +\n  ylab('H√§ufigkeit')\nggplot(data = kennzahlen, aes(x = median_all)) +\n  geom_histogram(bins = 5) +\n  xlab('Mediane der taglichen Anreise (min)') +\n  ylab('H√§ufigkeit')"},{"path":"aufgabensammlung.html","id":"zusammenhangsma√üe-f√ºr-nominalskalierte-merkmale","chapter":"B Aufgabensammlung","heading":"B.1.6 Zusammenhangsma√üe f√ºr nominalskalierte Merkmale","text":"Gibt es Pr√§ferenzen f√ºr bestimmte Verkehrsmittel je nach Geschlecht?\nWir sehen uns die Kontingenztabelle dazu .Lassen Sie den folgenden Code laufen.Nun berechnen wir die Zusammenhangsma√üe auf dieser Tabelle. Dazu nutzen wir die Bibliothek vcd (Visualizing Categorical Data), sehr empfehlenswert, wenn Sie kategorische Daten analysieren wollen.Lassen Sie den folgenden Code laufen.Der Phi-Koeffizient ist NA, weil er nur auf \\(2 \\times 2\\)-Tabellen definiert ist. Die beiden anderen, Contingency Coeff. (Pearson Koeffizient K) und Cramer‚Äôs V zeigen unterschiedliche Werte , je nachdem, wie unsere zuf√§llige Stichprobe ausf√§llt. Aber der Zusammenhang sollte eher klein sein.Allgemein gilt: Contingency Coeff. (Pearson Koeffizient K) und Cramer‚Äôs V schwanken zwischen 0 (kein Zusammenhang) und 1 (perfekter Zusammenhang). Wichtig: Sie zeigen keine Richtung . D.h. wir wissen nicht, ob der Zusammenhang positiv - ‚Äúje mehr desto mehr‚Äù oder negativ ‚Äúje mehr desto weniger‚Äù ist.Die ersten beiden Zeilen, die assocstats liefert, geh√∂ren zum Thema Hypothsentests. Das besprechen wir sp√§ter.Gibt es einen Zusammenhang zwischen dem Wohnort und dem ausgesuchten Verkehrsmittel?Berechnen Sie die Kontingenztabelle. Nutzen Sie die Funktion table() auf den Spalten wohnort und verkehrsmittel im Datensatz befragung.Berechnen Sie die Zusammenhangsma√üe. Lassen Sie den Code laufen.Gibt es eine Pr√§ferenz f√ºr den Wohnort je Geschlecht? Lassen Sie den Code laufen.Da Sie ja genau wissen, wie die Daten erstellt wurden, sollten Ihnen die Antworten nicht zu schwer fallen.","code":"\ntable(befragung$geschlecht, befragung$verkehrsmittel)\nlibrary(vcd)\nassocstats(table(befragung$geschlecht, befragung$verkehrsmittel))\nassocstats(table(befragung$wohnort, befragung$verkehrsmittel))\nassocstats(table(befragung$wohnort, befragung$geschlecht))"},{"path":"aufgabensammlung.html","id":"zusammenhangsma√üe-f√ºr-metrische-merkmale","chapter":"B Aufgabensammlung","heading":"B.1.7 Zusammenhangsma√üe f√ºr metrische Merkmale","text":"Zum Schluss wenden wir uns den beiden numerischen Variablen, anreise und zeit_bib. Besteht hier eine Korrelation?Zuerst stellen wir die Daten dar.Lassen Sie den folgenden Code laufen.Der Zusammenhang zwischen den beiden Variablen ist (ziemlich) linear und negativ.Wir berechnen beide Korrelationsma√üe, den Pearson Korrelationskoeffizienten f√ºr lineare Zusammenh√§nge und den Spearman Korrelationskoeffizient f√ºr monotone Zusammenh√§nge.Lassen Sie den Code laufen.Beide Koeffizienten sind nah dran dem Faktor 0.7, den wir zum Simulieren unserer Daten verwendet haben. Man darf hier beide verwenden. Wichtig ist es zu berichten, welchen Sie verwenden. D.h. es ist nicht genug, zu schreiben, dass Sie eine Korrelation berechnet haben.Gut gemacht! Lassen Sie den Code laufen.","code":"\nggplot(data = befragung, aes(x = anreise, y = zeit_bib)) +\n  geom_point() +\n  xlab('T√§gliche Anreise (min)') +\n  ylab('Zeit in der Bibliothek pro Woche (min)')\n# Pearson Korrelationskoeffizient\ncor(befragung$anreise, befragung$zeit_bib, method = 'pearson')\n\n# Spearman Korrelationskoeffizient\ncor(befragung$anreise, befragung$zeit_bib, method = 'spearman')\nlibrary(ggplot2)\nlibrary(emojifont)\nggplot() + geom_fontawesome(\"fa-coffee\", color='lightblue', size = 80) + theme_void() + ggtitle(\"It's time for coffee\")"},{"path":"aufgabensammlung.html","id":"aufgabe-einreichen","chapter":"B Aufgabensammlung","heading":"B.1.8 Aufgabe einreichen","text":"Speichern Sie Ihr Notebook.Laden Sie Ihr Notebook ILIAS hoch. Sie bekommen eine Musterl√∂sungVergleichen Sie Ihre L√∂sung mit der Musterl√∂sung.Fertig!","code":""},{"path":"aufgabensammlung.html","id":"der-explorative-workflow-mit-tidyverse","chapter":"B Aufgabensammlung","heading":"B.2 Der explorative Workflow mit tidyverse","text":"","code":""},{"path":"aufgabensammlung.html","id":"r-hausaufgaben","chapter":"B Aufgabensammlung","heading":"B.2.1 R-Hausaufgaben","text":"dem Kurs ‚ÄúEinf√ºhrung R‚Äù nehmen 49 Studierende teil. Der Leistungsnachweis besteht aus Hausaufgaben, die insgesamt mit 100 Punkten bewertet werden. Ab 50 Punkten gilt der Kurs als bestanden.Lesen Sie den Datensatz R-.txt, der die Endpunkte enth√§lt, ein.Ermitteln Sie, wie viele Teilnehmer bestanden und wie viele nicht bestanden haben. Nutzen Sie dazu die Funktion mutate() und die Funktion ifelse().","code":""},{"path":"aufgabensammlung.html","id":"klausur","chapter":"B Aufgabensammlung","heading":"B.2.2 Unfaire Klausur?","text":"Ar belegt im 4. Semester die Veranstaltung ‚ÄúSpa√ü mit R‚Äù. Bei der Klausur gibt es 2 Aufgabengruppen mit jeweils 60 Punkten. Aufgabengruppe 1 wird Studierende auf ungeraden Sitzpl√§tzen und Aufgabengruppe 2 Studierende auf geraden Sitzpl√§tzen ausgegeben.Lesen Sie den Datensatz Klausurpunkte.txt ein.√úberpr√ºfen Sie Ars Vermutung, dass die Aufgabengruppe 1 im Schnitt leichter war als Aufgabengruppe 2 (d.h. der Gruppe 1 im Schnitt mehr Punkte erzielt wurden). Nutzen Sie die Funktionen mutate(), um eine Spalte mit der Gruppenzugeh√∂rigkeit zu ermitteln und summarize(), um den Mittelwert zu berechnen. Tipp: mit dem Operator %% k√∂nnen Sie √ºberpr√ºfen, ob eine Zahl z.B. durch 2 teilbar ist.","code":""},{"path":"aufgabensammlung.html","id":"flederm√§use","chapter":"B Aufgabensammlung","heading":"B.2.3 Flederm√§use","text":"Ar Stat untersucht im Rahmen eines √∂kologischen Praktikums Flederm√§use Ecuador. Er misst die Gr√∂√üe der Tiere und bestimmt ihr Geschlecht.Laden Sie den Datensatz Fledermaus.txt. Dieser enth√§lt die Gr√∂√üe der Tiere cm.Die ersten 20 untersuchten Tiere sind M√§nnchen. Erstellen Sie einen factor ‚Äúgeschlecht‚Äù, der das Geschlecht der Tiere enth√§lt. Benutzen Sie dazu die Funktionen rep().F√ºgen Sie die Informationen √ºber die Gr√∂√üe und das Geschlecht einem tibble zusammen und bennen Sie die Spalten entsprechend.Bei dem 3. Individuum hat sich Ar vertippt. Die Gr√∂√üe des Tieres lautet Wirklichkeit 5,37 cm. Korrigieren Sie den Fehler.Speichern Sie den korrigierten Datensatz ab.Berechnen Sie die Mittelwerte und die Standardabweichungen der Gr√∂√üen der Tiere je Geschlecht.Plotten Sie die Gr√∂√üen je Geschlecht einem Boxplot und speichern Sie diesen ab.","code":""},{"path":"aufgabensammlung.html","id":"flederm√§use-revisited","chapter":"B Aufgabensammlung","heading":"B.2.4 Flederm√§use, revisited","text":"Wir gehen davon aus, dass das Alter der Flederm√§use mit ihrer Gr√∂√üe zusammenh√§ngt. Pauschal legen wir fest, dass ein Tier, das kleiner als 5 cm gro√ü ist, ein Jungtier ist.Klassifizieren Sie die Tiere J (Jungtier) und E (Erwachsen). Erstellen Sie dazu mit mutate() ein eigene Spalte.Wie viele Jungtiere gibt es im Datensatz?Sind die Jungtiere weiblich oder m√§nnlich?","code":""},{"path":"aufgabensammlung.html","id":"zeitreihen-aus-dem-lehstenbacheinzugsgebiet-ne-bayern","chapter":"B Aufgabensammlung","heading":"B.2.5 Zeitreihen aus dem Lehstenbacheinzugsgebiet (NE Bayern)","text":"Im Lehstenbacheinzugsgebiet wurden √ºber eine l√§ngere Zeit Niederschlag, Abfluss und Temperatur gemessen. Die Messeinheiten sind f√ºr Temperatur ¬∞C, f√ºr Niederschlag und Abfluss mm.Laden Sie den Datensatz Data.dat.Wandeln Sie die Spalte Date ein richtiges Datum um.Plotten Sie die Temperatur, den Niederschlag und den Abfluss (verschiedene Grafiken) untereinander. Beschriften Sie alles korrekt und f√ºgen Sie Titel hinzu. √úberlegen Sie, welche Darstellungsart (geom) f√ºr den Niederschlag besten ist.Speichern Sie die Grafik als pdf ab.","code":""},{"path":"aufgabensammlung.html","id":"umweltdaten-entlang-der-d√§nischen-k√ºste","chapter":"B Aufgabensammlung","heading":"B.2.6 Umweltdaten entlang der d√§nischen K√ºste","text":"Die Datei Temperatur.csv aus Zuur, Ieno, Meesters (2009) enth√§lt Messungen von Temperatur, Salinit√§t und Chlorophyl 31 Orten entlang der d√§nischen K√ºste. Der Datensatz kann hier heruntergeladen werden. Die Daten stammen vom d√§nischen Institut RIKZ (Monitoringprogramm MWTL: Monitoring Waterstaatkundige Toestand des Lands). Die Messungen wurden zwischen 1990 und 2005 durchgef√ºhrt mit einer H√§ufigkeit von 0‚Äì4 mal pro Monat je nach Jahreszeit.Lesen Sie den Datensatz Temperatur.csv ein.Konvertieren Sie die Spalte Date ein richtiges Datumsformat und plotten Sie die Temperaturen pro Station (facet_wrap()) als Zeitreihen.Berechnen Sie die Anzahl der Messwerte, Monatsmittelwerte der Temperatur f√ºr alle Stationen, sowie die Standardabweichungen.Stellen Sie die Monatsmittel der Temperatur als Linien dar.Beschriften Sie die Grafik sinnvoll.F√ºgen Sie die Standardabweichungen als Band hinzu.Speichern Sie die Grafik als pdf ab.","code":""},{"path":"aufgabensammlung.html","id":"umweltdaten-entlang-der-d√§nischen-k√ºste-revisited","chapter":"B Aufgabensammlung","heading":"B.2.7 Umweltdaten entlang der d√§nischen K√ºste, revisited","text":"Berechnen Sie die Monatsmittelwerte und Standardabweichungen je Station. Tipp: group_by(Station, Month).Stellen Sie die Daten mit einem Fehlerband dar (verschiedenen Plots mit facet_wrap()) und speichern Sie sie ab.","code":""},{"path":"aufgabensammlung.html","id":"bootstrapping-konfidenzintervalle-und-hypothesentests","chapter":"B Aufgabensammlung","heading":"B.3 Bootstrapping, Konfidenzintervalle und Hypothesentests","text":"","code":""},{"path":"aufgabensammlung.html","id":"unfaire-klausur-revisited","chapter":"B Aufgabensammlung","heading":"B.3.1 Unfaire Klausur, revisited","text":"Wir kommen zur√ºck auf die Klausurpunkte aus der Aufgabe B.2.2. Waren die Aufgaben Gruppe 1 leichter? Nutzen Sie das Framework infer.","code":""},{"path":"aufgabensammlung.html","id":"artenvielfalt-in-grasl√§ndern","chapter":"B Aufgabensammlung","heading":"B.3.2 Artenvielfalt in Grasl√§ndern","text":"Sie erhalten Daten aus dem Grasland-Monitoring im Yellowstone Nationalpark und dem National Bison Range (USA) aus Zuur, Ieno, Meesters (2009) und Sikkink et al. (2007). Der Datensatz kann hier heruntergeladen werden. Das Ziel des Monitorings ist die Untersuchung m√∂glicher √Ñnderungen der Biodiversit√§t und deren Zusammenhang mit Umweltfaktoren. Biodiversit√§t wurde durch die Anzahl unterschiedlicher Arten quantifiziert (Spalte R). Insgesamt haben die Forscher ca. 90 Arten 8 Transekten kartiert. Die Aufnahmen wurden alle 4 bis 10 Jahre wiederholt. Insgesamt liegen 58 Beobachtungen vor. Die Daten sind der Datei Vegetation2.xls gespeichert.Laden Sie den Datensatz R und sehen Sie sich seine Struktur . Von welchem Typ sind die einzelnen Variablen? Entspricht das Ihren Erwartungen? Diese Aufgabe dient dazu, das Einlesen von Excel-Dateien mit tidyverse zu erarbeiten. Tipp: nutzen Sie die Funktion read_xls() der Bibliothek readxl. Lesen Sie der Hilfe nach, wie Sie ein bestimmtes Tabellenblatt aus Excel einlesen k√∂nnen.Laden Sie den Datensatz R und sehen Sie sich seine Struktur . Von welchem Typ sind die einzelnen Variablen? Entspricht das Ihren Erwartungen? Diese Aufgabe dient dazu, das Einlesen von Excel-Dateien mit tidyverse zu erarbeiten. Tipp: nutzen Sie die Funktion read_xls() der Bibliothek readxl. Lesen Sie der Hilfe nach, wie Sie ein bestimmtes Tabellenblatt aus Excel einlesen k√∂nnen.Kurze explorative Analyse: Berechnen Sie die Anzahl der Messungen, den Mittelwert und die Standardabweichung der Artenzahl R pro Transekt.Kurze explorative Analyse: Berechnen Sie die Anzahl der Messungen, den Mittelwert und die Standardabweichung der Artenzahl R pro Transekt.Plotten Sie die Artenzahl gegen die Variable BARESOIL (Anteil von unbewachsenem Boden). F√§rben Sie die Punkte je nach Transekt unterschiedlich ein. Tipp: Transekt als as_factor() konvertieren.Plotten Sie die Artenzahl gegen die Variable BARESOIL (Anteil von unbewachsenem Boden). F√§rben Sie die Punkte je nach Transekt unterschiedlich ein. Tipp: Transekt als as_factor() konvertieren.F√ºgen Sie eine gl√§ttende Linie ohne Konfidenzband hinzu, die alle Punkte unabh√§ngig vom Transekt ber√ºcksichtigt (Abschnitt 4.2 im Buch ggplot2 (Wickham 2020)). Damit eine einzige gl√§ttende Linie f√ºr alle Transekte hinzugef√ºgt wird, m√ºssen Sie die aes f√ºr Farbe statt ggplot() geom_point() angeben.F√ºgen Sie eine gl√§ttende Linie ohne Konfidenzband hinzu, die alle Punkte unabh√§ngig vom Transekt ber√ºcksichtigt (Abschnitt 4.2 im Buch ggplot2 (Wickham 2020)). Damit eine einzige gl√§ttende Linie f√ºr alle Transekte hinzugef√ºgt wird, m√ºssen Sie die aes f√ºr Farbe statt ggplot() geom_point() angeben.Beschriften Sie die Grafik (auch die Legende!) sinnvoll und speichern Sie sie als pdf ab.Beschriften Sie die Grafik (auch die Legende!) sinnvoll und speichern Sie sie als pdf ab.Stellen Sie die Artenzahl je Transekt als Zeitreihe dar. Die Symbole sollen sowohl Punkte als auch Linien sein. Skalieren Sie die Gr√∂√üe der Punkte je nach Anteil des unbewachsenen Bodens (Abschnitt 12.1 im Buch ggplot2 (Wickham 2020)). Denken Sie nach, welcher Stelle die aes size stehen muss, damit nur die Punkte skaliert werden.Stellen Sie die Artenzahl je Transekt als Zeitreihe dar. Die Symbole sollen sowohl Punkte als auch Linien sein. Skalieren Sie die Gr√∂√üe der Punkte je nach Anteil des unbewachsenen Bodens (Abschnitt 12.1 im Buch ggplot2 (Wickham 2020)). Denken Sie nach, welcher Stelle die aes size stehen muss, damit nur die Punkte skaliert werden.Beschriften Sie die Grafik (auch die Legenden!) sinnvoll und speichern Sie sie als pdf ab.Beschriften Sie die Grafik (auch die Legenden!) sinnvoll und speichern Sie sie als pdf ab.Setzen Sie nun beide Grafiken untereinander, platzieren Sie die Legenden (ja nach gew√§hltem Layout) sinnvoll und speichern Sie die Grafiken ab.Setzen Sie nun beide Grafiken untereinander, platzieren Sie die Legenden (ja nach gew√§hltem Layout) sinnvoll und speichern Sie die Grafiken ab.Berechnen Sie den linearen Korrelationskoeffizienten zwischen der Biodiversit√§t und dem Anteil von unbewachsenem Boden und ermitteln Sie das 95%-Bootstrap-Konfidenzintervall.Berechnen Sie den linearen Korrelationskoeffizienten zwischen der Biodiversit√§t und dem Anteil von unbewachsenem Boden und ermitteln Sie das 95%-Bootstrap-Konfidenzintervall.Wenn Sie statt eines 95%-Konfidenzintervalls, das 90%-Konfidenzintervall berechnen, wird das Intervall breiter oder schmaler? Warum?Wenn Sie statt eines 95%-Konfidenzintervalls, das 90%-Konfidenzintervall berechnen, wird das Intervall breiter oder schmaler? Warum?","code":""},{"path":"aufgabensammlung.html","id":"verdichtung","chapter":"B Aufgabensammlung","heading":"B.3.3 Bodenverdichtung","text":"Schwere landwirtschaftliche Maschinen k√∂nnen beim Bearbeiten des Bodens zu Bodenverdichtung f√ºhren. einem randomisierten Design wurden zuf√§llig Parzellen (Spalte plots) auf einem sonst homogenen Feld entweder mit einer schweren Maschine bearbeitet (compacted) oder nicht (control). Auf allen Parzellen wurde die Lagerungsdichte bestimmt. Die Lagerungsdichte (auch Trockenrohdichte) ist ein Ma√ü f√ºr Bodenstruktur und gibt das Verh√§ltnis der Trockenmasse eines Bodens zu seinem Volumen. Sie wird h√§ufig [g/cm¬≥] gemessen und kann als ein Indikator f√ºr Bodenverdichtung genutzt werden. Der Datensatz ist der Date bd_compaction.csv gespeichert.Lesen Sie den Datensatz ein und f√ºhren Sie eine kurze explorative Datenanalyse durch.√úberpr√ºfen Sie, ob sich die Lagerungsdichte auf den bearbeiteten Feldern erh√∂ht hat.","code":""},{"path":"aufgabensammlung.html","id":"world-bank","chapter":"B Aufgabensammlung","heading":"B.3.4 Anteil von besch√§ftigten Frauen im privaten und √∂ffentlichen Sektor","text":"Diese √úbung ist inspiriert von Anrew Heiss Blog: https://www.andrewheiss.com/blog/2018/12/05/test--hypothesis/Wir nutzen den Datensatz Worldwide Bureaucracy Indicators (WWBI). Diesen kann man bei der Weltbank herunter laden.Laden Sie den Datensatz als csv file herunter, speichern Sie Ihn auf Ihrer Festplatte ab. Der Datensatz wird als zip-Datei heruntergeladen. Laden Sie diese zip-Datei direkt auf den R-Server den Ordner Data hoch. De Server entpackte die Datei automatisch.Die Daten sind der Datei WWBIData.csv gespeichert. Sehen Sie sich die beiden anderen csv-Dateien . steht drin?Lesen Sie den Datensatz WWBIData.csv ein.Gemeinsame √úbung: Wir bereinigen den Datensatz und machen ihn tidy.Filtern Sie Daten f√ºr das Jahr 2012 und die Indikatoren ‚ÄúBI.PWK.PRVS.FE.ZS‚Äù und ‚ÄúBI.PWK.PUBS.FE.ZS‚Äù. Enfernen Sie NAs. bedeuten diese Indikatoren?Stellen Sie die Histogramme dieser Indikatoren dar.Gibt es Unterschied zwischen den Anteilen der besch√§ftigen Frauen im privaten und √∂ffentlichen Sektor?","code":""},{"path":"aufgabensammlung.html","id":"funktionen-und-iterationen","chapter":"B Aufgabensammlung","heading":"B.4 Funktionen und Iterationen","text":"","code":""},{"path":"aufgabensammlung.html","id":"iterationen-im-kapitel-refstichproben","chapter":"B Aufgabensammlung","heading":"B.4.1 Iterationen im Kapitel 7","text":"Im Kapitel 7 haben wir die Funktion calculate_props() definiert. Schreiben Sie den folgenden Code mit Hilfe der Funktion map() um:Tipp: √úbergeben Sie map() einen Vektor mit den Gr√∂√üen der Stichproben.","code":"\nset.seed(123)\n\n# Stichprobengr√∂√üe 25\nwohnort_props_25 <- calculate_props(grund_data = grundgesamtheit, befragung_size = 25, befragung_reps = 1000)\n  \n# Stichprobengr√∂√üe 50\nwohnort_props_50 <- calculate_props(grund_data = grundgesamtheit, befragung_size = 50, befragung_reps = 1000)\n\n# Stichprobengr√∂√üe 100\nwohnort_props_100 <- calculate_props(grund_data = grundgesamtheit, befragung_size = 100, befragung_reps = 1000)"},{"path":"literatur.html","id":"literatur","chapter":"Literatur","heading":"Literatur","text":"","code":""}]
