[{"path":"index.html","id":"vorwort","chapter":"Vorwort","heading":"Vorwort","text":"‚Äúhoney, ‚Äôre gonna style‚Äù","code":""},{"path":"index.html","id":"organisatorisches","chapter":"Vorwort","heading":"Organisatorisches","text":"\nDie Coronaviruspandemie ver√§ndert unser Leben und unser Lernen. Die UzK bittet Lehrende, das SoSe 2021 als Hybridsemester zu gestalten. Wir werden unseren Kurs komplett online durchf√ºhren. Bitte seien Sie nachsichtig, wenn nicht alles klappt wie Pr√§senzveranstaltungen. Wir m√ºssen aktuell alle sehr viel dazu lernen Sachen digitale Lehre. Sie k√∂nnen sicher sein, dass das Geographische Institut bem√ºht ist, die Lehre effizient wie m√∂glich weiter laufen zu lassen, damit Sie Ihrem Studium fortfahren k√∂nnen.\ndieser Veranstaltung werden wir folgende Werkzeuge verwenden:ILIAS: die Online-Lernplattform der UzK. Entweder sind Sie bereits automatisch dem Kurs registriert oder werden von mir per Hand angemeldet.Campuswire: die Chatplattform dient der allgemeinen Kommunikation und der Selbstorganisation des Lernens. Verwenden Sie diese, um Fragen mit Ihren Kommilitonen*innen und mir zu diskutieren. Sie sollten eine Einladungsmail zu Campuswire erhalten haben.Zoom: die Videokonferenz-Software werden wir f√ºr Liveveranstaltungen nutzen. Die Anmeldemodalit√§ten sind auf den Kursseiten ILIAS erkl√§rt.RStudio Server Pro: der Server bietet die M√∂glichkeit, online R und RStudio zu arbeiten.","code":""},{"path":"index.html","id":"verwendete-literatur","chapter":"Vorwort","heading":"Verwendete Literatur","text":"Wir werden diesem Kurs haupts√§chlich das freie, englischsprachige Buch ModernDive: Statistical Inference via Data Science benutzen (Ismay Kim 2021). Bitte lassen Sie sich nicht davon abschrecken, dass das Buch englischsprachig ist. Es ist sehr gut verst√§ndlich und bietet einen modernen Zugang zur Datenanalyse. Als ein weiteres Buch werden wir Sauer (2019) nutzen.Ab und werde ich Ihnen auch andere Literatur empfehlen. F√ºr Ihren Abschlussbericht werden Sie auch selbst√§ndig weitere Literatur recherchieren.","code":""},{"path":"index.html","id":"sinn-und-unsinn-dieses-skripts","chapter":"Vorwort","heading":"Sinn und Unsinn dieses Skripts","text":"Dieses Skript ist ein lebendiges Begleitdokument des Kurses. Es wird laufend angepasst und aktualisiert.Ich nutze verschiedenfarbige Bl√∂cke, um wichtige Stellen hervorzuheben:\nInfoblock\n\nAchtung, wichtig!\n\nDefinition\n\nLernziele\n","code":""},{"path":"index.html","id":"inspiration-quellen-und-danksagung","chapter":"Vorwort","heading":"Inspiration, Quellen und Danksagung","text":"Dieses Skript baut stark auf folgenden freien Quellen auf:r4ds: Wickham Grolemund (2021)ggplot2: Wickham (2020)ModernDive: Ismay Kim (2021)Den Autoren dieser B√ºcher gilt ein gro√üer Dank f√ºr Ihren Beitrag zur -Community !","code":""},{"path":"index.html","id":"reproduzierbarkeit","chapter":"Vorwort","heading":"Reproduzierbarkeit","text":"Dieses Buch wurde RStudio mit Bookdown geschrieben und R version 4.1.0 (2021-05-18) gebaut. Folgende Pakete werden f√ºr die Beispiele und √úbungen ben√∂tigt:Die komplette Information zur Session lautet:Diese Skript ist lizensiert unter Creative Commons Namensnennung - Nicht-kommerziell - Weitergabe unter gleichen Bedingungen 4.0 International.","code":"## R version 4.1.0 (2021-05-18)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 20.10\n## \n## Matrix products: default\n## BLAS:   /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3\n## LAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3\n## \n## locale:\n##  [1] LC_CTYPE=de_DE.UTF-8       LC_NUMERIC=C              \n##  [3] LC_TIME=de_DE.UTF-8        LC_COLLATE=de_DE.UTF-8    \n##  [5] LC_MONETARY=de_DE.UTF-8    LC_MESSAGES=de_DE.UTF-8   \n##  [7] LC_PAPER=de_DE.UTF-8       LC_NAME=C                 \n##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C       \n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] forcats_0.5.1     stringr_1.4.0     dplyr_1.0.6       purrr_0.3.4      \n##  [5] readr_1.4.0       tidyr_1.1.3       tibble_3.1.2      ggplot2_3.3.3    \n##  [9] tidyverse_1.3.1   kableExtra_1.3.4  fontawesome_0.2.1\n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.6        svglite_2.0.0     lubridate_1.7.10  tufte_0.10       \n##  [5] assertthat_0.2.1  rprojroot_2.0.2   digest_0.6.27     utf8_1.2.1       \n##  [9] R6_2.5.0          cellranger_1.1.0  backports_1.2.1   reprex_2.0.0     \n## [13] evaluate_0.14     highr_0.9         httr_1.4.2        pillar_1.6.1     \n## [17] rlang_0.4.11      readxl_1.3.1      rstudioapi_0.13   jquerylib_0.1.4  \n## [21] rmarkdown_2.8     desc_1.3.0        webshot_0.5.2     munsell_0.5.0    \n## [25] broom_0.7.6       compiler_4.1.0    modelr_0.1.8      xfun_0.23        \n## [29] pkgconfig_2.0.3   systemfonts_1.0.2 htmltools_0.5.1.1 downlit_0.2.1    \n## [33] tidyselect_1.1.1  bookdown_0.22.3   fansi_0.5.0       viridisLite_0.4.0\n## [37] crayon_1.4.1      dbplyr_2.1.1      withr_2.4.2       grid_4.1.0       \n## [41] jsonlite_1.7.2    gtable_0.3.0      lifecycle_1.0.0   DBI_1.1.1        \n## [45] magrittr_2.0.1    scales_1.1.1      cli_2.5.0         stringi_1.6.2    \n## [49] fs_1.5.0          xml2_1.3.2        bslib_0.2.5.1     ellipsis_0.3.2   \n## [53] generics_0.1.0    vctrs_0.3.8       tools_4.1.0       glue_1.4.2       \n## [57] hms_1.1.0         yaml_2.2.1        colorspace_2.0-1  sessioninfo_1.1.1\n## [61] rvest_1.0.0       knitr_1.33        haven_2.4.1       sass_0.4.0"},{"path":"einfuehrung.html","id":"einfuehrung","chapter":"Kapitel 1 Der Kurs","heading":"Kapitel 1 Der Kurs","text":"","code":""},{"path":"einfuehrung.html","id":"zuordnung-zum-modul-und-leistungsnachweis","chapter":"Kapitel 1 Der Kurs","heading":"1.1 Zuordnung zum Modul und Leistungsnachweis","text":"Dieser Kurs geh√∂rt zum Modul Fachmethodik oder Fachmethodik II und ist aus 4 SWS Praktikum und 2 SWS Seminar aufgebaut. Das wichtigste Ziel besteht darin, Ihnen einen sicheren Umgang mit R beizubringen.Den Leistungsnachweis bildet ein benoteter Praktikumsbericht.","code":""},{"path":"einfuehrung.html","id":"lernziele-des-kurses","chapter":"Kapitel 1 Der Kurs","heading":"1.2 Lernziele des Kurses","text":"\nDaten f√ºr Analysen vorbereiten\n\neigene wiederverwendbare Skripte schreiben\n\neigene Funktionen schreiben\n\neinfache Datenanalysen durchf√ºhren\n\nDaten visualisieren\n\nErgebnisse reproduzierbar im Praktikumsbericht darstellen\n","code":""},{"path":"einfuehrung.html","id":"was-mir-im-umgang-miteinander-wichtig-ist","chapter":"Kapitel 1 Der Kurs","heading":"1.3 Was mir im Umgang miteinander wichtig ist","text":"P√ºnktlichkeit bei LivesitzungenGute Vorbereitung durch erledigen der HausaufgabenRespektieren anderer MeinungenOffenheit gegen√ºber neuen Sichtweisen, Themen und MethodenGeduld mit sich selbst und den anderen üòÑ","code":""},{"path":"erste-schritte.html","id":"erste-schritte","chapter":"Kapitel 2 Erste Schritte in R","heading":"Kapitel 2 Erste Schritte in R","text":"\nLayout und Bedeutung einzelner Fenster RStudio kennen\n\nAnweisungen aus dem Skript die Konsole schicken\n\nR als Taschenrechner benutzen\n\nerste Funktionen aufrufen\n\nObjekte mit eckigen Klammern [ ] ansprechen\n\nR-Hilfeseiten aufrufen\n\nIhren ersten Olot erstellen\n","code":""},{"path":"erste-schritte.html","id":"was-ist","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.1 Was ist ?","text":"R ist eine Programmiersprache f√ºr Datenanalyse und statistische Modellierung. Es ist frei verf√ºgbar (open source software) und neben Python einer der meisten benutzten Programmiersprachen zur Datenanalyse und -visualisierung. R wurde von Ross Ihaka und Robert Gentleman 1996 ver√∂ffentlicht (Ihaka Gentleman 1996). Es gibt f√ºr R eine Vielzahl von Zusatzpaketen, die die Funktionalit√§t und die Einsatzm√∂glichkeiten enorm erweitern.Sie k√∂nnen R f√ºr Ihren Computer auf der offiziellen R-Seite https://www.r-project.org/ herunter laden und installieren. Auch die Pakete finden Sie dort unter CRAN (Comprehensive R Archive Network). Auf den CRAN-Seiten finden Sie sogen. CRAN Task Views, eine √úbersicht √ºber Pakete verschiedenen Themenbereichen. F√ºr den Umweltbereich sind folgende Paketsammlungen besonders relevant:Environmetrics: Analyse von UmweltdatenMultivariate: Multivariate StatistikSpatial: Analyse von r√§umlichen DatenTimeSeries: ZeitreihenanalyseZu Beginn des Kurses, werden wir jedoch nicht auf Ihren lokalen Rechnern arbeiten, sondern auf der RStudio Server Pro, der extra f√ºr die digitale Lehre mit R der UzK eingef√ºhrt wurde. Das erm√∂glicht einen schnelleren Einstieg R und bietet eine fast-live Unterst√ºtzung durch den Dozenten beim Programmieren. Daher biete ich zu diesem fr√ºhen Zeitpunkt im Kurs keine Unterst√ºtzung bei der Installation von R auf Ihren Privatrechnern. F√ºr die ganz Ungeduldigen, gibt es hier eine kurze Einleitung zur Installation.","code":""},{"path":"erste-schritte.html","id":"was-ist-rstudio","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.2 Was ist RStudio?","text":"RStudio Desktop ist eine Entwicklungsumgebung f√ºr R. Sie k√∂nnen die open source Version kostenlos f√ºr Ihren Rechner hier herunterladen.Es gibt eine live Einf√ºhrung RStudio im Kurs. Zus√§tzlich k√∂nnen Sie hier ein Video dazu ansehen.","code":""},{"path":"erste-schritte.html","id":"rstudio-server-pro","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.3 RStudio Server Pro","text":"","code":""},{"path":"erste-schritte.html","id":"einloggen-und-eine-session-starten","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.3.1 Einloggen und eine Session starten","text":"Zu Beginn des Kurses werden wir auf dem RStudio Server Pro (RSSP) arbeiten. Jede(r) von Ihnen wird ein pers√∂nliches Login f√ºr den Server erhalten. Dieses Login darf nicht weiter gegeben werden. Damit ich mich im Falle von Programmierfehlern Ihre Konto einloggen darf, m√ºssen Sie eine Einwilligung unterschreiben und per Email mich.Bevor Sie sich auf dem RSSP einloggen k√∂nnen, m√ºssen Sie Ihre VPN-Verbindung aktivieren (aus dem Uni-Netz geht es auch ohne). Auf den Seiten der Rechenzentrums finden Sie eine Anleitung zur Einrichtung des VPN-Zugangs.Anschlie√üend k√∂nnen Sie sich hier einloggen. Alternativ k√∂nnen Sie die Adresse des Servers https://cheops-rstudio-edu.rrz.uni-koeln.de:8787/auth-sign-.htm Ihren Browser kopieren. Nach dem Einloggen sehen Sie die Home-Oberfl√§che, aus der Sie eine neue Sitzung starten k√∂nnen (Abbildung 2.1).\nAbbildung 2.1: RStudio Server Pro Home\nUm eine neue Sitzung zu starten, klicken Sie auf den blauen Button + New Session oder auf New Session neben dem R-Symbol und stellen Sie Folgendes ein (Sie d√ºrfen der Session einen anderen Namen geben, wenn Sie m√∂chten):\nAbbildung 2.2: Einstellungen f√ºr neue Session\nAnschlie√üend sehen Sie die neue Sitzung, auf die Sie nur noch klicken m√ºssen, damit es los geht:\nAbbildung 2.3: Neue Sitzung erstellt\nDer gro√üe Vorteil des RSSPs ist, dass ich direkt Ihre Projekte eingreifen kann, wenn es mal zu Fehlern kommt. W√§hrend ich Ihrem Projekt arbeite, werden Sie kurz aus der R-Sitzung ausgeloggt. Ihnen stehen auf dem Server unbegrenzt Arbeitsstunden zur Verf√ºgung.Sowohl auf dem RSSP als auch einer lokalen Installation, ist Ihr RStudio aufgebaut wie Abbildung 2.4.\nAbbildung 2.4: Aufbau von RStudio\n","code":""},{"path":"erste-schritte.html","id":"dateimanagement","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.3.2 Dateimanagement","text":"Sie werden regelm√§√üig Dateien die Sitzungen auf den RSSP hoch laden und auch herunter laden m√ºssen. F√ºr eine √ºbersichtliche Organisation bietet es sich , einen Ordner f√ºr Dateien anzulegen. Klicken Sie daf√ºr auf New Folder auf dem Reiter Files rechts unten und geben Sie dem Ordnder den Namen data.Um Dateien hoch zu laden, klicken Sie auf den Button Upload auf dem Reiter Files rechts unten (Abbildung 2.5).\nAbbildung 2.5: Dateien hoch laden\nAnschlie√üend klicken Sie auf den Button Browse unter der √úberschrift ‚ÄúFile upload‚Äù und navigieren zu der Datei, die Sie hoch laden m√∂chten (Abbildung 2.6).\nAbbildung 2.6: Dateien zum Hochladen ausw√§hlen\nUm Dateien herunter zu laden, markieren Sie zun√§chst die Datei oder die Dateien, die Sie herunterladen m√∂chten. Dann klicken Sie auf den Button im Reiter Files und dann auf Export und anschlie√üend auf Download (Abbildungen 2.7 und 2.8). Speichern Sie die Datei(en) auf Ihrem Rechner.\nAbbildung 2.7: Dateien zum herunter laden ausw√§hlen und herunter laden\n\nAbbildung 2.8: Dateien zum Herunterladen speichern\nSie sollten auch auf Ihrem eigenen Rechner einen Ordner f√ºr die Veranstaltung anlegen und darin jeweils einen Ordner f√ºr Daten und Skripte.","code":""},{"path":"erste-schritte.html","id":"lesestoff","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.4 Lesestoff","text":"Kapitel 1.1 und 1.2 Ismay Kim (2021).","code":""},{"path":"erste-schritte.html","id":"aufgaben","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.5 Aufgaben","text":"","code":""},{"path":"erste-schritte.html","id":"ars-haushaltsbuch","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.5.1 Ars Haushaltsbuch","text":"Der angehende Datenanalyst Ar Stat m√∂chte dem Rat seiner Mutter folgen und ein Haushaltsbuch anlegen. Als erstes m√∂chte er sich einen √úberblick √ºber seine Ausgaben der Uni-Mensa verschaffen und erstellt die folgende Tabelle:\nTabelle 2.1: Ars Mensaausgaben\nWie viel hat Ar insgesamt der Woche ausgegeben?Wie viel hat er im Schnitt pro Tag ausgegeben?Wie stark schwanken seine Ausgaben?Leider hat Ar sich beim √ºbertragen der Daten vertippt. Er hat Dienstag seine Freundin zum Essen eingeladen und 7,95 ‚Ç¨ statt 2,90 ‚Ç¨ ausgegeben.Korrigieren Sie Ars Fehler.Wie ver√§ndern sich die Ergebnisse aus den Teilaufgaben 1 bis 3 Warum?","code":""},{"path":"erste-schritte.html","id":"rob2","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.5.2 Fehlende Werte","text":"R kodiert fehlende Werte mit NA. Ar Stat hat Montag der darauffolgenden Woche der Mensa gegessen, aber vergessen die Ausgaben zu notieren.\nTabelle 2.2: Ars Mensaausgaben, cont.\nWie √§ndert der fehlende Wert die Berechnung der Summe?Lesen Sie passiert, wenn der Datenvektor bei der Berechnung der Summe fehlende Werte enth√§lt. Rufen Sie dazu die Hilfe auf, .e.¬†?sum.Korrigieren Sie die Berechnung der Summe entsprechend.","code":""},{"path":"erste-schritte.html","id":"firstplot","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.5.3 Ihr erster Plot","text":"Vor allem Anfang kann die Lernkurve R recht flach verlaufen. Daher sollten Sie nicht vergessen, warum Sie R lernen, n√§mlich um echte Datens√§tze zu analysieren.Auch wenn Sie den Code unten noch nicht (ganz) verstehen, kopieren Sie ihn Ihr R und lassen Sie ihn laufen.Welche Daten sind diesem Datensatz enthalten? Nutzen Sie die Hilfe, .e.¬†?gapminder.stellen die Farben der Abbildung dar?wird durch die Symbolgr√∂√üe dargestellt?Wie w√ºrden Sie den Zusammenhang zwischen den Variablen GDP per capita und Life expectancy beschreiben?","code":"\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder2007 <- gapminder %>% \n  filter(year == 2007)\n\nggplot(gapminder2007, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  geom_point() +\n  scale_x_log10() +\n  xlab('GDP per capita') +\n  ylab('Life expectancy') +\n  labs(title = 'Gapminder data for the year 2007')"},{"path":"erste-schritte.html","id":"r-als-taschenrechner","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.5.4 R als Taschenrechner","text":"R ist ein gro√üer Taschenrechner mit vielen voreingebauten Funktionen. Es gelten die √ºblichen Rechenregeln wie z.B. Punkt-vor-Strich und die Klammern.Schreiben Sie den Code, der 2 und 10 addiertDas korrekte Multiplikationszeichen R ist *.Geben Sie den folgenden Befehl korrekt R ein: (2 + 10) \\(\\times\\) 27Bei Dezimalzahlen wird der Dezimalpunkt und nicht das Komma verwendet. Das ist wichtig zu beachten, wenn Sie sp√§ter Daten R einlesen m√∂chten.Berechnen Sie die Summe von 2,34 und 4,98.","code":""},{"path":"erste-schritte.html","id":"zuweisungen","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.5.5 Zuweisungen","text":"R arbeitet man mit Objekten. Ein Objekt kann alles M√∂gliche sein: eine Variable, Daten, Vektoren etc. Wenn also das Ergebnis einer Berechnung oder ein Datenobjekt im R-Arbeitsbereich (workspace) zur Verf√ºgung stehen soll, muss daraus ein Objekt erstellt werden.Objekte erstellt man, indem man ihnen Namen gibt. Diesen Vorgang nennt man Zuweisung (assignment). Im Beispiel unten wird ein Objekt, diesem Fall ein Skalar, namens x erzeugt mit dem Wert 42. Um den Wert von x anzuzeigen, tippen Sie x ein.Zuweisungen k√∂nnen R entweder mit dem = erfolgen oder mit <-. Beide Varianten sind gleichwertig. Dabei ist allerdings Pfeilrichtung entscheidend! x <- 42 bedeutet: die linke Seite (Zahl 42) wird dem Objekt x zugeordnet. Wenn man die Pfeilrichtung umdreht, macht die Zuweisung keinen Sinn und man erh√§lt eine Fehlermeldung.Objektnamen k√∂nnen (fast) frei gew√§hlt werden. Sie m√ºssen mit einem Buchstaben beginnen und d√ºrfen keine Sonderzeichen enthalten. Bei l√§ngeren Namen empfiehlt sich ein _. Streng verboten sind Namen von vordefinierten Funktionen!Erstellen Sie ein Objekt namens mein_objekt und weisen Sie ihm das Ergebnis der Berechnung \\(23^{2}\\) zu. Eine Potenz berechnen Sie mit ^.Eine Zuweisung kann auch kompliziertere Anweisungen enthalten. Hier erstellen wir z.B. einen Vektor mit Hilfe der Funktion c (concatenate) und weisen das Ergebnis dem Objekt my_a zu.","code":"\nx <- 42\n\n# Zeige den Wert von x\nx\n# Sinnvolle Zuweisung\nx <- 42\n# Gleichwertige sinnvolle Zuweisung\nx = 42\n# Sinnloser Ausdruck. Fehlermeldung!\nx -> 42## Error in 42 <- x: ung√ºltige (do_set) linke Seite in Zuweisung\nmy_a <- c(32, 54, 1.2, 398)"},{"path":"erste-schritte.html","id":"funktionsaufruf","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.5.6 Funktionsaufruf","text":"R gibt es eine Vielzahl von vordefinierten Funktionen. Ein Funktionsaufruf hat immer die gleiche Form: mach_das(damit) oder mach_das(damit, und_mit_dieser_einstellung). Z.B. wird die Summe auf einem Objekt mein_objekt mit sum(mein_objekt) berechnet.Erstellen Sie einen Vektor mit den Zahlen 32, 54, 1,2 und 398 und weisen Sie ihn der Variablen my_a zu.Berechnen Sie die summe von my_a.Sie k√∂nnen im √úbrigen auch Vektoren sinnvoll addieren.Erstellen Sie einen Vektor my_b mit der passenden L√§nge und addieren Sie ihn zum Vektor my_a. Die Addition erfolgt elementeweise.H√§ufig wollen wir f√ºr unsere Daten den Mittelwert berechnen.Berechnen Sie den Mittelwerts von my_aBerechnen Sie die Standardabweichung von my_a.","code":""},{"path":"erste-schritte.html","id":"objekte-ansprechen","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.5.7 Objekte ansprechen","text":"Um das ‚ÄúInnenleben‚Äù der Objekte R anzusprechen, gibt es verschieden M√∂glichkeiten. diesem Tutorial konzentrieren wir uns auf Vektoren. Um die einzelnen Komponenten im Vektor anzusprechen, benutzt man eckige Klammern [ ]. Um eine bestimmte Komponente zu adressieren (anzusprechen), schreibt man die Platznummer der Komponente die Klammer. Wenn man im Vektor my_c, z.B. die dritte Komponente extrahieren m√∂chte, dann schreibt man my_c[3]Wir k√∂nnen auch Vektoren erstellen, bei denen einzelne Elemente benannt sind.Elemente solchen Vektoren kann man mit Namen eckigen Klammern ansprechen. Die Namen m√ºssen Anf√ºhrungszeichen geschrieben werden. Es spielt keine Rolle, ob Sie einfache oder doppelte Anf√ºhrungszeichen benutzen.Fragen Sie nach dem Element Koeln im Vektor benannt.","code":"\nmy_c <- c(2, 45.7, pi, sqrt(23), 2^6)\nmy_c[3]## [1] 3.141593\nbenannt <- c('Koeln' = 50667, 'Berlin' = 10965, \"Stuttgart\" = 70173)"},{"path":"erste-schritte.html","id":"ihre-arbeit-einreichen","chapter":"Kapitel 2 Erste Schritte in R","heading":"2.6 Ihre Arbeit einreichen","text":"Speichern Sie Ihre .R Datei auf dem Server ab.Laden Sie diese Datei herunter und speichern Sie sie auf Ihrem Computer ab.Laden Sie die Datei auf ILIAS der dazugeh√∂rigen √úbung hoch.Nach der Abgabe erhalten Sie die Musterl√∂sung.Vergelichen Sie Ihre L√∂sung sebstst√§ndig mit der Musterl√∂sung.Stellen Sie entweder Campuswire (im class-chat) oder der n√§chsten live Sitzung Fragen, falls Sie bei den Aufgaben etwas nicht verstanden haben und die Musterl√∂sung es nicht aufkl√§ren konnte.","code":""},{"path":"reproduzieren.html","id":"reproduzieren","chapter":"Kapitel 3 R Markdown","heading":"Kapitel 3 R Markdown","text":"\nWichtigkeit der Reproduzierbarkeit erkl√§ren\n\nBegriff literate programming definieren\n\nAufbau einer RMarkdown-Datei erkl√§ren\n\nEinen einfachen ersten reproduzierbaren Bericht schreiben\n","code":""},{"path":"reproduzieren.html","id":"warum-reproduzierbarkeit-in-der-forschung-wichtig-ist","chapter":"Kapitel 3 R Markdown","heading":"3.1 Warum Reproduzierbarkeit in der Forschung wichtig ist","text":"","code":""},{"path":"reproduzieren.html","id":"literate-programming-idee-von-donald-knuth","chapter":"Kapitel 3 R Markdown","heading":"3.2 Literate Programming Idee von Donald Knuth","text":"Die Idee, dass man den Code und die dazugeh√∂rige Interpretation (Text, Bericht etc.) nicht von einander trennen sollte, geht auf Knuth (1984) zur√ºck. Mit Literate Programming meinte Knuth, dass Programme auch nichts anderes wie literarische Werke sind. Er setzte den Fokus darauf, mit Programmen menschlichen Benutzern zu erkl√§ren, man den Computer machen lassen m√∂chte. Also weg vom computer- hin zum mensch-zentrierten Zugang. wird Programmieren und unserem Fall die Datenanalyse verst√§ndlich und vor allem reproduzierbar.Leider ist es unserer modernen Forschungslandschaft immer noch nicht Standard. Das Trennen von Analyseergebnissen und Berichten (Forschungsartikeln) sorgt f√ºr viele (unentdeckte und unn√∂tige) Fehler und Frust.","code":""},{"path":"reproduzieren.html","id":"reproduzierbare-berichte-mit-r-markdown","chapter":"Kapitel 3 R Markdown","heading":"3.3 Reproduzierbare Berichte mit R Markdown","text":"R hat sein eigenes System von reproduzierbaren Berichten, genannt R Markdown (Xie, Allaire, Grolemund 2021). Es ist benutzerfreundlich und erm√∂glicht unterschiedliche Formate von Berichten, wie HTML-Dokumente, PDF-Dateien, Pr√§sentationsfolien usw.Es wird Sie vielleicht √ºberraschen, aber das Skript, das Sie gerade lesen ist nichts anderes als ein ‚Äúliterarisch‚Äù programmiertes Buch R Bookdown (Xie, Allaire, Grolemund 2021), einem R-Paket speziell f√ºr lange R Markdown-Dokumente.Wir werden vor allem mit R Notebooks arbeiten, die eine gute Interaktion zwischen dem geschriebenen Text und dem R-Code erm√∂glichen. Das Notebook kann sowohl ein HTML-Dokument als auch PDF oder Word als endg√ºltiges Dokument umgewandelt werden. Diesen Prozess nennt man knit.","code":""},{"path":"reproduzieren.html","id":"ein-neues-r-notebook-erstellen","chapter":"Kapitel 3 R Markdown","heading":"3.4 Ein neues R Notebook erstellen","text":"Um ein neues R Notebook zu erstellen, klicken Sie das leine gr√ºne Plus oben links und w√§hlen Sie R Notebook aus. Sie k√∂nnen es erst einmal bei untitled belassen (Abbildung 3.1).\nAbbildung 3.1: Neues R Notebook anlegen\nWenn Sie ein neues Notebook erstellen, enth√§lt das Template etwas Code. Lesen Sie sich das ruhig noch einmal durch, da es ein paar hilfreiche Tastenk√ºrzel und Tipps. Danach k√∂nnen Sie den Text unterhalb des Headers l√∂schen.","code":""},{"path":"reproduzieren.html","id":"header","chapter":"Kapitel 3 R Markdown","heading":"3.5 Der Header eines Notebooks","text":"Ein R Notebook (und jedes andere R Markdown Dokument) besteht aus einem Header (Kopf) und dem eigentlichen Text und Code. Der Header hat dabei ein bestimmtes Layout, auf das Sie unbedingt achten m√ºssen (Rechtschreibung!). Der Header ist immer zwischen drei Minuszeichen --- eingeschlossen. Bei komplizierteren Headern gibt es auch Einr√ºckungen (mit der Tab-Taste), die auch Bedeutung haben (s. weiterf√ºhrende Literatur). Wir bleiben bei einem einfachen Header ohne Einr√ºckungen (Abbildung 3.2).\nAbbildung 3.2: Einen neuen Chunk hinzuf√ºgen\nText kann einfach unterhalb des Headers und au√üerhalb der Chunks getippt werden. Die wichtigsten Layoutelemente f√ºr den Text finden Sie hier. R Markdown unterst√ºtzt mathematische Notation Latex-Stil. Eine Einf√ºhrung Latex w√ºrde dieser Stelle aber zu weit f√ºhren.Das R Notebook hat den Vorteil, dass man √ºber den Button Preview oben der Leiste sofort die Ergebnisse anzeigen lassen kann. Sie m√ºssen also nicht knitten. Falls Sie es doch m√∂chten, klicken Sie auf das kleine Dreieck neben dem Preview und suchen Sie sich ein Output-Format aus. Ein einmal ‚Äúgeknittetes‚Äù Notebook ist kein Notebook mehr (kein Preview). Damit es wierder zum Nobebook wird, m√ºssen Sie im Header output: html_notebbok einstellen (Abbildung 3.2).","code":""},{"path":"reproduzieren.html","id":"wichtigste-regeln-f√ºr-reproduzierbarkeit","chapter":"Kapitel 3 R Markdown","heading":"3.6 Wichtigste Regeln f√ºr Reproduzierbarkeit","text":"","code":""},{"path":"reproduzieren.html","id":"lesestoff-1","chapter":"Kapitel 3 R Markdown","heading":"3.7 Lesestoff","text":"Intro zu Kapitel 2 (Basics), Kapitel 3.2.1 und 3.2.2 Xie, Allaire, Grolemund (2021)","code":""},{"path":"reproduzieren.html","id":"weiterf√ºhrende-literatur","chapter":"Kapitel 3 R Markdown","heading":"3.8 Weiterf√ºhrende Literatur","text":"r4ds, Kapitel 27 (Wickham Grolemund 2021)","code":""},{"path":"reproduzieren.html","id":"aufgaben-1","chapter":"Kapitel 3 R Markdown","heading":"3.9 Aufgaben","text":"","code":""},{"path":"reproduzieren.html","id":"erstes-notebook","chapter":"Kapitel 3 R Markdown","heading":"3.9.1 Erstes Notebook","text":"Erstellen Sie ein R Notebook.F√ºgen Sie Layoutelemente hinzu:\n√úberschrift\nUnter√ºberschrift\nkursiver Text\nein Exponent: R2\nein Mathematikelement: \\(x^2\\)\neine Liste\n√úberschriftUnter√ºberschriftkursiver Textein Exponent: R2ein Mathematikelement: \\(x^2\\)eine ListeNutzen Sie die unter 3.5 verlinkte Liste der Layoutelemente.","code":""},{"path":"reproduzieren.html","id":"erste-schritte-als-notebook","chapter":"Kapitel 3 R Markdown","heading":"3.9.2 Erste Schritte als Notebook","text":"Wandeln Sie beide R-Skripte der ersten Sessions R Notebooks um.F√ºgen Sie mehr Erkl√§rungstext zu den einzelnen Schritten hinzuGliedern Sie Ihre Notebooks mit passenden Layoutelementen.","code":""},{"path":"reproduzieren.html","id":"exploration-eines-datensatzes","chapter":"Kapitel 3 R Markdown","heading":"3.9.3 Exploration eines Datensatzes","text":"Arbeiten Sie das Kapitel 1.4 Explore first datasets Ismay Kim (2021) durch.","code":""},{"path":"ggplot.html","id":"ggplot","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"Kapitel 4 Einf√ºhrung in ggplot2","text":"\nAufbau des Aufrufs der Funktion ggplot() kennen\n\n5 wichtigste Grafiktypen kennen und einsetzten\n","code":""},{"path":"ggplot.html","id":"aufbau-eines-visualisierungsbefehls","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.1 Aufbau eines Visualisierungsbefehls","text":"Das Paket ggplot2 ist ein sehr m√§chtiges Visualisierungswerkzeug. Der Name steht f√ºr ‚Äúgrammar graphics.‚Äù Das Bedeutet, dass man mit Hilfe von verschiedenen Funktion ggplot2 seine Grafik Schritt f√ºr Schritt aufbaut, wie einen (grammatikalisch korrekten) Satz. aller K√ºrze bedeutet das:Eine statistische Grafik ist eine Zuordnung (mapping) von Variablen einem Datensatz (data) zu (√§sthetischen) Attributen (aes) von geometrischen Objekten (geom).Wir m√ºssen also f√ºr das Visualisieren Folgendes festlegen:data: der Datensatz, der die Variablen enth√§lt, die wir darstellen m√∂chten.data: der Datensatz, der die Variablen enth√§lt, die wir darstellen m√∂chten.aes: (√§sthetische) Attribute f√ºr die geometrischen Objekte, die dargestellt werden sollen. Diese Attribute sind, z.B. die x und y Koordinaten, Farbe, Form und Gr√∂√üe der geometrischen Obekteaes: (√§sthetische) Attribute f√ºr die geometrischen Objekte, die dargestellt werden sollen. Diese Attribute sind, z.B. die x und y Koordinaten, Farbe, Form und Gr√∂√üe der geometrischen Obektegeom: geometrische Objekte, die dargestellt werden sollen, z.B. Punkte, Linien, Boxen, S√§ulen etc.geom: geometrische Objekte, die dargestellt werden sollen, z.B. Punkte, Linien, Boxen, S√§ulen etc.Wir laden zun√§chst die n√∂tigen Bibliotheken und filtern den Datensatz gapminder, um nur die Daten aus dem Jahr 2007 zu visualisieren. Die Bibliothek ggplot2 ist tidyverse enthalten und wird mitgeladen.","code":"\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder2007 <- gapminder %>% \n  filter(year == 2007)"},{"path":"ggplot.html","id":"punktdiagramm","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.2 Punktdiagramm","text":"Ein typischer Befehl zur Visualisierung w√ºrde also aussehen:Worten k√∂nnte man es vielleicht wie folgt umschreiben:Nimm den Datensatz (data) gapminder undNimm den Datensatz (data) gapminder undordne folgende Attribute zu:\nauf die x-Achse die Variable gdpPercap\nauf die y-Achse die Variable lifeExp\nf√§rbe ein mit Hilfe der Variablen continent\nbestimme die Gr√∂√üe der Symbole mit Hilfe der Variablen pop\nordne folgende Attribute zu:auf die x-Achse die Variable gdpPercapauf die y-Achse die Variable lifeExpf√§rbe ein mit Hilfe der Variablen continentbestimme die Gr√∂√üe der Symbole mit Hilfe der Variablen popStelle das Ganze als geometrisches Objekte Punkte dar (geom_point())Stelle das Ganze als geometrisches Objekte Punkte dar (geom_point())Sie sehen, dass diese Zuordnungen klar nach einer Legende verlangen, die dann auch automatisch sowohl f√ºr die Farbe als auch f√ºr die Gr√∂√üe der Symbole, erstellt wird.Die Anweisungen zur Visualisierung ggplot2 werden mit einem + verbunden. Man kann (und diesem Fall soll) weitere Anweisungen geben. Z.B. sind die Beschriftungen der beiden Achsen nichtssagend und m√ºssen verbessert werden. Wir h√§ngen mit eine +-Zeichen weitere Befehle hinzu:","code":"\nggplot(data = gapminder2007, mapping = aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  geom_point()\nggplot(data = gapminder2007, mapping = aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  geom_point() + \n  labs(x = 'Bruttoinlandsprodukt pro Kopf (US$)', y = 'Lebenserwartung (Jahre)',\n       color = 'Kontinent', size = 'Bev√∂lkerung')"},{"path":"ggplot.html","id":"weitere-geoms","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.3 Weitere geoms","text":"Das geom_point() produziert eine xy-Grafik (scatter plot). Weiter wichtige Grafiktypen sindgeom_line(): Liniengeom_histogram(): Histogrammgeom_boxplot(): Boxplotgeom_bar(): S√§ulen","code":""},{"path":"ggplot.html","id":"scatter","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.4 Liniendiagramm","text":"Es macht wenig Sinn, die obere Grafik mit Linien darzustellen. Allerdings eignen sich Linien sehr gut, um einen zeitlichen Verlauf zu visualisieren. Daher filtern wir aus dem Datensatz gapminder die Zeitreihen f√ºr Frankreich und Deutschland heraus. Weil wir jetzt zwei L√§nder haben m√∂chten, muss beim Filtern ein Vektor mit L√§ndernamen angegeben werden und statt == der Operator %%. Wir werden sp√§ter noch ausf√ºhrlich auf diese Operatoren zur√ºck kommen.","code":"\nfrance_germany <- gapminder %>% \n  filter(country %in% c('France', 'Germany'))\nggplot(data = france_germany, mapping = aes(x = year, y = gdpPercap, color = country)) +\n  geom_line()"},{"path":"ggplot.html","id":"histogramm","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.5 Histogramm","text":"Wie ist das GDP im Jahre 2007 Afrika und Europa verteilt? Dazu nutzen wir das Histogramm und filtern die Daten vorher entsprechend. Als √Ñsthetik eignet sich hier fill besser als color.","code":"\nafrica_europe <- gapminder2007 %>% \n  filter(continent %in% c('Africa', 'Europe'))\n\nggplot(africa_europe, mapping = aes(x = gdpPercap, fill = continent)) +\n  geom_histogram(bins = 20)"},{"path":"ggplot.html","id":"boxplot","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.6 Boxplot","text":"Wie ist das GDP im Jahre 2007 auf verschiedenen Kontinenten verteilt? Ein Histogramm mit allen Kontinenten w√ºrde schnell sehr un√ºbersichtlich werden. Das geht mit einem Boxplot besser.","code":"\nggplot(gapminder2007, mapping = aes(x = continent, y = gdpPercap)) +\n  geom_boxplot()"},{"path":"ggplot.html","id":"s√§ulendiagramm","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.7 S√§ulendiagramm","text":"Wie viele Eintr√§ge gibt es pro Kontinent? Das S√§ulendiagramm z√§hlt f√ºr uns die Eintr√§ge im Datensatz zusammen","code":"\nggplot(data = gapminder, mapping = aes(x = continent)) +\n  geom_bar()"},{"path":"ggplot.html","id":"lesestoff-2","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.8 Lesestoff","text":"Kapitel 2.1 Ismay Kim (2021)","code":""},{"path":"ggplot.html","id":"aufgaben-2","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.9 Aufgaben","text":"","code":""},{"path":"ggplot.html","id":"grafiken-richtig-beschriften","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.9.1 Grafiken richtig beschriften","text":"Bis auf die Grafik 4.4 fehlen bei den Grafiken oben ordentliche Achsenbeschriftungen und Titel f√ºr die Legenden. Erg√§nzen Sie den Code entsprechend.","code":""},{"path":"ggplot.html","id":"zeitreihen","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.9.2 Zeitreihen","text":"Stellen Sie den zeitlichen Verlauf der Lebenserwartung f√ºnf europ√§ischen L√§ndern Ihrer Wahl dar. F√§rben Sie die Linien nach L√§ndern.","code":""},{"path":"ggplot.html","id":"boxplots","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.9.3 Boxplots","text":"Stellen Sie die Lebenserwartung im Jahr 1952 und im Jahr 2007 pro Kontinent dar. Das sind zwei verschiedene Boxplots.","code":""},{"path":"ggplot.html","id":"ihre-arbeit-einreichen-1","chapter":"Kapitel 4 Einf√ºhrung in ggplot2","heading":"4.10 Ihre Arbeit einreichen","text":"Speichern Sie Ihr Notebook ab und laden Sie nur die .Rmd Datei vom Server.Laden Sie Ihre .Rmd Datei ILIAS hoch. Beachten Sie die Deadline!Sie erhalten die Musterl√∂sung nach dem Hochladen.","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"daten-einlesen-und-visualisieren","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"Kapitel 5 Daten einlesen und visualisieren","text":"\nDaten aus Textdateien R einlesen\n\ndata.frame speichern\n\nGrafiken anpassen (nebeneinander, Facetten, Transparenz)\n\nGrafiken speichern\n","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"daten-aus-textdateien-in-r-einlesen","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.1 Daten aus Textdateien in R einlesen","text":"Um Daten aus Textdateien (z.B. aus .csv, .txt, .dat) R zu importieren (.e.¬†einzulesen) werden wir die Bibliothek readr aus tidyverse benutzen. Wir laden erst einmal tidyverse.Wir gehen davon aus, dass die Daten im Ordner data gespeichert sind. Falls Ihre Daten einem anderen Ort abgelegt sind, m√ºssen Sie den Pfad beim Einlesen entsprechend anpassen.Um die Daten zu laden, gibt es der Bibliothek readr verschiedene Funktionen, die alle mit read_ beginnen. Die allgemeinste davon ist read_delim. Darin kann man explizit einstellen, mit welchem Zeichen (z.B. Komma, Strichpunkt etc.) die einzlenen Spalten der zu importierenden Datei getrennt sind.Ein kurzer Blick auf den Datensatz. Hierbei handelt es sich um Daten zu Treibhausgasemissionen auf der EU-Ebene, die ich bei eurostat 30.4.2021 heruntergeladen und vorgefiltert habe. Die Datenbank bietet sehr viele Datens√§zte und ist als Quelle f√ºr Berichte hervorragend geeignet üòÑ.Das Ergebnis des Einlesens mit read_ Funktionen ist immer ein tibble. Kategorische Variablen werden als Text (character) eingelesen und nicht factor umgewandelt. Wenn man factor m√∂chte, muss man die Variablen per Hand umwandeln.Wir verschaffen uns einen kurzen √úberblick √ºber die Daten.Um die Anzahl der einzelnen L√§nder zu ermitteln, sehen wir uns die L√§nge der Ausgabe der Funktion unique() , die die einzelnen verschiedenen Eintr√§ge ermitteln kann. Es sind Eintr√§ge f√ºr 33 verschiedene L√§nder vorhanden.","code":"\nlibrary(tidyverse)\nemissions <- read_delim(file = 'data/emissions.csv', delim = ';')## \n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## cols(\n##   unit = col_character(),\n##   airpol = col_character(),\n##   vehicle = col_character(),\n##   geo = col_character(),\n##   time = col_date(format = \"\"),\n##   values = col_double()\n## )\nemissions## # A tibble: 2,871 x 6\n##    unit     airpol                  vehicle    geo             time       values\n##    <chr>    <chr>                   <chr>      <chr>           <date>      <dbl>\n##  1 Million‚Ä¶ Greenhouse gases (CO2,‚Ä¶ Fuel comb‚Ä¶ Austria         2018-01-01  14.4 \n##  2 Million‚Ä¶ Greenhouse gases (CO2,‚Ä¶ Fuel comb‚Ä¶ Belgium         2018-01-01  14.4 \n##  3 Million‚Ä¶ Greenhouse gases (CO2,‚Ä¶ Fuel comb‚Ä¶ Bulgaria        2018-01-01   5.78\n##  4 Million‚Ä¶ Greenhouse gases (CO2,‚Ä¶ Fuel comb‚Ä¶ Switzerland     2018-01-01  11.0 \n##  5 Million‚Ä¶ Greenhouse gases (CO2,‚Ä¶ Fuel comb‚Ä¶ Cyprus          2018-01-01   1.38\n##  6 Million‚Ä¶ Greenhouse gases (CO2,‚Ä¶ Fuel comb‚Ä¶ Czechia         2018-01-01  11.9 \n##  7 Million‚Ä¶ Greenhouse gases (CO2,‚Ä¶ Fuel comb‚Ä¶ Germany (until‚Ä¶ 2018-01-01  97.8 \n##  8 Million‚Ä¶ Greenhouse gases (CO2,‚Ä¶ Fuel comb‚Ä¶ Denmark         2018-01-01   6.85\n##  9 Million‚Ä¶ Greenhouse gases (CO2,‚Ä¶ Fuel comb‚Ä¶ Estonia         2018-01-01   1.52\n## 10 Million‚Ä¶ Greenhouse gases (CO2,‚Ä¶ Fuel comb‚Ä¶ Greece          2018-01-01   7.61\n## # ‚Ä¶ with 2,861 more rows\nsummary(emissions)##      unit              airpol            vehicle              geo           \n##  Length:2871        Length:2871        Length:2871        Length:2871       \n##  Class :character   Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n##                                                                             \n##                                                                             \n##                                                                             \n##                                                                             \n##       time                values         \n##  Min.   :1990-01-01   Min.   :  0.00609  \n##  1st Qu.:1997-01-01   1st Qu.:  0.25564  \n##  Median :2004-01-01   Median :  1.92403  \n##  Mean   :2004-01-01   Mean   :  8.52836  \n##  3rd Qu.:2011-01-01   3rd Qu.:  6.93899  \n##  Max.   :2018-01-01   Max.   :119.77824  \n##                       NA's   :232\nlength(unique(emissions$geo))## [1] 33"},{"path":"daten-einlesen-und-visualisieren.html","id":"legende-verschieben-und-facetten","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.2 Legende verschieben und Facetten","text":"Wir stellen die Zeitreihen der Emissionen eingef√§rbt nach Land dar. Die L√§nder stehen der Variablen geo.Als erstes f√§llt auf, dass die Legende sehr umfangreich ist (wir haben ja 33 L√§nder im Datensatz). Daher w√§re es g√ºnstig, die Legende unterhalb der Grafik zu positionieren und den Titel der Legende oberhalb der Legende zu belassen. Das geht mit Hilfe der Funktionen theme() und guides(). Wie immer, werden sie im Plotaufbau (denken Sie grammer graphics) mit + angeh√§ngt.Die Zeitreihen sehen echt seltsam aus. Wenn wir uns die Variable vehicle ansehen, wird auch klar, warum. Wir stellen gerade Emissionen f√ºr verschiedene Fahrzeuge dar, d.h. wir mischen mehrere Zeitreihen zusammen.Die einfachste L√∂sung ist, drei verschiedene Grafiken pro Verkehrsmittel zu erstellen. Dies gelingt sehr leicht mit der Funktion facet_wrap(), die den Namen der Variablen erwartet, mit Hilfe derer die Grafiken gesplittet werden sollen. Vor der Variablen muss eine Tilde (~) stehen. unserem Fall wollen wir nach Verkehrsmittel splitten, d.h. mit Hilfe der Varialben vehicle.Da die Emissionen sehr unterschiedlich sind, macht es Sinn, die Skalierungen der y-Achsen anzupassen. Aber Achtung: Das sollten Sie Ihren Berichten unbedingt ansprechen (z.B. der Bildunterschrift), da man unterschiedliche Skalierunge sehr leicht √ºbersieht und dann die Interpretation der Daten leicht die falsche Richtung gehen kann. Der Funktionsparameter labeller = label_wrap_gen() sorgt f√ºr geschickte Zeilenumbr√ºche bei zu langen Labels. Zum Vergleich k√∂nnen Sie ihn mal weglassen und sehen, dann passiert.","code":"\nggplot(data = emissions, mapping = aes(x = time, y = values, colour = geo)) +\n  geom_line()## Warning: Removed 7 row(s) containing missing values (geom_path).\nggplot(data = emissions, mapping = aes(x = time, y = values, colour = geo)) +\n  geom_line() +\n  theme(legend.position = \"bottom\") +\n  guides(colour = guide_legend(title.position = \"top\"))## Warning: Removed 7 row(s) containing missing values (geom_path).\nunique(emissions$vehicle)## [1] \"Fuel combustion in cars\"                       \n## [2] \"Fuel combustion in heavy duty trucks and buses\"\n## [3] \"Fuel combustion in railways\"\nggplot(data = emissions, mapping = aes(x = time, y = values, colour = geo)) +\n  geom_line() +\n  facet_wrap(~vehicle) +\n  theme(legend.position = \"bottom\") +\n  guides(colour = guide_legend(title.position = \"top\"))## Warning: Removed 203 row(s) containing missing values (geom_path).\nggplot(data = emissions, mapping = aes(x = time, y = values, colour = geo)) +\n  geom_line() +\n  facet_wrap(~vehicle, scales = 'free_y', labeller = label_wrap_gen()) +\n  theme(legend.position = \"bottom\") +\n  guides(colour = guide_legend(title.position = \"top\"))## Warning: Removed 203 row(s) containing missing values (geom_path)."},{"path":"daten-einlesen-und-visualisieren.html","id":"fehlerbalken-und-co.","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.3 Fehlerbalken und Co.","text":"Um die Variabilit√§t der Daten grafisch darzustellen, bieten sich Fehlerbalken, Bereiche etc. . Daf√ºr hat ggplot2 spezielle geoms. Hier ein Beispiel inspiriert vom dem Buch ggplot2 (Wickham 2020).Sie sehen, dass man ggplot Objekte wie andere Objekte R zuweisen kann, um mit ihnen sp√§ter zu arbeiten. diesem Fall ist basis_plot ein ggplot Objekt.","code":"\ny <- c(10, 5, 23)\ndf <- tibble(x = 1:3, y = y, se = c(0.9, 1.5, 3.3))\n\nbasis_plot <- ggplot(df, aes(x, y, ymin = y - se, ymax = y + se))\nbasis_plot + geom_pointrange()\nbasis_plot + geom_errorbar()\nclass(basis_plot)## [1] \"gg\"     \"ggplot\""},{"path":"daten-einlesen-und-visualisieren.html","id":"mehrere-grafiken-nebeneinander","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.4 Mehrere Grafiken nebeneinander","text":"Um mehrere Grafiken nebeneinander zu plotten, nutzen wir die Funktion grid.arrange() aus der Bibliothek gridExtra. Um sie zu nutzen, m√ºssen wir die ggplot Objekte abspeichern und dann mit Hilfe der Funktion grid.arrange, wie der Name schon sagt, ‚Äúarrangieren.‚Äù diesem Fall wollen wir der Gesamtgrafik eine Zeile (nrow = 1), sodass die Grafiken nebeneinander stehen.Um die Grafiken untereinander abzubilden, bestellen wir entsprechend zwei Zeilen.","code":"\nlibrary(gridExtra)\n\np1 <- basis_plot + geom_pointrange()\np2 <- basis_plot + geom_errorbar()\n\nalles <- grid.arrange(p1, p2, nrow = 1)\nalles <- grid.arrange(p1, p2, nrow = 2)"},{"path":"daten-einlesen-und-visualisieren.html","id":"grafiken-abspeichern","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.5 Grafiken abspeichern","text":"","code":"\nggsave(filename = 'Fehlerbalken.pdf', plot = alles, device = 'pdf', width = 7, height = 5)"},{"path":"daten-einlesen-und-visualisieren.html","id":"weitere-statistsiche-zusammenfassungen-in-grafiken","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.6 Weitere statistsiche Zusammenfassungen in Grafiken","text":"Arbeiten Sie selbst√§ndig das Kapitel 5: Statistical summaries Wickham (2020) (https://ggplot2-book.org/statistical-summaries.html).","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"lesestoff-3","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.7 Lesestoff","text":"Kapitel 2.2 bis 2.9 Ismay Kim (2021)","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"aufgaben-3","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.8 Aufgaben","text":"","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"grafiken-richtig-beschriften-1","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.8.1 Grafiken richtig beschriften","text":"Beschriften Sie die finale Grafik der Zeitreihen (Achsen, Titel, Legende).","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"bestandesaufnahme","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.8.2 Bestandesaufnahme im Wald","text":"Ar Stat arbeitet als HiWi der AG √ñkosystemforschung und soll im Nationalpark Eifel eine Bestandsaufnahme durchf√ºhren (d.h. Baumh√∂hen und -durchmesser vermessen). Er notiert den BHD (Brusth√∂hendurchmesser) und die Art der B√§ume.Lesen Sie den Datensatz BHD.txt ein und ordnen Sie ihn der Variable BHD zu.Erstellen Sie einen Vektor Nr mit durchlaufenden Baumnummern. Von welcher Art sind die Elemente des Vektors ?F√ºgen Sie die Datens√§tze BHD und Nr zu einem tibble zusammen und benennen Sie die Spalten sinnvoll.L√∂schen Sie den Vektor Nr.Lesen Sie den Datensatz Art.txt ein und ordnen Sie ihn der Variablen art zu.F√ºgen Sie die Art das tibble ein.Erstellen Sie eine Tabelle mit der Anzahl der jeweiligen Arten. Nutzen Sie die Funktion table().Speichern Sie die Tabelle mit write_delim() ab. Schlagen Sie der Hilfe nach, wie diese Funktion arbeitet!","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"wahlbeteiligung","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.8.3 Wahlbeteiligung bei der Bundestagswahl 2017","text":"Bauen Sie die Grafik nach (Abbildung 5.1).\nAbbildung 5.1: Wahlbeteiligung bei den Bundestagswahlen. Quelle: Der Bundeswahlleiter.\nLesen Sie den Datensatz Wahlbeteiligung.csv R ein und ordnen Sie ihn dem Objekt beteiligung zu.Sehen Sie sich den Datensatz und fassen Sie ihn zusammen.Stellen Sie die Wahlbeteiligung als Funktion der Zeit dar, wie Abbildung 5.1 gezeigt.Beschriften Sie die Grafik.Speichern Sie die Grafik als pdf ab.","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"zweitstimme","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.8.4 Zweitstimme bei der Bundestagswahl 2017","text":"Bauen Sie die Grafik nach (Abbildung 5.2).\nAbbildung 5.2: Zweitstimme bei der Bundestagswahl 2017. Quelle: Der Bundeswahlleiter.\nLesen Sie den Datensatz Zweitstimme.csv R ein und ordnen Sie ihn dem Objekt zweitstimme zu.Sehen Sie sich den Datensatz und fassen Sie ihn zusammen.Stellen Sie die die Zweitstimmen pro Partei einem S√§ulendiagramm dar. Nutzen Sie das geom geom_col() und lesen Sie den Unterschied zu geom_bar() der Hilfe nach. Tipps:\nDer Variablenname Zweitstimme 2017 enth√§lt ein Leerzeichen. Daher m√ºssen Sie es beim Aufruf zu ggplot unbedingt ‚Äú`‚Äù setzten.\nDamit die Parteien der selben Reihenfolge dargestellt werden, wie im Datensatz angegeben, wandeln Sie die Spalte Partei ein factor um: zweitstimme$Partei <- as_factor(zweitstimme$Partei).\nFarben stellen Sie direkt geom_col() ein mit fill = c('black', 'red', 'magenta', 'darkgreen', 'yellow', 'blue', 'grey')\nDer Variablenname Zweitstimme 2017 enth√§lt ein Leerzeichen. Daher m√ºssen Sie es beim Aufruf zu ggplot unbedingt ‚Äú`‚Äù setzten.Damit die Parteien der selben Reihenfolge dargestellt werden, wie im Datensatz angegeben, wandeln Sie die Spalte Partei ein factor um: zweitstimme$Partei <- as_factor(zweitstimme$Partei).Farben stellen Sie direkt geom_col() ein mit fill = c('black', 'red', 'magenta', 'darkgreen', 'yellow', 'blue', 'grey')Beschriften Sie die Grafik.Speichern Sie die Grafik als pdf ab.","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"zweigrafiken","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.8.5 Ergebnisse der Bundestagswahl in einer Grafik","text":"Stellen Sie beide Grafiken untereinander dar wie Abbildung (5.3) gezeigt.\nAbbildung 5.3: Ergebnisse der Bundestagswahl 2017. Quelle: Der Bundeswahlleiter.\n","code":""},{"path":"daten-einlesen-und-visualisieren.html","id":"ihre-arbeit-einreichen-2","chapter":"Kapitel 5 Daten einlesen und visualisieren","heading":"5.9 Ihre Arbeit einreichen","text":"Speichern Sie Ihr Notebook ab und laden Sie nur die .Rmd Datei vom Server.Laden Sie Ihre .Rmd Datei ILIAS hoch. Beachten Sie die Deadline!Sie erhalten die Musterl√∂sung nach dem Hochladen.","code":""},{"path":"tidyverse.html","id":"tidyverse","chapter":"Kapitel 6 Der explorative Workflow mit tidyverse","heading":"Kapitel 6 Der explorative Workflow mit tidyverse","text":"\nKernpakete aus tidyverse benennen\n\nein einfaches Workflow (Daten einlesen, zusammenfassen, darstellen) mit tidyverse durchf√ºhren\n\nFunktionen des Pakets dplyr f√ºr Datentransformation anwenden\ntidyverse ist eine Sammlung von R-Pakete, die explizit f√ºr Datenanalyse entwickelt wurden (https://www.tidyverse.org/). tidyverse versucht durch gemeinsame Philosophie Design, Grammatik und Datenstruktur die Datenanalyse zu erleichtern (https://design.tidyverse.org/). Auch wenn tidyverse auf den ersten Blick etwas fremd erscheint, es ist ein Teil von R, kein eigenes Universum. Es ist also v√∂llig Ordnung, R-Basisfunktionen mit Funktionen aus tidyverse zu mischen.Das wichtigste Einf√ºhrungsbuch zu tidyverse ist sicherlich R4DS: ‚ÄúR Data Science‚Äù (Wickham Grolemund 2021), das Sie kostenlos online lesen k√∂nnen (https://r4ds..co.nz/).","code":""},{"path":"tidyverse.html","id":"grundpakete","chapter":"Kapitel 6 Der explorative Workflow mit tidyverse","heading":"6.1 Grundpakete","text":"tidyverse enth√§lt folgende Grundpakete, die alle installiert werden, wenn Sie install.packages('tidyverse') ausf√ºhren.Jedes dieser Pakete hat ein Cheat Sheet, eine √ºbersichtliche Zusammenstellung der Funktionen des Pakets. Sie bekommen die Cheet Sheats √ºber die tidyverse-Seite (https://www.tidyverse.org/packages/), indem Sie auf das jeweilige Paket klicken und zum Abschnitt ‚ÄòCheatsheet‚Äô scrollen.","code":""},{"path":"tidyverse.html","id":"der-explorative-workflow","chapter":"Kapitel 6 Der explorative Workflow mit tidyverse","heading":"6.2 Der explorative Workflow","text":"","code":""},{"path":"tidyverse.html","id":"daten-einlesen-revisited","chapter":"Kapitel 6 Der explorative Workflow mit tidyverse","heading":"6.2.1 Daten einlesen, revisited","text":"Als erstes laden wir die Bibliothek tidyverse.Sie kennen bereits die Funktion read_delim() zum Einlesen von Textdateien. Die Funktion ist die allgemeinste Funktion der read_* Familie aus readr tidyverse; read_csv() und read_csv2() sind jeweils f√ºr komma- und strichpunkt-getrennte Datens√§tze gedacht. der Basisinstallation von R (also au√üerhalb von tidyverse) gibt die sehr umfangreiche Funktion read.table(), die ebenfalls zum Einlesen von Textdateien verwendet wird. Man k√∂nnte berechtigterweise fragen, warum neue Funktion (read_*) f√ºr etwas erfinden, es schon gibt. Die Autoren von tidyverse versprechen Konsistenz und Geschwindigkeit. Ersteres war schon immer ein Problem von R, da es nicht von Computerspezialisten, sondern von Anwendern erfunden wurde. Daher ist eine Vereinheitlichung durch tidyverse mehr als willkommen. Und Geschwindigkeit ist sp√§testens bei gr√∂√üeren Datens√§tzen ein wichtiger Punkt.Wir sehen uns Daten des Deutschen Wetterdienstes , die ich 24. Mai 2020 herunter geladen habe (https://www.dwd.de/DE/leistungen/klimadatendeutschland/klimadatendeutschland.html). Auch das ist eine tolle Datenquelle f√ºr Berichte üòÑ. Der Datensatz enth√§lt Stundenwerte f√ºr relative Luftfeuchte (%) und Lufttemperatur (¬∞C) von drei Wetterstationen, n√§mlich Hof, Frankfurt und K√∂ln-Bonn. Die Daten sind der Datei Drei_Stationen.csv gespeichert.Beim Einlesen zeigt Ihnen read_delim() bereits, welche Spalten und welche Datentypen es erkennt, mit trim_ws = T werden Leerzeichen aus Spalten entfernt.Eine weitere Kontrolle bietet die Funktion print(), die das eingelesene Ergebnis √ºbersichtlich (und im Notebook interaktiv) darstellt. Sie m√ºssen hier nicht head() verwenden, da grunds√§tzlich nur die ersten 10 Zeilen dargestellt werden.Das gleiche Ergebnis bekommen Sie auch ohne print(), wenn Sie wie gewohnt den Namen des Objekts tippen.diesem Datensatz sind folgende Variablen (Spalten) enthalten (s. Datensatzbeschreibung des DWDs)Das Objekt temp_humid ist ein tibble, ein data.frame mit ‚Äúmodernem‚Äù Verhalten. Z.B. gibt die Funktion print() nur die ersten 10 Zeilen aus, die Datentypen den Spalten werden hellgrau zwischen ‚Äò<>‚Äô mit angegeben etc. Mehr zu Tibbles finden Sie Kapitel 10 ‚ÄúTibbles‚Äù R4DS.","code":"\nlibrary(tidyverse)\ntemp_humid <- read_delim('data/Drei_Stationen.csv', delim = ';',    trim_ws = T)## \n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## cols(\n##   STATIONS_ID = col_double(),\n##   MESS_DATUM = col_double(),\n##   QN_9 = col_double(),\n##   TT_TU = col_double(),\n##   RF_TU = col_double(),\n##   eor = col_character()\n## )\nprint(temp_humid)## # A tibble: 39,600 x 6\n##    STATIONS_ID MESS_DATUM  QN_9 TT_TU RF_TU eor  \n##          <dbl>      <dbl> <dbl> <dbl> <dbl> <chr>\n##  1        2261 2018111900     3  -2.8    99 eor  \n##  2        2261 2018111901     3  -2.5   100 eor  \n##  3        2261 2018111902     3  -2.3   100 eor  \n##  4        2261 2018111903     3  -2     100 eor  \n##  5        2261 2018111904     3  -1.9    99 eor  \n##  6        2261 2018111905     3  -2.1    99 eor  \n##  7        2261 2018111906     3  -1.8    99 eor  \n##  8        2261 2018111907     3  -1.5    99 eor  \n##  9        2261 2018111908     3  -1.1    99 eor  \n## 10        2261 2018111909     3  -0.6    97 eor  \n## # ‚Ä¶ with 39,590 more rows\ntemp_humid## # A tibble: 39,600 x 6\n##    STATIONS_ID MESS_DATUM  QN_9 TT_TU RF_TU eor  \n##          <dbl>      <dbl> <dbl> <dbl> <dbl> <chr>\n##  1        2261 2018111900     3  -2.8    99 eor  \n##  2        2261 2018111901     3  -2.5   100 eor  \n##  3        2261 2018111902     3  -2.3   100 eor  \n##  4        2261 2018111903     3  -2     100 eor  \n##  5        2261 2018111904     3  -1.9    99 eor  \n##  6        2261 2018111905     3  -2.1    99 eor  \n##  7        2261 2018111906     3  -1.8    99 eor  \n##  8        2261 2018111907     3  -1.5    99 eor  \n##  9        2261 2018111908     3  -1.1    99 eor  \n## 10        2261 2018111909     3  -0.6    97 eor  \n## # ‚Ä¶ with 39,590 more rows\nclass(temp_humid)## [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\""},{"path":"tidyverse.html","id":"geschickter-umgang-mit-zeit-und-datum","chapter":"Kapitel 6 Der explorative Workflow mit tidyverse","heading":"6.3 Geschickter Umgang mit Zeit und Datum","text":"Ein weiteres Paket, dass zwar nicht zum Kern von tidyverse geh√∂rt, jedoch trotzdem extrem n√ºtzlich ist, hei√üt lubridate. Es hilft, Text sehr einfach richtige Datums-Objekte zu transformieren (Base-R muss man sich daf√ºr kryptischen Datumsformate merken). Wir transformieren die Spalte temp_humid$MESS_DATUM ein richtiges Datum mit Uhrzeit. Die Funktion ymd_h() kann character ein richtiges Datumsformat transformieren, wenn das Datum als year, month, day, hour codiert ist. Es gibt noch weitere Varianten der Codierung, die Sie bei Bedarf der Hilfe nachschlagen sollten.","code":"\nlibrary(lubridate)\n\ntemp_humid$MESS_DATUM <- ymd_h(temp_humid$MESS_DATUM)\n\ntemp_humid## # A tibble: 39,600 x 6\n##    STATIONS_ID MESS_DATUM           QN_9 TT_TU RF_TU eor  \n##          <dbl> <dttm>              <dbl> <dbl> <dbl> <chr>\n##  1        2261 2018-11-19 00:00:00     3  -2.8    99 eor  \n##  2        2261 2018-11-19 01:00:00     3  -2.5   100 eor  \n##  3        2261 2018-11-19 02:00:00     3  -2.3   100 eor  \n##  4        2261 2018-11-19 03:00:00     3  -2     100 eor  \n##  5        2261 2018-11-19 04:00:00     3  -1.9    99 eor  \n##  6        2261 2018-11-19 05:00:00     3  -2.1    99 eor  \n##  7        2261 2018-11-19 06:00:00     3  -1.8    99 eor  \n##  8        2261 2018-11-19 07:00:00     3  -1.5    99 eor  \n##  9        2261 2018-11-19 08:00:00     3  -1.1    99 eor  \n## 10        2261 2018-11-19 09:00:00     3  -0.6    97 eor  \n## # ‚Ä¶ with 39,590 more rows"},{"path":"tidyverse.html","id":"daten-zusammenfassen","chapter":"Kapitel 6 Der explorative Workflow mit tidyverse","heading":"6.3.1 Daten zusammenfassen","text":"Die drei Wetterstationen haben folgende IDs:Wir z√§hlen nach, wie viele Messpunkte es pro Station gibt. Dazu m√ºssen wir den Datensatz nach der Variablen STATION_ID gruppieren und dann pro Gruppe die Anzahl der Datenpunkte ermitteln:Die Zeichenkombination %>% hei√üt Pipe-Operator (pipe) und wird als ‚Äòund dann‚Äô gelesen (). Der Ausdruck temp_humid %>% group_by(STATIONS_ID) %>% count() hei√üt also: nimm das Objekt temp_humid, gruppiere es nach der Variablen STATION_ID und dann z√§hle die Eintr√§ge pro Gruppe zusammen. Der Pipe-Operator ist die Kernphilosophie von tidyverse und wird Ihnen √ºberall begegnen. Der Operator stammt aus dem Paket magrittr (https://magrittr.tidyverse.org/). Seine Hauptaufgabe ist es, den Code √ºbersichtlicher und besser lesbar zu machen (vielleicht nicht gleich zu Beginn der Lernkurve aber schon sehr bald üòé).","code":"\nstation_ids <-  c('2261' = 'Hof', '1420' = 'Frankfurt', '2667' = 'Koeln')\ntemp_humid %>% \n  group_by(STATIONS_ID) %>% \n  count()## # A tibble: 3 x 2\n## # Groups:   STATIONS_ID [3]\n##   STATIONS_ID     n\n##         <dbl> <int>\n## 1        1420 13200\n## 2        2261 13200\n## 3        2667 13200"},{"path":"tidyverse.html","id":"die-grammatik-der-datenmanipulation-dplyr","chapter":"Kapitel 6 Der explorative Workflow mit tidyverse","heading":"6.4 Die Grammatik der Datenmanipulation ‚Äì dplyr","text":"Die Funktion count() geh√∂rt zum Paket dplyr, das f√ºr Datentransformationen zust√§ndig ist. Es ist mal wieder eine Grammatik. Dieses Paket enth√§lt 5 Grundfunktionen (alle nach Verben benannt, damit man gleich wei√ü, frau tut üòÑ):Wir m√∂chten nur von einer bestimmten Station die Anzahl der Messwerte wissen m√∂chten, dann filtern wir vorher.Beim Filtern l√§uft eine logische Abfrage. D.h. es wird bei jeden Eintrag STATION_ID nachgesehen, ob da der Wert 2667 steht. Wenn da 2667 steht, dann gibt == ein TRUE zur√ºck, wenn da etwas anderes steht, dann gibt == ein FALSE zur√ºck. Und die Funktion filter() beh√§lt nur die Zeilen, bei denen == ein TRUE zur√ºck gegeben hat.Weiter wichtige logische und relationale Operatoren finden Sie hier der Hilfe zu filter(). Hier ein paar einfache BeispieleZudem kann man bei filter() die Anfragen auch kombinieren. Wir wollen z.B. die Stationen K√∂ln und Hof haben. | ist der logische Operator oder. Wenn man also sowohl K√∂ln als auch Hof haben , sagt man: finde alles, entweder gleich K√∂ln oder gleich Hof ist.Das Gleiche erreicht man mit folgendem Code, indem man Frankfurt ausschlie√üt:Alternative kann man auch den Operator %% verwenden. Dieser ist sehr n√ºtzlich, wenn man anhand einer einzelnen Variablen filtert, aber unterschiedliche Eintr√§ge ausw√§hlen m√∂chte (z.B. zwei Messstationen). Es wird bei jeder Zeile der Variablen STATIONS_ID nun √ºberpr√ºft, ob hier entweder 2667 oder 2261 stehen.","code":"\ntemp_humid %>% \n  filter(STATIONS_ID == '2667') %>%\n  count()## # A tibble: 1 x 1\n##       n\n##   <int>\n## 1 13200\ntemp_humid %>% \n  filter(STATIONS_ID == '2667' | STATIONS_ID == '2261') %>%\n  group_by(STATIONS_ID) %>% \n  count()## # A tibble: 2 x 2\n## # Groups:   STATIONS_ID [2]\n##   STATIONS_ID     n\n##         <dbl> <int>\n## 1        2261 13200\n## 2        2667 13200\ntemp_humid %>% \n  filter(STATIONS_ID != '1420') %>%\n  group_by(STATIONS_ID) %>% \n  count()## # A tibble: 2 x 2\n## # Groups:   STATIONS_ID [2]\n##   STATIONS_ID     n\n##         <dbl> <int>\n## 1        2261 13200\n## 2        2667 13200\ntemp_humid %>% \n  filter(STATIONS_ID %in% c('2667', '2261')) %>%\n  group_by(STATIONS_ID) %>% \n  count()## # A tibble: 2 x 2\n## # Groups:   STATIONS_ID [2]\n##   STATIONS_ID     n\n##         <dbl> <int>\n## 1        2261 13200\n## 2        2667 13200"},{"path":"tidyverse.html","id":"daten-plotten","chapter":"Kapitel 6 Der explorative Workflow mit tidyverse","heading":"6.4.1 Daten plotten","text":"Wir sehen uns die Daten erst mal , bevor wir weiter machen. Wir plotten die Temperatur. Weil es sich um Zeitreihen handelt, m√∂chten wir sie eher untereinander als nebeneinander haben. Daher setzen wir bei facet_wrap() den Parameter nrow = 3.","code":"\nggplot(data = temp_humid, aes(x = MESS_DATUM, y = TT_TU)) + \n  geom_line() +\n  facet_wrap(~STATIONS_ID, nrow = 3) +\n  labs(x = 'Zeit', y = 'Temperatur (¬∞C)')"},{"path":"tidyverse.html","id":"neue-variablen-erstellen-mit-mutate","chapter":"Kapitel 6 Der explorative Workflow mit tidyverse","heading":"6.4.2 Neue Variablen erstellen mit mutate()","text":"Wir wollen nun die Monatsmittelwerte und die Standardabweichungen f√ºr die Temperatur berechnen und diese darstellen. Als erstes erstellen wir zwei neue Spalten, die jeweils das Jahr und den Monat beinhalten. Die beiden neuen Spalten werden Ende von temp_humid angeh√§ngt. Um neue Spalten zu erstellen, nutzen wir die Funktion mutate(). Die Funktionen year()und month() geh√∂ren zur Bibliothek lubridate und extrahieren jeweils das Jahr und den Monat aus MESS_DATUM.Jetzt k√∂nnen wir einen neuen Datensatz mit den Mittelwerten erstellen. Daf√ºr gruppieren wir erst einmal die Daten nach STATIONS_ID, year und month. Die Mittelwerte sollen ja je Station, Jahr und Monat berechnet werden. Beim Gruppieren gibt man die Variablen ohne Anf√ºhrungszeichen und ohne einen Vektor zu bilden einfach durch Kommas getrennt .Die Struktur von monthly_means zeigt uns, dass es sich um gruppierte Daten handelt.Da wir aber mit den Daten weiter rechnen wollen, ist es besser, die Gruppierung wieder aufzugeben. Es k√∂nnte sonst sp√§ter Fehlermeldungen geben.Um die Daten als Zeitreihen zu plotten, erstellen wir noch eine ordentliche Zeit-Spalte. Die Funktion parse_date_time() kann aus Character richtige Datums-Zeitobjekte erstellen. Sie ist allgemeiner als die oben verwendete ymd_h() Funktion, da man hier das Format explizit angeben kann. unserem Fall ist das Format ‚Äòym‚Äô f√ºr Jahr und Monat.Der Code paste0(year, month) ‚Äúklebt‚Äù die Daten der Variablen year und month zusammen. Das ist n√∂tig, da die Funktion parse_date_time() einen Charaktervektor als Input erwartet und keine zwei getrennten Spalten. Da das Datum au√üer dem Jahr und dem Monat noch einen Tag braucht, hat parse_date_time() den ersten eines jeden Monats genommen.Alternativ k√∂nnen wir die Mittelwerte mit den Standardabweichungen darstellen.Oder, weil es gerade Spa√ü macht, als halb-transparentes Band. Ich hoffe, Sie haben jetzt Lust, das Kapitel 5 im ggplot2 Buch zu lesen üòé.Ein letzter Trick. Die √úberschriften f√ºr die Teilgrafiken sind ungeschickt, da man die IDs als Mensch einfach nicht zuordnen kann. Weiter oben haben wir einen benannten Vektor definiert, der die Klarnamen enth√§lt.Diesen Vektor nutzen wir als Titel.","code":"\ntemp_humid <- temp_humid %>% \n  mutate(year = year(MESS_DATUM),\n         month = month(MESS_DATUM))\n\ntemp_humid## # A tibble: 39,600 x 8\n##    STATIONS_ID MESS_DATUM           QN_9 TT_TU RF_TU eor    year month\n##          <dbl> <dttm>              <dbl> <dbl> <dbl> <chr> <dbl> <dbl>\n##  1        2261 2018-11-19 00:00:00     3  -2.8    99 eor    2018    11\n##  2        2261 2018-11-19 01:00:00     3  -2.5   100 eor    2018    11\n##  3        2261 2018-11-19 02:00:00     3  -2.3   100 eor    2018    11\n##  4        2261 2018-11-19 03:00:00     3  -2     100 eor    2018    11\n##  5        2261 2018-11-19 04:00:00     3  -1.9    99 eor    2018    11\n##  6        2261 2018-11-19 05:00:00     3  -2.1    99 eor    2018    11\n##  7        2261 2018-11-19 06:00:00     3  -1.8    99 eor    2018    11\n##  8        2261 2018-11-19 07:00:00     3  -1.5    99 eor    2018    11\n##  9        2261 2018-11-19 08:00:00     3  -1.1    99 eor    2018    11\n## 10        2261 2018-11-19 09:00:00     3  -0.6    97 eor    2018    11\n## # ‚Ä¶ with 39,590 more rows\nmonthly_means <- temp_humid %>%\n  group_by(STATIONS_ID, year, month) %>% \n  summarize(mean_T = mean(TT_TU), mean_RH = mean(RF_TU),\n            sd_T = sd(TT_TU), sd_RH = sd(RF_TU))## `summarise()` has grouped output by 'STATIONS_ID', 'year'. You can override using the `.groups` argument.\nmonthly_means## # A tibble: 57 x 7\n## # Groups:   STATIONS_ID, year [9]\n##    STATIONS_ID  year month mean_T mean_RH  sd_T sd_RH\n##          <dbl> <dbl> <dbl>  <dbl>   <dbl> <dbl> <dbl>\n##  1        1420  2018    11   4.00    79.7  1.82  9.96\n##  2        1420  2018    12   4.73    83.7  4.20 11.7 \n##  3        1420  2019     1   2.12    79.3  3.76 10.0 \n##  4        1420  2019     2   4.48    74.1  4.69 17.7 \n##  5        1420  2019     3   8.28    68.5  4.08 16.1 \n##  6        1420  2019     4  11.7     61.0  5.52 21.8 \n##  7        1420  2019     5  12.7     67.5  4.64 20.1 \n##  8        1420  2019     6  21.4     60.6  6.05 21.2 \n##  9        1420  2019     7  21.6     55.6  5.90 21.8 \n## 10        1420  2019     8  20.7     65.6  4.94 20.8 \n## # ‚Ä¶ with 47 more rows\nstr(monthly_means)## grouped_df [57 √ó 7] (S3: grouped_df/tbl_df/tbl/data.frame)\n##  $ STATIONS_ID: num [1:57] 1420 1420 1420 1420 1420 1420 1420 1420 1420 1420 ...\n##  $ year       : num [1:57] 2018 2018 2019 2019 2019 ...\n##  $ month      : num [1:57] 11 12 1 2 3 4 5 6 7 8 ...\n##  $ mean_T     : num [1:57] 4 4.73 2.12 4.48 8.28 ...\n##  $ mean_RH    : num [1:57] 79.7 83.7 79.3 74.1 68.5 ...\n##  $ sd_T       : num [1:57] 1.82 4.2 3.76 4.69 4.08 ...\n##  $ sd_RH      : num [1:57] 9.96 11.68 10.04 17.73 16.1 ...\n##  - attr(*, \"groups\")= tibble [9 √ó 3] (S3: tbl_df/tbl/data.frame)\n##   ..$ STATIONS_ID: num [1:9] 1420 1420 1420 2261 2261 ...\n##   ..$ year       : num [1:9] 2018 2019 2020 2018 2019 ...\n##   ..$ .rows      : list<int> [1:9] \n##   .. ..$ : int [1:2] 1 2\n##   .. ..$ : int [1:12] 3 4 5 6 7 8 9 10 11 12 ...\n##   .. ..$ : int [1:5] 15 16 17 18 19\n##   .. ..$ : int [1:2] 20 21\n##   .. ..$ : int [1:12] 22 23 24 25 26 27 28 29 30 31 ...\n##   .. ..$ : int [1:5] 34 35 36 37 38\n##   .. ..$ : int [1:2] 39 40\n##   .. ..$ : int [1:12] 41 42 43 44 45 46 47 48 49 50 ...\n##   .. ..$ : int [1:5] 53 54 55 56 57\n##   .. ..@ ptype: int(0) \n##   ..- attr(*, \".drop\")= logi TRUE\nmonthly_means <- ungroup(monthly_means)\nmonthly_means <- monthly_means %>%\n  mutate(year_month = parse_date_time(paste0(year, month), orders = 'ym', tz = 'CET'))\n\nmonthly_means## # A tibble: 57 x 8\n##    STATIONS_ID  year month mean_T mean_RH  sd_T sd_RH year_month         \n##          <dbl> <dbl> <dbl>  <dbl>   <dbl> <dbl> <dbl> <dttm>             \n##  1        1420  2018    11   4.00    79.7  1.82  9.96 2018-11-01 00:00:00\n##  2        1420  2018    12   4.73    83.7  4.20 11.7  2018-12-01 00:00:00\n##  3        1420  2019     1   2.12    79.3  3.76 10.0  2019-01-01 00:00:00\n##  4        1420  2019     2   4.48    74.1  4.69 17.7  2019-02-01 00:00:00\n##  5        1420  2019     3   8.28    68.5  4.08 16.1  2019-03-01 00:00:00\n##  6        1420  2019     4  11.7     61.0  5.52 21.8  2019-04-01 00:00:00\n##  7        1420  2019     5  12.7     67.5  4.64 20.1  2019-05-01 00:00:00\n##  8        1420  2019     6  21.4     60.6  6.05 21.2  2019-06-01 00:00:00\n##  9        1420  2019     7  21.6     55.6  5.90 21.8  2019-07-01 00:00:00\n## 10        1420  2019     8  20.7     65.6  4.94 20.8  2019-08-01 00:00:00\n## # ‚Ä¶ with 47 more rows\nggplot(data = monthly_means, aes(x = year_month, y = mean_T, col = factor(STATIONS_ID))) + \n  geom_line() + \n  labs(x = 'Zeit', y = 'Temperatur (¬∞C)', color = 'Messstation')\nggplot(monthly_means, aes(x = year_month, y = mean_T, ymin = mean_T - sd_T, ymax = mean_T + sd_T)) +\n  geom_errorbar() +\n  geom_point() +\n  facet_wrap(~STATIONS_ID, nrow = 3) + \n  labs(x = 'Zeti', y = 'Temperatur (¬∞C)')\nggplot(monthly_means, aes(x = year_month, y = mean_T, ymin = mean_T - sd_T, ymax = mean_T + sd_T)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line() +\n  facet_wrap(~STATIONS_ID, nrow = 3) + \n  labs(x = 'Zeit', y = 'Temperatur (¬∞C)')\nstation_ids##        2261        1420        2667 \n##       \"Hof\" \"Frankfurt\"     \"Koeln\"\nggplot(monthly_means, aes(x = year_month, y = mean_T, ymin = mean_T - sd_T, ymax = mean_T + sd_T)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line() +\n  facet_wrap(~STATIONS_ID, nrow = 3, labeller = labeller(STATIONS_ID = station_ids)) + \n  labs(x = 'Zeit', y = 'Temperatur (¬∞C)')"},{"path":"tidyverse.html","id":"lesestoff-4","chapter":"Kapitel 6 Der explorative Workflow mit tidyverse","heading":"6.5 Lesestoff","text":"Kapitel 3 Ismay Kim (2021)","code":""},{"path":"tidyverse.html","id":"weiterf√ºhrende-literatur-und-videos","chapter":"Kapitel 6 Der explorative Workflow mit tidyverse","heading":"6.6 Weiterf√ºhrende Literatur und Videos","text":"R4DS Wickham Grolemund (2021): Kapitel 5 ‚ÄúData transformation‚ÄùR4DS Wickham Grolemund (2021): Kapitel 5 ‚ÄúData transformation‚ÄùEine live Analyse des Hauptautors von tidyverse, Hadley Wickham. Empfehlenswert, auch wenn er viel zu schnell tippt üòÑ.Eine live Analyse des Hauptautors von tidyverse, Hadley Wickham. Empfehlenswert, auch wenn er viel zu schnell tippt üòÑ.","code":""},{"path":"stichproben.html","id":"stichproben","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"Kapitel 7 Stichproben und Variabilit√§t","text":"\nBegriffe Stichprobenverteilung und Standardfehler erkl√§ren\n\nZufall bei wiederholter Stichprobenerhebung erkennen\n\nStichprobenverteilung darstellen\n\nEinfluss der Stichprobengr√∂√üe auf Stichprobenverteilung benennen\nMit diesem Kapitel steigen wir die schlie√üende Statistik ein. Wir beginnen damit, wie man der Statistik zu Daten kommt (Stichprobenerhebung) und welche Rolle der Zufall dabei spielt. Dabei konzentrieren wir uns auf die Erhebung von einfachen zuf√§lligen Stichproben (Zufallsstichproben), nicht auf das Designen von komplizierten Erhebungen. Das ist eine Kunst f√ºr sich und geht √ºber die Ziele dieses Kurses hinaus.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)"},{"path":"stichproben.html","id":"stichproben-1","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.1 Stichproben","text":"Wir nutzen wieder die selbst erstellten Daten aus der Aufgabe 13.1. Die 12000 Studierenden sind unsere Grundgesamtheit. Da wir die Daten selbst erstellt haben, wissen wir alles √ºber sie. Das ist ein gro√üer Vorteil von Computerexperimenten üòÑ. Damit alle dieselben Daten erstellen, ist die Zeile set.seed(123) sehr wichtig. Sie sorgt daf√ºr, dass der Generator f√ºr Zufallszahlen einen zuf√§lligen, aber reproduzierbaren Zustand versetzt wird. Die Zahl den Klammern ist nicht wichtig. Wichtig ist, dass alle dieselbe benutzen.F√ºr bessere Nachvollziehbarkeit, nummerieren wir unsere Studierenden diesmal durch.Wir wollen nun 50 Studierende befragen. Durch die zuf√§llige Auswahl Befragten erzeugen wir eine zuf√§llige Stichprobe. √úbersetzt unser Computerexperiment bedeutet es, dass wir zuf√§llig 50 Zeilen aus dem Datensatz grundgesamtheit ziehen und zwar , dass sich diese Zeilen nicht wiederholen (d.h. niemand mehrfach befragt wird). Dazu nutzen wir die Funktion rep_sample_n(), die wiederholt (rep) n Zeilen zieht (sample), und zwar mit der Einstellung replace = FALSE, also ohne Zur√ºcklegen. Wir befragen nur einmal, daher reps = 1.Damit alle wieder dieselben Daten bekommen, setzen wir vorher den seed (Zustand des Zufallszahlengenerators). Die Variable befragung_size gibt die Stichprobengr√∂√üe .Die Variable replicate zeigt immer 1. Das bedeutet, dass wir die Befragung einmal wiederholt (repliziert) haben und alle Datenpunkte zu dieser Wiederholung geh√∂ren.Wir wollen nun wissen, wie viele Studierende unter den Befragten der Stadt oder auf dem Land wohnen.Das Ganze m√∂chten wir als Anteile ausdr√ºcken. Die Funktion n() kann innerhalb der Funktion summarise() zum Ausz√§hlen genutzt werden. Wir teilen durch die Stichprobengr√∂√üe.Es wohnen also 42% auf dem Land und 58% der Stadt.passiert, wenn wir die Befragung mehrfach wiederholen, sagen wir 33 Mal? der Realit√§t ist dieses Szenario sehr unwahrscheinlich, aber einem Computerexperiment einfach zu implementieren. Es hilft uns ein Gef√ºhl f√ºr die Variabilit√§t, die durch das zuf√§llige Ausw√§hlen der Studierenden bei der Befragung entsteht, zu entwickeln.Wir setzten erneut den seed, damit alle dieselben Ergebnisse bekommen.Jetzt wird uns angezeigt, dass es dem Datensatz befragung_reps 33 replicates (Wiederholgungen) gibt. Diese sind einfach nacheinander befragung_reps angeordnet (bl√§ttern Sie durch den Datensatz). Dementsprechend hat der Datensatz 50 \\(\\times\\) 33 = 1650 Zeilen.Wie sieht es jetzt mit den Anteilen von Stadt- und Landbewohnern aus? Wir m√ºssen nun zus√§tzlich zum wohnort auch noch nach replicate gruppieren.Erwartungsgem√§√ü bringt jede Wiederholung der Befragung, die wir ja als zuf√§lliges Herausgreifen der Studierenden ohne Mehrfachbefragung programmiert haben, etwas andere Ergebnisse. Sehen Sie sich die student_id den replicates , es werde unterschiedliche Studierende befragt! einem Histogramm sieht das ganze aus:Die h√§ufigsten Anteile sind um die 40% f√ºr Land und um die 60% f√ºr Stadt. Wir k√∂nnen es sogar etwas genauer ablesen, da wir binwidth = 0.05 gew√§hnt haben, also Schritte von 5%. Es sind 35‚Äì40% f√ºr Land und 55‚Äì60% f√ºr Stadt.","code":"\nset.seed(123)\n\nstudent_id <- 1:12000\n  \nanreise <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n             runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngeschlecht <- sample(c('m', 'w'), size = 12000, replace = TRUE)\n\nwohnort <- sapply(anreise, function(x) {\n  if(x < 30) 'stadt'\n  else 'land'\n})\n\nverkehrsmittel <- sapply(anreise, function(x) {\n  if(x <= 10) 'zu_fuss'\n  else if(x > 10 & x <= 15) sample(c('zu_fuss', 'fahrrad'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'fahrrad', 'auto'), size = 1)\n  else sample(c('bus', 'auto'), size = 1)\n})\n\nzeit_bib <- 5 * 60 - 0.7 * anreise + rnorm(length(anreise), 0, 20)\n\ngrundgesamtheit <- tibble(student_id, geschlecht, wohnort, verkehrsmittel, anreise, zeit_bib)\n\ngrundgesamtheit## # A tibble: 12,000 x 6\n##    student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##         <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1          1 w          stadt   bus              15.1      294.\n##  2          2 w          land    fahrrad          32.6      254.\n##  3          3 w          stadt   fahrrad          19.3      231.\n##  4          4 m          land    auto             35.9      245.\n##  5          5 m          land    bus              37.9      234.\n##  6          6 w          stadt   zu_fuss           6.59     303.\n##  7          7 w          stadt   bus              23.5      284.\n##  8          8 m          land    auto             36.2      274.\n##  9          9 m          stadt   fahrrad          24.3      299.\n## 10         10 w          stadt   bus              21.0      282.\n## # ‚Ä¶ with 11,990 more rows\nset.seed(345)\n\nbefragung_size <- 50\n\nbefragung <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = 1)\n\nbefragung## # A tibble: 50 x 7\n## # Groups:   replicate [1]\n##    replicate student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##        <int>      <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1         1       1623 m          stadt   zu_fuss           7.06     299.\n##  2         1       9171 m          stadt   fahrrad          11.3      278.\n##  3         1      10207 w          land    bus             107.       199.\n##  4         1       3506 w          stadt   bus              25.0      326.\n##  5         1       8892 w          stadt   bus              28.1      259.\n##  6         1       5460 m          stadt   bus              23.6      299.\n##  7         1       6120 w          stadt   bus              20.0      268.\n##  8         1        865 w          stadt   fahrrad          26.6      290.\n##  9         1      11586 m          land    bus             114.       207.\n## 10         1       8153 w          stadt   zu_fuss           8.06     297.\n## # ‚Ä¶ with 40 more rows\nbefragung %>% \n  group_by(wohnort) %>% \n  count()## # A tibble: 2 x 2\n## # Groups:   wohnort [2]\n##   wohnort     n\n##   <chr>   <int>\n## 1 land       21\n## 2 stadt      29\nbefragung %>% \n  group_by(wohnort) %>% \n  summarise(prop = n()/befragung_size)## # A tibble: 2 x 2\n##   wohnort  prop\n##   <chr>   <dbl>\n## 1 land     0.42\n## 2 stadt    0.58\nset.seed(234)\n\nbefragung_reps <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = 33)\n\nbefragung_reps## # A tibble: 1,650 x 7\n## # Groups:   replicate [33]\n##    replicate student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##        <int>      <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1         1       2079 m          land    auto              38.8     262.\n##  2         1       1314 m          stadt   fahrrad           13.3     301.\n##  3         1       1710 m          stadt   auto              26.5     272.\n##  4         1       4386 w          stadt   bus               23.9     269.\n##  5         1       9490 m          land    auto              34.2     262.\n##  6         1      11757 w          land    bus              102.      227.\n##  7         1      11649 w          land    bus              111.      202.\n##  8         1       2244 m          land    bus               38.9     256.\n##  9         1       3652 w          stadt   fahrrad           10.3     254.\n## 10         1       3127 m          stadt   fahrrad           29.6     271.\n## # ‚Ä¶ with 1,640 more rows\nwohnort_props <- befragung_reps %>% \n  group_by(replicate, wohnort) %>% \n  summarise(prop = n()/befragung_size)\n\nwohnort_props## # A tibble: 66 x 3\n## # Groups:   replicate [33]\n##    replicate wohnort  prop\n##        <int> <chr>   <dbl>\n##  1         1 land     0.42\n##  2         1 stadt    0.58\n##  3         2 land     0.36\n##  4         2 stadt    0.64\n##  5         3 land     0.4 \n##  6         3 stadt    0.6 \n##  7         4 land     0.38\n##  8         4 stadt    0.62\n##  9         5 land     0.4 \n## 10         5 stadt    0.6 \n## # ‚Ä¶ with 56 more rows\nggplot(data = wohnort_props, aes(x = prop)) + \n  geom_histogram(binwidth = 0.05, boundary = 0.4, col = 'white') +\n  facet_wrap(~ wohnort) +\n  labs(x = 'Anteile beim Wohnort', title = 'Verteilung der Wohnorte', y = 'H√§ufigkeit')"},{"path":"stichproben.html","id":"anzahl-der-wiederholungen-und-variabilit√§t","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.2 Anzahl der Wiederholungen und Variabilit√§t","text":"passiert, wenn wir unsere Umfrage nun 1000 Mal wiederholen? Wir k√∂nnen den ganzen Code wiederverwenden und m√ºssen nur die reps entsprechend ver√§ndern. Wir erstellen daf√ºr eine extra Variable befragung_num. Beim Histogramm sollten wir die binwidth etwas heruntersetzten, da wir jetzt sehr viel mehr Daten haben und diese detaillierter anzeigen lassen k√∂nnen. Zus√§tzlich wird die x-Achse ‚Äúfrei‚Äù gegeben, .e.¬†die Skalierung wird jetzt auf jeder Achse separat bestimmt scales = 'free_x' facet_wrap(), um die Verteilungen besser zu sehen.Die h√§ufigsten Anteile beim Land liegen bei 40‚Äì42% und bei der Stadt bei 58‚Äì60%.Die Histogramme geben nun sehr gut die Verteilung der Anteile der Stadt- und Landbewohner wieder. Solche Verteilungen nennt man Stichprobenverteilungen. Sie zeigen die Verteilung einer statistischen Kenngr√∂√üe (Statistik), unserem Fall Anteil, die aus zuf√§lligen Stichproben ausgerechnet wurde. Die Stichprobenverteilung beantwortet die Frage: Wenn ich eine zuf√§llige Menge (Stichprobengr√∂√üe) Daten (Zufallsstichprobe) aus der Grundgesamtheit herausgreife und eine Kenngr√∂√üe (z.B. Anteil) berechne, welchen Wert werde ich im Mittel erhalten und wie stark wird der Wert schwanken.","code":"\nset.seed(345)\n\nbefragung_num <- 1000\n\nbefragung_reps <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = befragung_num)\n\nwohnort_props <- befragung_reps %>% \n  group_by(replicate, wohnort) %>% \n  summarise(prop = n()/befragung_size)\n\n\nggplot(data = wohnort_props, aes(x = prop)) + \n  geom_histogram(binwidth = 0.02, boundary = 0.4, col = 'white') +\n  facet_wrap(~ wohnort, scales = 'free_x') +\n  labs(x = 'Anteile beim Wohnort', title = 'Verteilung der Wohnorte', y = 'H√§ufigkeit')"},{"path":"stichproben.html","id":"stichprobengr√∂√üe","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.3 Stichprobengr√∂√üe","text":"passiert, wenn wir die Gr√∂√üe der Stichproben variieren? Das w√ºrde Befragungen mit unterschiedlicher Anzahl von Teilnehmern entsprechen. Wir vergleichen 25, 50 und 100 Befragte und wiederholen jeweils 1000 Mal, um erneut Stichprobenverteilungen plotten zu k√∂nnen. Das ist eine repetitive Aufgabe und ich werde daf√ºr eine Funktion definieren. Wie man das macht, wird einer sp√§teren Stunde erkl√§rt.Nun wenden wir diese selbst definierte Funktion und f√ºhren die Befragungen durch.Wir stellen die drei Stichprobenverteilungen dar. Die binwidth ist jeweils eine andere.Wie kann man diese Stichprobenverteilungen charakterisieren? Nun, es sind erster Linie Daten und wie bei jedem Datensatz kann man auch hier einen Mittelwert und eine Stadanrdabweichung angeben. Die Standardabweichung der Stichprobenverteilung hei√üt Standardfehler und fasst den Einfluss der Variabilit√§t (zuf√§lliges Herausgreifen der Studierenden) zusammen.Sie sehen, dass mit steigender Stichprobengr√∂√üe der Standardfehler sinkt. Das ist intuitiv verst√§ndlich, denn je mehr Studierende wir (pro Wiederholung!) befragen, desto repr√§sentativer ist die Stichprobe.Da dieses Kapitel wichtig ist, gibt es ausnahmsweise vorformulierte Take-Home-Messages üòÑ:\nEine zuf√§llige Stichprobe ist (meistens\\(^*\\)) der K√∂nigsweg, um repr√§sentative Informationen √ºber die Grundgesamtheit zu bekommen.\n\nDie Verteilung einer statistischen Kenngr√∂√üe, die aus Zufallsstichproben ausgerechnet wurde, hei√üt Stichprobenverteilung. Um diese Verteilung zu bekommen, muss man wiederholt Stichproben erheben. Je mehr Stichproben man erhebt, desto genauer kann man die Stichprobenverteilung beschreiben.\n\nDie Standardabweichung der Kenngr√∂√üe, die durch die Stichprobenverteilung dargestellt wird, hei√üt Standardfehler.\n\nDer Zufall macht sich durch eine Streuung (erfasst durch den Standardfehler) der Stichprobenverteilung bemerkbar. Je gr√∂√üer die einzelnen Stichproben, desto kleiner der Standardfehler.\n\\(^*\\): Manchmal ist die interessierte Gr√∂√üe unterschiedlichen Untergruppen der Grundgesamtheit unterschiedlich verteilt. Z.B. k√∂nnten bestimmte Haltungen der Bev√∂lkerung gegen√ºber irgendwelchen Sachverhalten von Alter oder Bildungsstand oder Wohnort (Stadt vs.¬†Land) abh√§ngen. Dann sollte man sich √ºberlegen, ob man statt einer zuf√§lligen Stichprobe lieber eine geschichtete Zufallsstichprobe zieht, d.h. innerhalb dieser Kategorien zuf√§llig beprobt.","code":"\ncalculate_props <- function(grund_data = grundgesamtheit, befragung_size, befragung_reps = 1000) {\n  \n  befragung <- rep_sample_n(grund_data, size = befragung_size, replace = FALSE, reps = befragung_reps)\n\nwohnort_props <- befragung %>% \n  group_by(replicate, wohnort) %>% \n  summarise(prop = n()/befragung_size)\n\nwohnort_props\n}\nset.seed(123)\n\n# Stichprobengr√∂√üe 25\nwohnort_props_25 <- calculate_props(grund_data = grundgesamtheit, befragung_size = 25, befragung_reps = 1000)\n  \n# Stichprobengr√∂√üe 50\nwohnort_props_50 <- calculate_props(grund_data = grundgesamtheit, befragung_size = 50, befragung_reps = 1000)\n\n# Stichprobengr√∂√üe 100\nwohnort_props_100 <- calculate_props(grund_data = grundgesamtheit, befragung_size = 100, befragung_reps = 1000)\nggplot(data = wohnort_props_25, aes(x = prop)) + \n  geom_histogram(binwidth = 0.05, boundary = 0.4, col = 'white') +\n  facet_wrap(~ wohnort, scales = 'free_x') +\n  labs(x = 'Anteile beim Wohnort', title = 'Verteilung der Wohnorte, Stichprobengr√∂√üe = 25', y = 'H√§ufigkeit')\nggplot(data = wohnort_props_50, aes(x = prop)) + \n  geom_histogram(binwidth = 0.02, boundary = 0.4, col = 'white') +\n  facet_wrap(~ wohnort, scales = 'free_x') +\n  labs(x = 'Anteile beim Wohnort', title = 'Verteilung der Wohnorte, Stichprobengr√∂√üe = 50', y = 'H√§ufigkeit')\nggplot(data = wohnort_props_100, aes(x = prop)) + \n  geom_histogram(binwidth = 0.02, boundary = 0.4, col = 'white') +\n  facet_wrap(~ wohnort, scales = 'free_x') +\n  labs(x = 'Anteile beim Wohnort', title = 'Verteilung der Wohnorte, Stichprobengr√∂√üe = 100', y = 'H√§ufigkeit')\nwohnort_props_25 %>% \n  group_by(wohnort) %>% \n  summarise(prop_sd = sd(prop))## # A tibble: 2 x 2\n##   wohnort prop_sd\n##   <chr>     <dbl>\n## 1 land      0.102\n## 2 stadt     0.102\nwohnort_props_50 %>% \n  group_by(wohnort) %>% \n  summarise(prop_sd = sd(prop))## # A tibble: 2 x 2\n##   wohnort prop_sd\n##   <chr>     <dbl>\n## 1 land     0.0679\n## 2 stadt    0.0679\nwohnort_props_100 %>% \n  group_by(wohnort) %>% \n  summarise(prop_sd = sd(prop))## # A tibble: 2 x 2\n##   wohnort prop_sd\n##   <chr>     <dbl>\n## 1 land     0.0458\n## 2 stadt    0.0458"},{"path":"stichproben.html","id":"lesestoff-5","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.4 Lesestoff","text":"Kapitel 7 Ismay Kim (2021)","code":""},{"path":"stichproben.html","id":"aufgaben-4","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.5 Aufgaben","text":"","code":""},{"path":"stichproben.html","id":"wahrer-wert-in-der-grundgesamtheit","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.5.1 Wahrer Wert in der Grundgesamtheit","text":"Berechnen Sie die Anteile von Studierenden der Grundgesamtheit, die der Stadt bzw. auf dem Land leben. Wie gut waren die Sch√§tzungen im Vergleich zum wahren Wert der Grundgesamtheit?","code":""},{"path":"stichproben.html","id":"lohnt-eine-station-zum-ausleihen-von-farr√§hdern","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.5.2 Lohnt eine Station zum Ausleihen von Farr√§hdern?","text":"unserem fiktiven Beispiel der Uni Werdeschlau geht es eigentlich darum, ob sich eine Station zum Ausleihen von Fahrr√§dern lohnen w√ºrde. Daher ist die Frage interessant, wie viele Studierende mit dem Fahrrad zur Uni kommen. Wiederholen Sie die obige Analyse und ermitteln Sie statt der Anteile von Stadt- und Landbewohnern nun die Anteile der unterschiedlichen Verkehrsmittel. Die selbst definierte Funktion m√ºssen Sie durch die folgende ersetzten.der L√∂sung zu dieser Aufgabe erhalten Sie auch weitere Tipps zur Darstellung von Histogrammen und Dichtefunktionen üòé.","code":"\ncalculate_props_verkehr <- function(grund_data = grundgesamtheit, befragung_size, befragung_reps = 1000) {\n  \n  befragung <- rep_sample_n(grund_data, size = befragung_size, replace = FALSE, reps = befragung_reps)\n\nverkehrsmittel_props <- befragung %>% \n  group_by(replicate, verkehrsmittel) %>% \n  summarise(prop = n()/befragung_size)\n\nverkehrsmittel_props\n}"},{"path":"stichproben.html","id":"ihre-arbeit-einreichen-3","chapter":"Kapitel 7 Stichproben und Variabilit√§t","heading":"7.6 Ihre Arbeit einreichen","text":"Speichern Sie Ihr Notebook ab und laden Sie nur die .Rmd Datei vom Server.Laden Sie Ihre .Rmd Datei ILIAS hoch. Beachten Sie die Deadline!Sie erhalten die Musterl√∂sung nach dem Hochladen.","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"bootstrapping-und-konfidenzintervalle","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"Kapitel 8 Bootstrapping und Konfidenzintervalle","text":"\nFunktionsweise von Bootstrap erkl√§ren\n\nBootstrap-Konfidenzintervalle f√ºr Mittelwert berechnen\nIm Kapitel 7 haben Sie gesehen, dass Statistiken aus zuf√§llig gezogenen Stichproben, dem Zufall unterliegen. Sie sind Zufallsvariablen. Diesen Zufall haben wir mit Hilfe der Stichprobenverteilung dieser Statistiken quantifiziert, dem wir den Standardfehler, die Standardabweichung aus der Stichprobenverteilung, berechnet haben.Die Statistik, die wir im Kapitel 7 berechnet haben, war der Anteil von Studierenden, die entweder der Stadt oder auf dem Land wohnen. Mit jeder Stichprobe, aus der wir diesen Anteil berechnet haben, haben wir eigentlich gesch√§tzt, wie gro√ü der wahre Anteil der Stadt- und Landbewohner der Grundgesamtheit (allen 12000 Studierenden von Werdeschlau) ist. Der aus der Stichprobe berechnete Anteil ist also ein Sch√§tzer f√ºr den wahren Anteil der Grundgesamtheit. Dieser Sch√§tzer ist eine Zufallsvariable (s.o.) und wie jede andere Zufallsvariable ist er durch seine Verteilung, n√§mlich die Stichprobenverteilung charakterisiert.Wir k√∂nnen jetzt also eine Menge statistischer Begriffe mit Hilfe unseres Beispiels mit Leben f√ºllen:\nGrundgesamtheit: alle Studierenden der Universit√§t Werdeschlau\n\nzuf√§llige Stichprobe: eine zuf√§llig ausgesuchte Gruppe von Studierenden\n\nParameter der Grundgesamtheit: z.B. der wahre Anteil von Studierenden, die der Stadt oder auf dem Land leben\n\nSch√§tzer f√ºr diesen Parameter der Grundgesamtheit: Anteil der Studierenden, die der Stadt oder auf dem Land leben, berechnet aus der zuf√§lligen Stichprobe. Da die Stichprobe zuf√§llig ist, kann man davon ausgehen, dass sie repr√§sentativ f√ºr die Grundgesamtheit ist und der Sch√§tzer unverzerrt (unbiased, d.h. ohne einen systematischen Fehler).\n\nInferenz: schlie√üen auf die Grundgesamtheit darf man, wenn die Stichprobe zuf√§llig erhoben wurde und repr√§sentative f√ºr die Fragestellung ist.\nDie Begriffe Statistik, Sch√§tzer, Sch√§tzfunktion und Stichprobenfunktion werden als Synonyme verwendet. Die Statistik ist ja auch eine Funktion, da sie mit einer Formel eine Zahl aus Daten (Stichprobe) berechnet. Sie fasst die Stichprobe also zusammen.Im echten Leben werden Sie kaum wiederholt befragen (Stichproben ziehen) k√∂nnen. Das ist vollkommen unrealistisch und nur f√ºr Computerexperimente ein tolles Werkzeug. diesem Kapitel wird es darum gehen, wie man nun im richtigen Leben mit einer Stichprobe einen Parameter der Grundgesamtheit sch√§tzen kann und dabei seine Variabilit√§t quantifiziert.","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"bootstrapping-die-m√ºnchhausenmethode","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.1 Bootstrapping, die M√ºnchhausenmethode","text":"Wenn wir nur eine Stichprobe haben, werden wir den Parameter der Grundgesamtheit daraus sch√§tzen. Der Sch√§tzer kann der Mittelwert oder eben auch der Anteil sein, wie bei unseren vorherigen Beispielen mit Mittelwert der Anreisezeit oder dem Anteil der Studierenden aus der Stadt bzw. vom Land.Da jede Stichprobe dem Zufall unterliegt und der Sch√§tzer somit eine Zufallsvariable darstellt, w√ºrden wir gerne wissen, wie gut wir sch√§tzen.Gibt es einen plausiblen Bereich f√ºr den Mittelwert der Anreisezeit? Plausibel meint, dass wenn wir sehr oft verschiedene zuf√§llige Stichprobe ziehen, dieser Bereich sagen wir mal 95% der F√§lle den wahren Mittelwert einschlie√üt. Solche Plausibilit√§tsbereiche nennt man Konfidenzintervalle.Es gibt mehrere Methoden, solche Konfidenzintervalle zu berechnen. Wenn man die Verteilung des Sch√§tzers kennt, wie z.B. beim Mittelwert (Normalverteilung), dann kann man daraus den Standardfehler berechnen (nennen wir diesen \\(SE\\)). Ein 95%-Konfidenzintervall w√§re dann \\(\\hat{\\mu} \\pm 1.96 \\cdot SE\\), wobei \\(\\hat{\\mu}\\) der gesch√§tzter Mittelwert ist. Das H√ºtchen steht f√ºr gesch√§tzt. Diese Formel haben Sie bestimmt schon der Grundvorlesung Statistik gesehen.Es gibt aber Sch√§tzer, f√ºr die keine theoretische Verteilung bekannt ist. Da muss man eine andere Methode anwenden. Eine bekannte Methode hei√üt Bootstrapping (Bootstrap-Verfahren), manchmal auch M√ºnchhausenmethode. Sie klingt auf den ersten Blick wie ein Selbstbetrug, als ob man sich selbst den Haaren aus dem Sumpf zieht (Abblildung 8.1), hat aber sehr gut fundierte mathematische Wurzeln. Bootstrap wurde von Efron (1979) Ende der 70er vorgestellt und hat sich seit dem als eine der wichtigsten Resampling-Strategien etabliert.\nAbbildung 8.1: M√ºnchhausen zieht sich aus dem Sumpf (Theodor Hosemann (1807-1875), Public domain, via Wikimedia Commons)\nDas Prinzip beim Bootstrap ist, dass die Stichprobe die Rolle der Grundgesamtheit √ºbernimmt. Abbildung 8.2 zeigt das Vorgehen aus dem Kapitel 7. Wir ziehen mehrere echte Stichproben aus einer Grundgesamtheit, erhalten eine Stichprobenverteilung und k√∂nnen den Parameter der Grundgesamtheit sch√§tzen.\nAbbildung 8.2: Berechnen einer Stichprobenverteilung durch wiederholtes Stichproben ziehen. Abbildung aus (Hesterberg 2015), dort Figure 4. Die Publikation ist open-access und darf f√ºr nicht-kommerzielle Zwecke verwendet werden.\nAbbildung 8.3 haben wir nur eine Stichprobe. Wir gehen davon aus, dass diese Stipchprobe eine Miniatur der Grundgesamtheit ist, also zuf√§llig gezogen wurde und repr√§sentative ist. Mit dieser Grundidee im Kopf, ersetzten wir die Grundgesamtheit durch dies Stichprobe und verfahren (fast) genauso, um die Stichprobenverteilung zu ermitteln. Der einzige Unterschied ist, dass wir aus dieser einen Stichprobe mit Zur√ºcklegen neue Stichproben (Bootstrap-Stichproben) ziehen.\nAbbildung 8.3: Berechnen einer Stichprobenverteilung durch wiederholtes Ziehen aus einer Stichprobe mit Zur√ºcklegen. Abbildung aus (Hesterberg 2015), dort Figure 5. Die Publikation ist open-access und darf f√ºr nicht-kommerzielle Zwecke verwendet werden.\nDie Konfidenzintervalle berechnet man aus der Stichprobenverteilung der Bootstrap-Stichproben, indem man z.B. das 2.5% und das 97.5% Quantil berechnet. Zwischen diesen beiden Quantilen sind 95% der Werte enthalten. Dieses Intervall nennt man das 95%-Konfidenzintervall.","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"konfidenzintervall-f√ºr-den-mittelwert-der-anreisezeit","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.2 Konfidenzintervall f√ºr den Mittelwert der Anreisezeit","text":"Wir sehen uns das Ganze anhand des Beispiels der Studierenden aus Werdeschlau. Zun√§chst wieder der Code f√ºr die Grundgesamtheit.Nun befragen wir 200 Studierende.Wir berechnen den wahren Mittelwert der Anreisezeit und den Mittelwert aus der Befragung.Wir ziehen nun unsere Bootstrap-Stichproben aus der einen Stichprobe, n√§mlich der befragung. Achten Sie darauf, wie √§hnlich der Code zum Ziehen von echten Stichproben ist. Wir √§ndern nur replace = TRUE.Studierende kommen jetzt mehrfach vor, da wir ja mit Zur√ºcklegen gezogen haben. Wir sehen uns das den erste 50 Bootstrap-Stichproben .Nun berechnen wir die Mittelwerte aus den Bootstrap-Stichproben.Der Standardfehler des Bootstraps und das 95%-Konfidenzintervall basierend auf Quantilen berechnen sich wie folgt:Wir stellen die Stichprobenverteilung mit dem Standardfehler des Bootstraps und dem 95%-Konfidenzintervall dar. Das ist eine umfangreiche Grafik und wir gehen der √úbung Schritt f√ºr Schritt vor. Die Funktion scale_color_manual erlaubt es uns, die Legende anzupassen.Der Standardfehler des Bootstraps stimmt sehr gut mit dem Fehler \\(s/\\sqrt(n)\\) f√ºr den Sch√§tzer des Mittelwerts √ºberein. Da wir normalerweise die wahre Standardabweichung der Grundgesamtheit nicht kennen, wird der Formel \\(s/\\sqrt(n)\\) die Standardabweichung der Stichprobe als Sch√§tzung verwendet.Standardfehler des Bootstraps.Und der wahre Standardfehler, wenn man die Standardabweichung der Grundgesamtheit kennt.","code":"\nlibrary(tidyverse)\nlibrary(infer)\nset.seed(123)\n\nstudent_id <- 1:12000\n  \nanreise <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n             runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngeschlecht <- sample(c('m', 'w'), size = 12000, replace = TRUE)\n\nwohnort <- sapply(anreise, function(x) {\n  if(x < 30) 'stadt'\n  else 'land'\n})\n\nverkehrsmittel <- sapply(anreise, function(x) {\n  if(x <= 10) 'zu_fuss'\n  else if(x > 10 & x <= 15) sample(c('zu_fuss', 'fahrrad'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'fahrrad', 'auto'), size = 1)\n  else sample(c('bus', 'auto'), size = 1)\n})\n\nzeit_bib <- 5 * 60 - 0.7 * anreise + rnorm(length(anreise), 0, 20)\n\ngrundgesamtheit <- tibble(student_id, geschlecht, wohnort, verkehrsmittel, anreise, zeit_bib)\n\ngrundgesamtheit## # A tibble: 12,000 x 6\n##    student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##         <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1          1 w          stadt   bus              15.1      294.\n##  2          2 w          land    fahrrad          32.6      254.\n##  3          3 w          stadt   fahrrad          19.3      231.\n##  4          4 m          land    auto             35.9      245.\n##  5          5 m          land    bus              37.9      234.\n##  6          6 w          stadt   zu_fuss           6.59     303.\n##  7          7 w          stadt   bus              23.5      284.\n##  8          8 m          land    auto             36.2      274.\n##  9          9 m          stadt   fahrrad          24.3      299.\n## 10         10 w          stadt   bus              21.0      282.\n## # ‚Ä¶ with 11,990 more rows\nset.seed(345)\n\nbefragung_size <- 200\n\nbefragung <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = 1)\n\nbefragung## # A tibble: 200 x 7\n## # Groups:   replicate [1]\n##    replicate student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##        <int>      <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1         1       1623 m          stadt   zu_fuss           7.06     299.\n##  2         1       9171 m          stadt   fahrrad          11.3      278.\n##  3         1      10207 w          land    bus             107.       199.\n##  4         1       3506 w          stadt   bus              25.0      326.\n##  5         1       8892 w          stadt   bus              28.1      259.\n##  6         1       5460 m          stadt   bus              23.6      299.\n##  7         1       6120 w          stadt   bus              20.0      268.\n##  8         1        865 w          stadt   fahrrad          26.6      290.\n##  9         1      11586 m          land    bus             114.       207.\n## 10         1       8153 w          stadt   zu_fuss           8.06     297.\n## # ‚Ä¶ with 190 more rows\nmean_grundgesamtheit <- grundgesamtheit %>% \n  summarise(Mittelwert = mean(anreise))\n\nmean_grundgesamtheit## # A tibble: 1 x 1\n##   Mittelwert\n##        <dbl>\n## 1       36.0\nmean_befragung <- befragung %>% \n  summarise(Mittelwert = mean(anreise))\n\nmean_befragung## # A tibble: 1 x 2\n##   replicate Mittelwert\n##       <int>      <dbl>\n## 1         1       34.1\nset.seed(345)\n\nbefragung_size <- 200\nnumber_reps <- 10000\n\nbefragung_reps_bootstrap <- rep_sample_n(befragung, size = befragung_size, replace = TRUE, reps = number_reps)\n\nbefragung_reps_bootstrap## # A tibble: 2,000,000 x 7\n## # Groups:   replicate [10,000]\n##    replicate student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##        <int>      <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1         1       7038 m          stadt   bus              29.4      243.\n##  2         1        493 m          stadt   bus              21.1      303.\n##  3         1       3442 m          land    auto             30.3      311.\n##  4         1       4920 w          stadt   bus              21.1      290.\n##  5         1       3178 w          stadt   zu_fuss           8.07     325.\n##  6         1       7694 w          land    auto             31.3      295.\n##  7         1       9479 m          land    bus              39.5      302.\n##  8         1       3367 m          land    auto             36.4      272.\n##  9         1       5985 w          stadt   fahrrad          15.9      303.\n## 10         1        843 m          stadt   fahrrad          26.4      300.\n## # ‚Ä¶ with 1,999,990 more rows\nbefragung_reps_bootstrap %>% \n  filter(replicate %in% (1:50)) %>% \n  group_by(replicate, student_id) %>% \n  tally() %>% \n  filter(n != 1) %>% \n  arrange(desc(n))## # A tibble: 2,657 x 3\n## # Groups:   replicate [50]\n##    replicate student_id     n\n##        <int>      <int> <int>\n##  1        43       4787     8\n##  2        20       5985     6\n##  3        34       7749     6\n##  4        38       5641     6\n##  5        41       8456     6\n##  6         3       7083     5\n##  7         3      10943     5\n##  8         5       8994     5\n##  9         6      10118     5\n## 10         8      11425     5\n## # ‚Ä¶ with 2,647 more rows\nres_means_bootstrap <- befragung_reps_bootstrap %>%\n  group_by(replicate) %>% \n  summarise(Mittelwert = mean(anreise))\n\nres_means_bootstrap## # A tibble: 10,000 x 2\n##    replicate Mittelwert\n##        <int>      <dbl>\n##  1         1       32.6\n##  2         2       31.9\n##  3         3       38.9\n##  4         4       33.5\n##  5         5       35.4\n##  6         6       37.2\n##  7         7       34.4\n##  8         8       33.4\n##  9         9       35.5\n## 10        10       31.0\n## # ‚Ä¶ with 9,990 more rows\nstat_bootstrap <- res_means_bootstrap %>% \n  summarize(mean = mean(Mittelwert), sd = sd(Mittelwert), ci_2.5 = quantile(Mittelwert, probs = 0.025), ci_97.5 = quantile(Mittelwert, probs = 0.975))\nggplot(res_means_bootstrap, aes(Mittelwert)) + \n  geom_histogram(bins = 50 , color=\"white\") +\n  labs(y = 'H√§ufigkeit', x = 'Mittelwerte der Anreise (min)') +\n  geom_vline(aes(xintercept = mean_grundgesamtheit$Mittelwert, col = 'grundgesamtheit'), linetype = \"dashed\", size = 2) + \n  geom_vline(aes(xintercept = stat_bootstrap$mean, col = 'boot')) + \n  geom_vline(aes(xintercept = mean_befragung$Mittelwert, col = 'stichprobe'), linetype = 'dashed', size = 2) +\n  geom_vline(aes(xintercept = stat_bootstrap$mean + stat_bootstrap$sd, col = 'sd')) + \n  geom_vline(xintercept = stat_bootstrap$mean - stat_bootstrap$sd, col = 'orange') + \n  geom_vline(aes(xintercept = stat_bootstrap$ci_2.5, col = 'ci')) +\n  geom_vline(xintercept = stat_bootstrap$ci_97.5, col = 'brown') + \n  scale_color_manual(name = \"Statistik\", values = c(grundgesamtheit = 'black', sd = 'orange', boot = 'red', ci = 'brown', stichprobe = 'gray90'), breaks = c('stichprobe', 'boot', 'sd', 'ci', 'grundgesamtheit'), label = c('Mittelwert Stichprobe', 'Mittelwert Bootstrap', 'Standardfehler Bootstrap', '95% Konfidenzintervall Bootstrap', 'Mittelwert Population'))\nbefragung %>% \n  summarize(sd_error = sd(anreise)/sqrt(length(anreise)))## # A tibble: 1 x 2\n##   replicate sd_error\n##       <int>    <dbl>\n## 1         1     2.04\nres_means_bootstrap %>% \n  summarize(sd_error = sd(Mittelwert))## # A tibble: 1 x 1\n##   sd_error\n##      <dbl>\n## 1     2.06\ngrundgesamtheit %>% \n  summarize(sd_error = sd(anreise)/sqrt(length(befragung$anreise)))## # A tibble: 1 x 1\n##   sd_error\n##      <dbl>\n## 1     2.09"},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"bootstrap-konfidenzintervalle-mit-infer","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.3 Bootstrap-Konfidenzintervalle mit infer","text":"Das Paket infer bietet eine sehr bequeme M√∂glichkeit, Konfidentintervalle mit Bootstrap zu berechnen und zu visualisieren. Das Vorgehen aus dem vorherigen Abschnitt √ºbertragen wir nun den Workflow mit infer.Brechnen der Bootstrap-Stichproben.Visualisieren der Stichprobenverteilung.Berechnen KonfidenzintervalleVisualisieren der Stichprobenverteilung mit den KonfidenzintervallenDie Konfidenzintervalle, die wir als Quantile aus der Bootstrap-Stichprobenverteilung berechnet haben, und die, die man mit der Formel \\(\\hat{\\mu} \\pm 1.96 \\cdot s/\\sqrt(n)\\) berechnen kann, sind sehr √§hnlich. Das werden Sie einer der Aufgaben (s.u.) nachrechnen. Das liegt daran, dass der Zentrale Grenzwertsatz garantiert, dass die Stichprobenverteilung des Sch√§tzers des Mittelwerts eine Normalverteilung ist. Es gibt aber Sch√§tzer, wie z.B. den des Medians, f√ºr den es keine theoretische Verteilung gibt. Daher gilt als Take-Home-Message, dass man Bootstrap zum Berechnen der Konfidenzintervalle gut einsetzten kann, egal ob es eine theoretische Verteilung des Sch√§tzers gibt. Die Quantilmethode zur Berechnung der Bootstrap-Konfidenzintervalle liefert gute Ergebnisse. Wir werden im sp√§teren Kapitel lernen, dass man Bootstrap auch f√ºr die Regressionsanalyse nutzen kann.","code":"\nset.seed(345)\n\nbootstrap_distribution <- befragung %>%\n  specify(response = anreise) %>% \n  generate(reps = 10000, type = 'bootstrap') %>% \n  calculate(stat = 'mean')\nvisualize(bootstrap_distribution)\npercentile_ci <- bootstrap_distribution %>% \n  get_confidence_interval(level = 0.95, type = \"percentile\")\npercentile_ci## # A tibble: 1 x 2\n##   lower_ci upper_ci\n##      <dbl>    <dbl>\n## 1     30.1     38.3\nvisualize(bootstrap_distribution) + \n  shade_confidence_interval(endpoints = percentile_ci, color = \"orange\", fill = \"khaki\") +\n  geom_vline(xintercept = mean_grundgesamtheit$Mittelwert, linetype = 'dashed')"},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"bedeutung-der-konfidenzintervalle","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.4 Bedeutung der Konfidenzintervalle","text":"Ein Konfidenzintervall h√§ngt von der Stichprobe ab, ist also vom Zufall betroffen. Man kann also sagen, dass die Grenzen des Konfidenzintervalls Zufallsvariablen sind. Das Konfidenzintervall wird nicht immer den wahren Parameter der Grundgesamtheit einschlie√üen. Die Definition eines 95%-Konfidenzintervalls kann wie folgt formuliert werden:Wenn wir sehr oft die Stichproben neu ziehen und jedesmal ein 95%-Kofidenzintervall berechnen, dann erwarten wir, dass 95% der F√§lle diese Konfidenzintervalle den wahren Parameter der Grundgesamtheit enthalten.Das Konfidenzintervall ist unsere Absch√§tzung der Lage des wahren Parameters der Grundgesamtheit. Eine andere Absch√§tzung haben wir nicht (es sei denn, es gibt wiederum eine theoretische Verteilung). Die Interpretation wird oft abgek√ºrzt, dass man sagt, man sei zu 95% sicher, dass das 95%-Konfidenzintervall den wahren Parameter enth√§lt. Das ist nicht richtig (s. Definition oben). Es ist besser zu sagen, dass 95% der F√§lle, das Konfidenzintervall den wahren Parameter der Grundgesamtheit enth√§lt. mit 95% der F√§lle gemeint ist, wissen Sie ja, wenn Sie sich die genaue Definition erinnern üòÑ.","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"lesestoff-6","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.5 Lesestoff","text":"Kapitel 8 Ismay Kim (2021)","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"aufgaben-5","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.6 Aufgaben","text":"","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"konfidenzintervall-aus-dem-zentralen-grenzwertsatz","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.6.1 Konfidenzintervall aus dem Zentralen Grenzwertsatz","text":"Der Zentrale Grenzwertsatz besagt, dass die Stichprobenverteilung des Sch√§tzers des Mittelwerts sich asymptotisch (also bei vielen Stichproben) der Normalverteilung n√§hert. Daher kann man f√ºr die Konfidenzintervalle auch die folgende Formel nutzen: \\(\\hat{\\mu} \\pm 1.96 \\cdot SE\\), wobei \\(\\hat{\\mu}\\) der gesch√§tzter Mittelwert ist. Das H√ºtchen steht f√ºr gesch√§tzt.Berechnen Sie die Konfidenzintervalle mit dieser Formel f√ºr die Stichprobenverteilung aus dem Bootstrap f√ºr den Mittelwert der Anreisezeit. Dazu passen Sie der Funktion get_confidence_interval den Typ des Konfidenzintervalls type = \"se\" und geben Sie den Mittelwert der Befragung : get_confidence_interval(type = \"se\", point_estimate = mean_befragung$Mittelwert).Stellen Sie die Stichprobenverteilung mit diesem Konfidenzintervall dar und vergleichen Sie mit dem Konfidenzintervall, das wir mit der Quantil-Methode berechnet haben.","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"anteile-an-stadt--und-landbewohnern","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.6.2 Anteile an Stadt- und Landbewohnern","text":"Wiederholen Sie die Analyse, die wir mit dem Paket infer f√ºr die Sch√§tzung des Mittelwerts der Anreisezeit gemacht haben, nun f√ºr die Anteile Stadt- und Landbewohnern. Tip: specify(response = wohnort, success = 'stadt').","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"wie-h√§ngt-das-konfidenzintervall-von-der-stichprobengr√∂√üe-ab","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.6.3 Wie h√§ngt das Konfidenzintervall von der Stichprobengr√∂√üe ab?","text":"Wiederholen Sie die Analyse f√ºr den Mittelwert der Anreisezeit f√ºr eine Stichprobe von 30 Studierenden. Wie ver√§ndert sich das Konfidenzintervall?","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"ihre-arbeit-einreichen-4","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.7 Ihre Arbeit einreichen","text":"Speichern Sie Ihr Notebook ab und laden Sie nur die .Rmd Datei vom Server.Laden Sie Ihre .Rmd Datei ILIAS hoch. Beachten Sie die Deadline!Sie erhalten die Musterl√∂sung nach dem Hochladen.","code":""},{"path":"bootstrapping-und-konfidenzintervalle.html","id":"weiterf√ºhrende-literatur-1","chapter":"Kapitel 8 Bootstrapping und Konfidenzintervalle","heading":"8.8 Weiterf√ºhrende Literatur","text":"Kapitel 17.3 Sauer (2019)","code":""},{"path":"hypothesentsts.html","id":"hypothesentsts","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"Kapitel 9 Hypothesentests mit dem Paket infer","text":"\nIdee hinter simulationsbasierten Tests erkl√§ren\n\nTests mit dem Paket infer durchf√ºhren\n","code":""},{"path":"hypothesentsts.html","id":"es-gibt-nur-einen-test","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.1 Es gibt nur einen Test","text":"Ich wei√ü nicht, wie es Ihnen ergangen ist, aber ich habe meiner Grundausbildung Statistik verschiedene Tests kennen gelernt: \\(t\\)-Test f√ºr gepaarte und ungepaarte Stichproben, \\(F\\)-Test f√ºr die Varianz, \\(z\\)-Test etc. Ende war ich verwirrt und wusste nicht mehr, welchen ich wann nehmen soll. Ende des Tages musste ich, wenn ich einen neuen Datensatz hatte und einen Test brauchte, lange nachdenken, welchen ich nutzen soll.Diese vielen Tests stammen noch aus der Urzeit der Statistik, als Rechenzeit, wenn √ºberhaupt vorhanden, unbezahlbar war. Daher haben die V√§ter der Statistik (war damals √ºberwiegend eine reine M√§nnerclique üò†) viele N√§herungsverfahren entwickelt. Diese N√§herungsverfahren leiten eine theoretische Verteilung f√ºr verschiedene Teststatistiken ab. Tests, die auf solchen N√§herungen basieren, haben h√§ufig starke Annahmen √ºber die Daten, wie z.B. dass die Daten normalverteilt sein m√ºssen, oder dass der Datensatz gro√ü sein muss, damit die N√§herung stimmt.Heute ist f√ºr die meisten unserer Analysen ausreichen Rechenkapazit√§t vorhanden. Daher m√ºssen wir nicht mehr auf solche N√§herungen zur√ºck greifen, sondern k√∂nnen Computersimulationen benutzen. Diese Computersimulationen setzt man ein, um bei Hypothesentests Daten unter der Normalverteilung zu generieren. Abbildung 9.1 zeigt das allgemeine Vorgehen, wie es f√ºr jeden beliebigen Test g√ºltig ist. Dieses Vorgehen kann man auf folgende Schritte ‚Äúrunterkochen‚Äù:Teststatistik aus Stichprobe berechnen\nWir haben eine Stichprobe (data der Abbildung 9.1), die wir mit Hilfe einer Teststatistik zusammen fassen. Wir nennen diese Teststatistik \\(\\sigma^*\\). Sie kann z.B. der Mittelwert der Differenzen zwischen einer Behandlung und einer Kontrolle einem Experiment sein.Nullhypothese formulieren\nWir denken gut √ºber unsere Forschungsfrage nach und √ºberlegen uns, welches Modell besten die Nullhypothese \\(H_0\\), also eine Situation ohne jeglichen Effekt, wiedergibt. Im Falle des Mittelwerts eben eine Welt, der der besagte Mittelwert null ist, es also keinen Behandlungseffekt unserem Experiment gibt. Das Modell f√ºr \\(H_0\\) kann eine Permutation der Daten sein (Permutationstests) oder aus einer theoretischen Verteilung stammen (z.B. Normalverteilung). Es kann auch ein richtig kompliziertes Modell sein. Letzteres ist nicht Bestandteil diese Kurses.Simulation der Daten unter der Nullhypothese\nWir simulieren Daten aus diesem Modell ohne Effekt, d.h. Daten unter der Nullhypothese und berechnen aus jedem simulierten Datensatz dieselbe Teststatistik wie aus der echten Stichprobe.Berechnen der Stichprobenverteilung\nDie vielen simulierten Teststatistiken ergeben eine Stichprobenverteilung.Vergleich der beobachteten Teststatistik mit der Stichprobenverteilung ‚Äì Entscheidung\nNun k√∂nnen wir die Teststatistik \\(\\sigma^*\\) mit der Stichprobenverteilung der simulieren Teststatistiken vergleichen. Wir entscheiden, ob wir \\(\\sigma^*\\) der Welt ohne Effekt, also unter der Nullhypothese h√§ufig oder eher selten vorkommt. Wenn \\(\\sigma^*\\) selten vorkommt, verwerfen wir die Nullhypothese und sagen, dass es einen Effekt gibt. Mit anderen Worte, es ist dann unwahrscheinlich, dass das beobachtete \\(\\sigma^*\\) auf den Zufall zur√ºck zu f√ºhren ist. Als Entscheidungshilfe nutzen wir das Signifikanzniveau \\(\\alpha\\), das h√§ufig auf 5% gesetzt wird.Diese Schritte gelten wirklich f√ºr jeden beliebigen Test. Daher kann man verallgemeinert sagen, dass es nur einen Test (ein Testframework) gibt.\nAbbildung 9.1: Logik hinter den Hypothesentests aus der Sicht der modernen Datenanlyse (Quelle: http://allendowney.blogspot.com/2016/06/--still--one-test.html).\n","code":""},{"path":"hypothesentsts.html","id":"workflow-in-infer","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.2 Workflow in infer","text":"Das Paket infer bietet ein einheitliches Framework f√ºr Hypothesentests (Abbildung 9.2). Es hat 4 Verben, die den oben beschriebenen Prozess der Hypothesentests vereinheitlichen:specify() Variablen festlegenhypothesize() Nullhypothese definierengenerate() Daten unter der Nullhypothese generierencalculate() Stichprobenverteilung (d.g. Verteilung der Teststatistik) berechnenvisualize() Stichprobenverteilung darstellenMit get_p_value kann man den \\(p\\)-Wert berechnen und mit shade_p_value diesen darstellen lassen.\nAbbildung 9.2: Verallgemeinertes Vorgehen bei Hypothesentests (Quelle: https://infer.netlify.app/).\n","code":""},{"path":"hypothesentsts.html","id":"stadt--und-landbewohner-in-werdeschlau","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.2.1 Stadt- und Landbewohner in Werdeschlau","text":"Wir m√∂chten gerne wissen, ob sich unter den Studierenden Werdeschlau genauso viele Stadt- und Landbewohner gibt.Wir simulieren erneut unsere Grundgesamtheit.Nun befragen wir 200 Studierende.Wir berechnen den Anteil der Stadtbewohner der Befragung.Die Nullhypothese und die Alternativhypothese lauten:\\(H_0\\): es gibt keinen Unterschied der Anzahl der Stadt- und Landbewohner, d.h. Anteil der Stadtbewohner \\(p = 0.5\\).\\(H_A\\): Anteil der Stadtbewohner \\(p \\neq 0.5\\)Wir setzten das Ganz nun mit infer um.Das Paket infer setzt generate() die Art der Simulation automatisch (bootstrap, simulate oder permute). F√ºr sogen. Punkthypothesen null = \"point\" bei kategoriellen Variablen, z.B. \\(H_0\\): Anteil der Stadtbewohner = 0.5, simuliert generate() neue Daten mit Hilfe der Funktion sample() und nutzt die hypothesize() definierten Anteil p als Wahrscheinlichket f√ºr success. D.h. unserem Fall simuliert generate() 10000 neue Stichproben mit der Wahrscheinlichkeit von 0.5 f√ºr Stadtbewohner (success = 'stadt'), wie von \\(H_0\\) verlangt üòÑ.Wir sehen uns die Stichprobenverteilung .Ende berechnen wir den \\(p\\)-Wert.Der \\(p\\)-Wert ist sehr klein und kleiner als das Standard-Signifikanzniveau von 5%. Daher schlie√üen wir, dass es sehr unwahrscheinlich ist, dieses Verh√§ltnis von Stadt- und Landbewohnern unter den Studierenden zu beobachten, wenn es wirklich gleich viele Stad- und Landbewohner sind. Ergo, das Verh√§ltnis von Stadt- zu Landbewohnern ist nicht eins zu eins.","code":"\nlibrary(tidyverse)\nlibrary(infer)\nset.seed(123)\n\nstudent_id <- 1:12000\n  \nanreise <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n             runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngeschlecht <- sample(c('m', 'w'), size = 12000, replace = TRUE)\n\nwohnort <- sapply(anreise, function(x) {\n  if(x < 30) 'stadt'\n  else 'land'\n})\n\nverkehrsmittel <- sapply(anreise, function(x) {\n  if(x <= 10) 'zu_fuss'\n  else if(x > 10 & x <= 15) sample(c('zu_fuss', 'fahrrad'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'fahrrad', 'auto'), size = 1)\n  else sample(c('bus', 'auto'), size = 1)\n})\n\nzeit_bib <- 5 * 60 - 0.7 * anreise + rnorm(length(anreise), 0, 20)\n\ngrundgesamtheit <- tibble(student_id, geschlecht, wohnort, verkehrsmittel, anreise, zeit_bib)\n\ngrundgesamtheit## # A tibble: 12,000 x 6\n##    student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##         <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1          1 w          stadt   bus              15.1      294.\n##  2          2 w          land    fahrrad          32.6      254.\n##  3          3 w          stadt   fahrrad          19.3      231.\n##  4          4 m          land    auto             35.9      245.\n##  5          5 m          land    bus              37.9      234.\n##  6          6 w          stadt   zu_fuss           6.59     303.\n##  7          7 w          stadt   bus              23.5      284.\n##  8          8 m          land    auto             36.2      274.\n##  9          9 m          stadt   fahrrad          24.3      299.\n## 10         10 w          stadt   bus              21.0      282.\n## # ‚Ä¶ with 11,990 more rows\nset.seed(345)\n\nbefragung_size <- 200\n\nbefragung <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = 1)\n\nbefragung## # A tibble: 200 x 7\n## # Groups:   replicate [1]\n##    replicate student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##        <int>      <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1         1       1623 m          stadt   zu_fuss           7.06     299.\n##  2         1       9171 m          stadt   fahrrad          11.3      278.\n##  3         1      10207 w          land    bus             107.       199.\n##  4         1       3506 w          stadt   bus              25.0      326.\n##  5         1       8892 w          stadt   bus              28.1      259.\n##  6         1       5460 m          stadt   bus              23.6      299.\n##  7         1       6120 w          stadt   bus              20.0      268.\n##  8         1        865 w          stadt   fahrrad          26.6      290.\n##  9         1      11586 m          land    bus             114.       207.\n## 10         1       8153 w          stadt   zu_fuss           8.06     297.\n## # ‚Ä¶ with 190 more rows\nprop_hat <- befragung %>% \n  specify(response = wohnort, success = \"stadt\") %>%\n  calculate(stat = \"prop\")\n\nprop_hat## # A tibble: 1 x 1\n##    stat\n##   <dbl>\n## 1  0.62\nset.seed(123)\n\nnull_distn <- befragung %>%\n  specify(response = wohnort, success = \"stadt\") %>%\n  hypothesize(null = \"point\", p = .5) %>%\n  generate(reps = 10000) %>%\n  calculate(stat = \"prop\")## Setting `type = \"simulate\"` in `generate()`.\nvisualize(null_distn) +\n  shade_p_value(obs_stat = prop_hat, direction = \"two-sided\")\nnull_distn %>%\n  get_p_value(obs_stat = prop_hat, direction = \"two-sided\")## # A tibble: 1 x 1\n##   p_value\n##     <dbl>\n## 1  0.0008"},{"path":"hypothesentsts.html","id":"pr√§ferenzen-f√ºr-den-wohnort","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.2.2 Pr√§ferenzen f√ºr den Wohnort","text":"Haben weibliche und m√§nnliche Studierende unterschiedliche Pr√§ferenzen f√ºr den Wohnort? Das ist ein √§hnlicher Fall wie der Einf√ºhrung zu Hypothesentests, als wir uns mit den Bef√∂rderungschancen von Fraune und M√§nnern den 70ern befasst haben. Wir haben hier zwei kategoriale Variablen, n√§mlich wohnort und geschlecht und wollen wissen, ob die beiden miteinander zusammenh√§ngen. Daher lauten unsere Nullhypothese und Alternativhypothese:\\(H_0\\): wohnort und geschlecht sind unabh√§ngig.\\(H_A\\): wohnort und geschlecht h√§ngen zusammen.Wir berechnen zun√§chst den Unterschied zwischen Stadtbewohnern nach Geschlecht.Wir basieren unseren Test auf Permutation, d.h. wir permutieren mehrfach eine der Variablen und berechnen die Differenzen den Anteilen der Stadtbewohner je nach Geschlecht f√ºr jede Permutation. Die Permutation wird von generate() automatisch richtig gew√§hlt.Nun Plotten wir die Stichprobenverteilung, die auf Permutation basiert, und f√§rben den \\(p\\)-Wert ein.Der \\(p\\)-Wert betr√§gtDer \\(p\\)-Wert ist gr√∂√üer als das Standard-Signifikanzniveau von 5%, daher k√∂nnen wir die Nullhypothese nicht ablehnen und behalten sie bei. dieser Stelle ist es wichtig, nicht Signifikanzniveau zu drehen, es etwas auf 10% zu setzten, oder irgendwie Datenpunkte heraus zu filtern. Das w√§re \\(p\\)-Hacking. nennt man das k√ºnstliche Dr√ºcken des \\(p\\)-Werts unter die 5%-Schranke, um statistische Signifikanz zu erzeugen. Viel besser ist es, zu berichten, dass die beobachteten Differenzen der Pr√§ferenz von Frauen und M√§nnern f√ºr den Wohnort zuf√§llig sein k√∂nnen, und den \\(p\\)-Wert mit anzugeben. BTW, der wahre Unterschied aus der Grundgesamtheit ist extrem klein.\\(p\\)-Werte werden h√§ufig miss- oder √ºberinterpretiert. Es geht soweit, dass nur wissenschaftliche Ergebnisse mit signifikanten Ausg√§ngen bei Hypothesentests als wertvoll und publizierbar angesehen werden. Davor kann man nur dringend warnen. Diese Einstellung f√ºhrt zur Verzerrung der wissenschaftlichen Ergebnissen. Ich lade Sie ein, mehr dazu bei Wasserstein Lazar (2016) und im Kapitel 9.6 Ismay Kim (2021) nachzulesen.","code":"\nd_hat <- befragung %>% \n  specify(wohnort ~ geschlecht, success = \"stadt\") %>%\n  calculate(stat = \"diff in props\", order = c(\"w\", \"m\"))\n\nd_hat## # A tibble: 1 x 1\n##    stat\n##   <dbl>\n## 1 0.142\nset.seed(123)\n\nnull_distn <- befragung %>%\n  specify(wohnort ~ geschlecht, success = \"stadt\") %>%\n  hypothesize(null = \"independence\") %>% \n  generate(reps = 10000) %>% \n  calculate(stat = \"diff in props\", order = c(\"w\", \"m\"))## Setting `type = \"permute\"` in `generate()`.\nnull_distn## # A tibble: 10,000 x 2\n##    replicate     stat\n##        <int>    <dbl>\n##  1         1 -0.0381 \n##  2         2 -0.0180 \n##  3         3  0.0221 \n##  4         4  0.00201\n##  5         5 -0.0982 \n##  6         6  0.0221 \n##  7         7  0.0622 \n##  8         8  0.0421 \n##  9         9  0.00201\n## 10        10 -0.0581 \n## # ‚Ä¶ with 9,990 more rows\nvisualize(null_distn) +\n  shade_p_value(obs_stat = d_hat, direction = \"two-sided\")\nnull_distn %>%\n  get_p_value(obs_stat = d_hat, direction = \"two-sided\")## # A tibble: 1 x 1\n##   p_value\n##     <dbl>\n## 1  0.0542\ngrundgesamtheit %>% \n  specify(wohnort ~ geschlecht, success = \"stadt\") %>%\n  calculate(stat = \"diff in props\", order = c(\"w\", \"m\"))## # A tibble: 1 x 1\n##       stat\n##      <dbl>\n## 1 -0.00930"},{"path":"hypothesentsts.html","id":"lesestoff-7","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.3 Lesestoff","text":"Kapitel 9.3 bis 9.6 Ismay Kim (2021)","code":""},{"path":"hypothesentsts.html","id":"aufgaben-6","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.4 Aufgaben","text":"","code":""},{"path":"hypothesentsts.html","id":"sind-anreisezeit-und-zeit-in-der-bibliothek-correliert","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.4.1 Sind Anreisezeit und Zeit in der Bibliothek correliert?","text":"F√ºhren Sie einen Hypothesentest durch, ob die Anreisezeit und die Zeit der Bibliothek (zeit_bib) korreliert sind. Formulieren Sie die Null- und die Alternativhypothese mit Ihren eigenen Worten, bevor Sie den Test durchf√ºhren.Tipps: specify(anreise ~ zeit_bib) und calculate(stat = \"correlation\"). Sehr hilfreich dazu ist auch die Webseite von infer (s.u.).","code":""},{"path":"hypothesentsts.html","id":"h√§ngt-die-wahl-des-verkehrsmittels-vom-wohnort-ab","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.4.2 H√§ngt die Wahl des Verkehrsmittels vom Wohnort ab?","text":"Vielleicht k√∂nnen Sie sich noch erinnern, dass wir einer der fr√ºheren Stunden die Kontingenztabelle durchgenommen haben (s. Vorlesung Zusammenhangsma√üe). einer solchen Tabelle fasst man die H√§ufigkeiten von zwei kategoriellen Variablen zusammen. Anhand von einer Tabelle kann man entscheiden, ob es einen Zusammenhang zwischen diesen beiden Variablen gibt. Die Statistik, die man f√ºr eine Tabelle ausrechnet, hei√üt Kontingenzkoeffizient. einem Hypothesentest, genannt \\(\\chi^2\\)-Test, kann man √ºberpr√ºfen, ob dieser Zusammenhang signifikant ist.√úberpr√ºfen Sie nun, ob es einen Zusamenhang gibt zwischen der Wahl des Verkehrsmittels und dem Wohnort der befragten Studierenden. Tipps: specify(formula = wohnort ~ verkehrsmittel) und calculate(stat = \"Chisq\"). Auch hier lohnt ein Blick auf die Webseite von infer (s.u.)\n.\n## Ihre Arbeit einreichen\n- Speichern Sie Ihr Notebook ab und laden Sie nur die .Rmd Datei vom Server.\n- Laden Sie Ihre .Rmd Datei ILIAS hoch. Beachten Sie die Deadline!\n- Sie erhalten die Musterl√∂sung nach dem Hochladen.","code":""},{"path":"hypothesentsts.html","id":"weiterf√ºhrende-literatur-und-videos-1","chapter":"Kapitel 9 Hypothesentests mit dem Paket infer","heading":"9.5 Weiterf√ºhrende Literatur und Videos","text":"Webseite von infer: https://infer.netlify.app/Vortrag des Autors von infer.\n\n\ninfer: package tidy statistical inference - RStudio\n","code":""},{"path":"effekte.html","id":"effekte","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","text":"\nUnterschied zwischen einem Hypothesentest und der Sch√§tzung von Effekten erkl√§ren\nDas starre Konzept der statistischen Signifikanz, bei dem ein willk√ºrlich gesetzter Grenzwert \\(\\alpha\\) dar√ºber entscheidet, ob ein Ergebnis weiter beachtet wird oder nicht, wird seit Jahrzehnten kritisiert (Wasserstein, Schirm, Lazar 2019). Es ist schlie√ülich auch wirklich schwer zu verstehen, warum \\(p = 0.049\\) qualitativ etwas anderes aussagen soll als \\(p = 0.051\\). Und machen wir mit \\(p = 0.05\\)? Die beste L√∂sung ist, wie von der ASA (American Statistical Association) vorgeschlagen, den Begriff ‚Äúsignifikant‚Äù nicht mehr zu verwenden. Statt dessen berichten Sie den berechneten \\(p\\)-Wert und interpretieren ihn im wissenschaftlichen Kontext (Wasserstein, Schirm, Lazar 2019).Ich w√ºrde nicht soweit gehen, statistische Tests abschaffen zu wollen. Sie sind manchmal n√ºtzlich, aber auch nicht h√§ufiger als manchmal. Allerdings kann ich auch aus eigener Praxis sagen, dass sehr oft statistische Signifikanz mit wissenschaftlicher Relevanz verwechselt wird. Daher werden wir uns diesem Kapitel damit besch√§ftigen, wie man die relevanten Aspekte, n√§mlich die Gr√∂√üe des Effekts, aus den beobachteten Daten sch√§tzen kann.","code":""},{"path":"effekte.html","id":"gr√∂√üe-des-effekts","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.1 Gr√∂√üe des Effekts","text":"Daten sind immer mit Unsicherheiten behaftet. Modelle und Tests haben Annahmen, z.B. dass die gew√§hlte Statistik oder das Modell der Nullhypothese f√ºr die Forschungsfrage geeignet sind. Diese Unsicherheit gilt es richtig zu erfassen und zu berichten. Daher ist es wichtig, f√ºr jede Sch√§tzung entweder ein Konfidenzintervall anzugeben oder den Standardfehler (Wasserstein, Schirm, Lazar 2019).Wir werden zum Sch√§tzen von Effekten f√ºr einfache statistische Analysen das Paket dabestr (https://github.com/ACCLAB/dabestr) benutzen. Es bietet die M√∂glichkeit, Effekte zu sch√§tzen, Bootstrap-Konfidenzintervalle zu berechnen und das Ganze sehr ansprechenden Grafiken zu visualisieren. Zu dem Paket gibt es eine Publikation, Ho et al. (2019), die ich sehr empfehlen kann.","code":""},{"path":"effekte.html","id":"sch√§tzen-des-effekts-mit-dem-bootstrap-dabestr","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.2 Sch√§tzen des Effekts mit dem Bootstrap ‚Äì dabestr","text":"Wir laden die n√∂tigen Bibliotheken.und holen uns wieder unsere Lieblingsgrundgesamtheit und Befragung üòÑ.Wir w√ºrden gerne den Unterschied der Arbeitszeit der Bibliothek zwischen M√§nnern und Frauen kennen. D.h. wir m√∂chten diesen Unterschied sch√§tzen und auch die Unsicherheit der Sch√§tzung quantifizieren. Ich hoffe, der Wortlaut kommt Ihnen bekannt vor ü§ì.Die wichtigste Funktion dabestr hei√üt dabest(). Sie erstellt einen tidy tibble im richtigen Format f√ºr die Sch√§tzung und das Bootstrap. Aktuell k√∂nnen Sie mit dabestr folgende Effekte sch√§tzen:Differenz zwischen Mittelwerten von Gruppen mit mean_diff().Differenz zwischen Medianen von Gruppen mit median_diff().Cohen‚Äôs d mit cohens_d(). Cohen‚Äôs d ist die Differenz zwischen Mittelwerten geteilt durch die Standardabweichung der Daten.Hedges‚Äô g mit hedges_g(). √Ñhnlich wie Cohen‚Äôs d.Cliff‚Äôs delta mit cliffs_delta(). Entwickelt f√ºr ordinalskalierte Daten, √ºberpr√ºft wie oft Datenpunkte einer Gruppe gr√∂√üer sind als Datenpunkte einer Vergleichsgruppe.Wir stellen unsere Daten mit Hilfe von dabest() zusammen. Der Parameter idx gibt , wie die Differenz berechnet wird, n√§mlich M√§nner - Frauen.Wir sehen uns mit dabest() erstellten Datensatz .Wir berechnen die Unterschiede Mittelwerten von zeit_bib zwischen den Geschlechtern und bootstrappen einem Schritt.Sie bekommen die R√ºckmeldung, dass ungepaarte Differenzen zwischen den Gruppen ausgerechnet wurden. Ungepaart bedeutet, dass die Frauen und M√§nner statistisch nicht verbunden sind. Etwas anderes w√§re es, wenn man dieselben Studierenden wiederholt befragen w√ºrde (und dazwischen z.B. etwas unternehmen w√ºrde, um deren Meinung zu beeinflussen). Weiterhin zeigt Ihnen dabest auch gleich die Bootstrap-Konfidenzintervalle.Die Sch√§tzung des Unterschieds stellen wir einem Gardner-Altman-Plot dar (Gardner Altman 1986). Wir beschriften die Grafik gleich sinnvoll und schalten die Legende ab, da sie diesem Fall redundant ist.der Abbildung k√∂nnen Sie mehrere Dinge sehen:alle gemessenen Punkte (Zeiten der Bibliothek). D.h. Sie k√∂nnen die Streuung der Daten erkennen. Das ist sehr informativ.die Sch√§tzung der mittleren Differenz (dicker schwarzer Punkt) und als graue Fl√§che die Stichprobenverteilung aus dem Bootstrap.das 95%-Konfidenzintervall f√ºr die Sch√§tzung (dicker schwarzer vertikaler Strich)Wir k√∂nnen nun erkennen, dass die Sch√§tzung recht genau ist, da das Konfidenzintervall schmal ist. Frauen scheinen mehr Zeit der Bibliothek zu verbringen, da die gesch√§tzte Differenz negativ ist. Das k√∂nnen wir genau abfragen, mit mean_diff_zeit_bib$result$difference. Das sind -4.4 Minuten. Dieser Unterschied ist f√ºr alle praktischen Belange irrelevant. Das Konfidenzintervall umfasst auch die Null, sodass kein Unterschied auch eine plausible Sch√§tzung ist. Somit w√ºrden wir schlie√üen, dass es nicht genug Evidenz f√ºr einen relevanten Unterschied der Arbeitszeit der Bibliothek zwischen M√§nnern und Frauen gibt.","code":"\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(dabestr)\nset.seed(123)\n\nstudent_id <- 1:12000\n  \nanreise <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n             runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngeschlecht <- sample(c('m', 'w'), size = 12000, replace = TRUE)\n\nwohnort <- sapply(anreise, function(x) {\n  if(x < 30) 'stadt'\n  else 'land'\n})\n\nverkehrsmittel <- sapply(anreise, function(x) {\n  if(x <= 10) 'zu_fuss'\n  else if(x > 10 & x <= 15) sample(c('zu_fuss', 'fahrrad'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'fahrrad', 'auto'), size = 1)\n  else sample(c('bus', 'auto'), size = 1)\n})\n\nzeit_bib <- 5 * 60 - 0.7 * anreise + rnorm(length(anreise), 0, 20)\n\ngrundgesamtheit <- tibble(student_id, geschlecht, wohnort, verkehrsmittel, anreise, zeit_bib)\n\ngrundgesamtheit## # A tibble: 12,000 x 6\n##    student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##         <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1          1 w          stadt   bus              15.1      294.\n##  2          2 w          land    fahrrad          32.6      254.\n##  3          3 w          stadt   fahrrad          19.3      231.\n##  4          4 m          land    auto             35.9      245.\n##  5          5 m          land    bus              37.9      234.\n##  6          6 w          stadt   zu_fuss           6.59     303.\n##  7          7 w          stadt   bus              23.5      284.\n##  8          8 m          land    auto             36.2      274.\n##  9          9 m          stadt   fahrrad          24.3      299.\n## 10         10 w          stadt   bus              21.0      282.\n## # ‚Ä¶ with 11,990 more rows\nset.seed(345)\n\nbefragung_size <- 200\n\nbefragung <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = 1)\n\nbefragung## # A tibble: 200 x 7\n## # Groups:   replicate [1]\n##    replicate student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##        <int>      <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1         1       1623 m          stadt   zu_fuss           7.06     299.\n##  2         1       9171 m          stadt   fahrrad          11.3      278.\n##  3         1      10207 w          land    bus             107.       199.\n##  4         1       3506 w          stadt   bus              25.0      326.\n##  5         1       8892 w          stadt   bus              28.1      259.\n##  6         1       5460 m          stadt   bus              23.6      299.\n##  7         1       6120 w          stadt   bus              20.0      268.\n##  8         1        865 w          stadt   fahrrad          26.6      290.\n##  9         1      11586 m          land    bus             114.       207.\n## 10         1       8153 w          stadt   zu_fuss           8.06     297.\n## # ‚Ä¶ with 190 more rows\ndiff_zeit_bib <- befragung %>%\n  dabest(x = geschlecht, y = zeit_bib, \n         idx = c('w', 'm'), \n         paired = FALSE)\ndiff_zeit_bib## dabestr (Data Analysis with Bootstrap Estimation in R) v0.3.0\n## =============================================================\n## \n## Good morning!\n## The current time is 08:56  on Mittwoch Juni 16, 2021.\n## \n## Dataset    :  .\n## The first five rows are:\n## # A tibble: 5 x 7\n## # Groups:   replicate [1]\n##   replicate student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##       <int>      <int> <fct>      <chr>   <chr>            <dbl>    <dbl>\n## 1         1       1623 m          stadt   zu_fuss           7.06     299.\n## 2         1       9171 m          stadt   fahrrad          11.3      278.\n## 3         1      10207 w          land    bus             107.       199.\n## 4         1       3506 w          stadt   bus              25.0      326.\n## 5         1       8892 w          stadt   bus              28.1      259.\n## \n## X Variable :  geschlecht\n## Y Variable :  zeit_bib\n## \n## Effect sizes(s) will be computed for:\n##   1. m minus w\nset.seed(123)\n\nmean_diff_zeit_bib <- diff_zeit_bib %>%\n  mean_diff(reps = 10000)\n\nmean_diff_zeit_bib## dabestr (Data Analysis with Bootstrap Estimation in R) v0.3.0\n## =============================================================\n## \n## Good morning!\n## The current time is 08:56  on Mittwoch Juni 16, 2021.\n## \n## Dataset    :  .\n## X Variable :  geschlecht\n## Y Variable :  zeit_bib\n## \n## Unpaired mean difference of m (n = 105) minus w (n = 95)\n##  -4.41 [95CI  -12.9; 4.2]\n## \n## \n## 10000 bootstrap resamples.\n## All confidence intervals are bias-corrected and accelerated.\nplot(mean_diff_zeit_bib, color.column = geschlecht, rawplot.ylabel = 'Zeit in der Bibliothek (min)', effsize.ylabel = 'Ungepaarte mittlere Differenz', show.legend = F)"},{"path":"effekte.html","id":"welche-fragen-soll-ich-mir-bei-der-analyse-stellen","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.3 Welche Fragen soll ich mir bei der Analyse stellen?","text":"Bei der statistischen Analyse Ihrer Daten, sollten Sie sich vom ‚Äústatistischen Denken‚Äù leiten lassen. Wasserstein, Schirm, Lazar (2019) beschreibt es alsAccept uncertainty. thoughtful, open, modest.Denken Sie immer daran, dass Unsicherheit jedem Forschungsunterfangen inne wohnt. Wenn Sie Ihre Daten interpretieren, stellen Sie sich folgende Fragen (Anderson 2019):Welche praktische Bedeutung hat meine Sch√§tzung des Effekts? Ist er √ºberhaupt relevant?Wie pr√§zise ist die Sch√§tzung?Passt das Modell zu meiner Forschungsfrage? Ist es korrekt formuliert?","code":""},{"path":"effekte.html","id":"was-dabest-nicht-kann-besorgt-infer","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.4 Was dabest nicht kann, besorgt infer","text":"Aktuell kann dabestr nicht mit Anteilen (proportions) arbeiten. Daher werden wir bei solchen Aufgaben auf unser bew√§hrtes Framework infer zur√ºck greifen.","code":""},{"path":"effekte.html","id":"lesestoff-8","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.5 Lesestoff","text":"Ho et al. (2019)","code":""},{"path":"effekte.html","id":"aufgaben-7","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.6 Aufgaben","text":"","code":""},{"path":"effekte.html","id":"bodenverdichtung-revisited","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.6.1 Bodenverdichtung, revisited","text":"Wir kommen zur√ºck zu Daten aus Aufgabe 13.3.3. Wie stark hat sich die Lagerungsdichte auf den befahrenen Feldern ver√§ndert? Sch√§tzen Sie den Effekt und geben Sie 95%-Konfidenzintervalle . Vergleichen Sie die Aussagen, die Sie mit dieser L√∂sung treffen k√∂nnen, mit der Aussage aus dem Hypothesentest.","code":""},{"path":"effekte.html","id":"anteil-von-besch√§ftigten-frauen-im-privaten-und-√∂ffentlichen-sektor-revisited","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.6.2 Anteil von besch√§ftigten Frauen im privaten und √∂ffentlichen Sektor, revisited","text":"Wir sehen uns erneut die Daten der Weltbank aus der Aufgabe 13.3.4 . Diesmal interessiert und die Sch√§tzung des Anteils und ihr 95%-Bootstrap-Konfidenzintervall. Vergleichen Sie die Aussagen, die Sie mit dieser L√∂sung treffen k√∂nnen, mit der Aussage aus dem Hypothesentest.","code":""},{"path":"effekte.html","id":"ihre-arbeit-einreichen-5","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.7 Ihre Arbeit einreichen","text":"Speichern Sie Ihr Notebook ab und laden Sie nur die .Rmd Datei vom Server.Laden Sie Ihre .Rmd Datei ILIAS hoch. Beachten Sie die Deadline!Sie erhalten die Musterl√∂sung nach dem Hochladen.","code":""},{"path":"effekte.html","id":"weitere-infos","chapter":"Kapitel 10 Sch√§tzen von Effekten: Raus aus der \\(p < 0.05\\)-Falle","heading":"10.8 Weitere Infos","text":"Vignette von dabestr: https://cran.r-project.org/web/packages/dabestr/vignettes/using-dabestr.html","code":""},{"path":"regression.html","id":"regression","chapter":"Kapitel 11 Lineare Regression","heading":"Kapitel 11 Lineare Regression","text":"\nallgemeinen Aufbau eines Regressionsmodells erkl√§ren\n\nAnnahmen der linearen Normalregression benennen\n\neinfache lineare Regression selbst R durchf√ºhren\ndiesem Kapitel werden wir die statistische Modellierung einsteigen. Bisher haben Sie gelernt, wie man mit Hilfe von modernen Resamplingverfahren oder Simulationen Hypothesentests durchf√ºhrt und Konfidenzintervalle berechnet. Wir werden sp√§teren Kapiteln auch f√ºr die Modellierung Bootstrap verwenden.Es gibt im wesentlichen zwei Gr√ºnde, warum man modelliert.Wir vermuten einen Zusammenhang zwischen Variablen und wollen diesen √ºberpr√ºfen (explikatives Modellieren).Wir wollen ein Modell zur Vorhersage entwickeln (pr√§diktives Modellieren).Wir werden uns diesem Kurs nur mit dem explikativen (erkl√§renden) Modellieren besch√§ftigen.","code":""},{"path":"regression.html","id":"begriff-regression","chapter":"Kapitel 11 Lineare Regression","heading":"11.1 Begriff Regression","text":"Woher kommt der Begriff Regression? Diesen pr√§gte Sir Francis Galdon (1822-1911) (Fahrmeir, Kneib, Lang 2009). Galton interessierte sich unter anderem f√ºr den Zusammenhang zwischen der durchschnittlichen K√∂rpergr√∂√üe der Eltern und der K√∂rpergr√∂√üe ihrer erwachsenen Kinder. Leider war er nicht nur einer der V√§ter der Statistik, sondern auch ein Rassist.Galton stellte fest, dass Kinder von unterdurchschnittlich kleinen Eltern eher gr√∂√üer waren und umgekehrt, Kinder von √ºberdurchschnittlich gro√üen Eltern eher kleiner waren. Diesen Effekt nannte er Regression (R√ºckkehr) zur Mitte.","code":""},{"path":"regression.html","id":"idee-der-regression","chapter":"Kapitel 11 Lineare Regression","heading":"11.2 Idee der Regression","text":"Die Regression ist ein Modell, dass einen Zusammenhang zwischen Variablen analysiert. Wenn dieser Zusammenhang linear ist, dann nennt man das Modell lineare Regression. Wir werden uns ausschlie√ülich mit solchen linearen Modellen besch√§ftigen.Die lineare Regression untersucht also den linearen Zusammenhang zwischen den sogen. erkl√§renden Variablen und der Zielvariablen. Im historischen Beispiel von Galton gab es nur eine erkl√§rende Variable, n√§mlich die Durchschnittsgr√∂√üe der Eltern. Die Zielvariable war die zu erwartende Gr√∂√üe der Kinder. Es ging also nicht darum, die exakte Gr√∂√üe eines bestimmten Kindes zu berechnen, sondern den Einfluss der Durchschnittsgr√∂√üe der Eltern auf die zu erwartende Gr√∂√üe der Kinder. Es ging also um den systematischen Einfluss, nicht um bestimmte Eltern-Kind-Paare. Diese waren nur Stichproben. Sp√§testens hier sollte es klingeln, denn die Gr√∂√üe der Kinder ist somit eine Zufallsvariable.Die Zielvariable muss nicht immer stetig wie die K√∂rpergr√∂√üe sein. Sie kann bin√§r, kategorial oder eine Z√§hlvariable sein. Auch die erkl√§renden Variablen k√∂nnen stetig, bin√§r oder kategorial sein. Das macht die Regessionsmodelle sehr divers. Wir werden uns im Wesentlichen mit numerischen Zielvariablen besch√§ftigen.Wir k√∂nnen somit die Regression zusammenfassen:\nDie Regression ist ein Modell der Form\n\n\\[y = f(X) + \\varepsilon\\]\n\n\\(y\\): Zielvariable\n\n\\(f\\): Art des Zusammenhangs\n\n\\(X\\): Pr√§diktoren (erkl√§rende Variablen auch Kovariablen)\n\n\\(\\varepsilon\\): Fehlerterm\n\nWenn:\n\n\\(f\\) linear ist (Einfluss der Pr√§diktoren addiert sich), spricht man von linearer Regression\n\n\\(X\\) nur ein Pr√§diktor ist, spricht man von einfacher Regression, sonst von multipler Regression\n\nModellkomponenten:\n\n\\(f(X)\\): systematische oder deterministische Komponente\n\n\\(\\varepsilon\\): stochastische Komponente (St√∂rgr√∂√üe, Fehlerterm)\nEs geht bei der Regression also darum, die systematische Komponente zu modellieren. Der Zusammenhang zwischen Pr√§diktoren und der Zielvariablen ist nie exakt, es gibt also einen Fehlerterm. Die Zielgr√∂√üe ist eine Zufallsvariable, deren Verteilung von den Pr√§diktoren abh√§ngt.","code":""},{"path":"regression.html","id":"einfache-lineare-regression","chapter":"Kapitel 11 Lineare Regression","heading":"11.3 Einfache lineare Regression","text":"Bei einer einfachen linearen Regression gibt es nur einen Pr√§diktor. Der Zusammenhang zwischen der Zielvariablen und diesem Pr√§diktor ist linear. Somit hat das Model die Form einer Geraden. Eine Gerade kann man ja mit Hilfe des \\(y\\)-Achsenabschnitts und der Steigung beschreiben. Und genauso sieht das einfache lineare Regressionsmodell aus.\nGegeben sind Datenpaare: \\((y_i,x_i), \\quad =1,\\dots,n\\) zu metrischen Variablen \\(y\\) und \\(x\\).\n\nDas Modell \\[y_i=\\beta_0 + \\beta_1x_i + \\varepsilon_i, \\qquad =1,\\dots,n.\\] hei√üt einfaches lineares Regressionsmodell, wenn die Fehler \\(\\varepsilon_1,\\dots, \\varepsilon_n\\) unabh√§ngig und identisch verteilt sind (iid) mit\n\n\\[\\mathrm{E}(\\varepsilon_i) = 0, \\qquad \\mathrm{Var}(\\varepsilon_i)=\\sigma^2.\\] Wenn zus√§tzlich gilt \\[\\varepsilon_i \\sim N(0,\\sigma^2)\\] d.h. die Residuen normalverteilt sind, sprechen wir von klassischer Normalregression.\n\n\\(\\beta_0\\) hei√üt \\(y\\)-Achsenabschnitt und \\(\\beta_1\\) Steigung des Modells.\n\\(\\varepsilon_i\\) steht f√ºr Fehler, die wir im Modell machen. Das sind die Unterschiede, genannt Residuen, zwischen dem, das Modell dem systematischen Teil (Geradengleichung) \\(\\beta_0 + \\beta_1x_i\\) ausrechnet und dem tats√§chlich gemessenen Wert \\(y_i\\).\\(\\mathrm{E}\\) steht f√ºr Erwartungswert. nennt man den theoretischen Mittelwert einer Zufallsvariablen. Und \\(\\mathrm{Var}\\) steht f√ºr Varianz. Beim einfachen linearen Regressionsmodell nimmt man also , dass die Fehler im Mittel Null sind. Sie werden nat√ºrlich nie alle Null sein, sondern sie werden variieren. Der Fehlerterm ist also eine Zufallsvariable, dessen Varianz fest sein soll. Eine feste Varianz nennt man Homoskedastizit√§t und die Fehler entsprechend homoskedastisch. Eine Varianz, die schwankt, bezeichnet man als Heteroskedastizit√§t. Das bedeutet unter anderem, dass die Fehler f√ºr kleine und gro√üe Werte im Modell √§hnlich sein m√ºssen. Um auf Galtons Beispiel zur√ºck zu kommen, das Modell soll sowohl die Gr√∂√üe der gro√üen als auch der kleinen Kinder gleich gut erkl√§ren.","code":""},{"path":"regression.html","id":"beispiel-zusammenhang-zwischen-der-anreisezeit-und-der-arbeitszeit-in-der-bibliothek","chapter":"Kapitel 11 Lineare Regression","heading":"11.4 Beispiel: Zusammenhang zwischen der Anreisezeit und der Arbeitszeit in der Bibliothek","text":"Als Beispiel f√ºr eine einfache lineare Regression nutzen wir wieder unsere simulierten Daten. Zun√§chst laden wir die Bibliotheken. Die Bibliothek kntir brauchen wir f√ºr das Layouten der Tabellen.Wir generieren wieder unsere Liebliengsgrundgesamtheit.Und befragen 200 Studierende.","code":"\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(moderndive)\nlibrary(knitr)\nset.seed(123)\n\nstudent_id <- 1:12000\n  \nanreise <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n             runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngeschlecht <- sample(c('m', 'w'), size = 12000, replace = TRUE)\n\nwohnort <- sapply(anreise, function(x) {\n  if(x < 30) 'stadt'\n  else 'land'\n})\n\nverkehrsmittel <- sapply(anreise, function(x) {\n  if(x <= 10) 'zu_fuss'\n  else if(x > 10 & x <= 15) sample(c('zu_fuss', 'fahrrad'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'fahrrad', 'auto'), size = 1)\n  else sample(c('bus', 'auto'), size = 1)\n})\n\nzeit_bib <- 5 * 60 - 0.7 * anreise + rnorm(length(anreise), 0, 20)\n\ngrundgesamtheit <- tibble(student_id, geschlecht, wohnort, verkehrsmittel, anreise, zeit_bib)\n\ngrundgesamtheit## # A tibble: 12,000 x 6\n##    student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##         <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1          1 w          stadt   bus              15.1      294.\n##  2          2 w          land    fahrrad          32.6      254.\n##  3          3 w          stadt   fahrrad          19.3      231.\n##  4          4 m          land    auto             35.9      245.\n##  5          5 m          land    bus              37.9      234.\n##  6          6 w          stadt   zu_fuss           6.59     303.\n##  7          7 w          stadt   bus              23.5      284.\n##  8          8 m          land    auto             36.2      274.\n##  9          9 m          stadt   fahrrad          24.3      299.\n## 10         10 w          stadt   bus              21.0      282.\n## # ‚Ä¶ with 11,990 more rows\nset.seed(345)\n\nbefragung_size <- 200\n\nbefragung <- rep_sample_n(grundgesamtheit, size = befragung_size, replace = FALSE, reps = 1)\n\nbefragung## # A tibble: 200 x 7\n## # Groups:   replicate [1]\n##    replicate student_id geschlecht wohnort verkehrsmittel anreise zeit_bib\n##        <int>      <int> <chr>      <chr>   <chr>            <dbl>    <dbl>\n##  1         1       1623 m          stadt   zu_fuss           7.06     299.\n##  2         1       9171 m          stadt   fahrrad          11.3      278.\n##  3         1      10207 w          land    bus             107.       199.\n##  4         1       3506 w          stadt   bus              25.0      326.\n##  5         1       8892 w          stadt   bus              28.1      259.\n##  6         1       5460 m          stadt   bus              23.6      299.\n##  7         1       6120 w          stadt   bus              20.0      268.\n##  8         1        865 w          stadt   fahrrad          26.6      290.\n##  9         1      11586 m          land    bus             114.       207.\n## 10         1       8153 w          stadt   zu_fuss           8.06     297.\n## # ‚Ä¶ with 190 more rows"},{"path":"regression.html","id":"modell-anpassen","chapter":"Kapitel 11 Lineare Regression","heading":"11.4.1 Modell anpassen","text":"Wir unterstellen einen linearen Zusammenhang zwischen zeit_bib und anreise und passen ein lineares Modell . Dazu nutzen wir die Funktion lm(). Sie braucht die Zielvariable und den Pr√§diktor, die Sie mit Tilde verbunden angeben. Das ist die sogen. formula (Formel, √§hnlich wie eine Matheformel). Die Tilde hatten wir schon √§hnlich bei der Bibliothek infer benutzt. Au√üerdem m√ºssen wir noch den Datensatz, dem die Variablen zu finden sind, benennen.","code":"\nlin_mod <- lm(zeit_bib ~ anreise, data = befragung)"},{"path":"regression.html","id":"modellergebnisse-ansehen","chapter":"Kapitel 11 Lineare Regression","heading":"11.4.2 Modellergebnisse ansehen","text":"Die Struktur eines solchen linearen Modellobjekts ist richtig kompliziert. Daher gibt es verschiedene Methoden, um aus diesem Objekt sinnvolle Information zu entnehmen.Als erstes sehen wir uns die Zusammenfassung des Modells . Die nicht tidy-Form enth√§lt sehr viel Information, die man Anfang gar nicht braucht. Und sie gl√§nzt mit vielen Signifikanz-Sternchen, die wir liebsten gleich verbannen w√ºrden.Daher empfehle ich die tidy-Form. Wir nutzen die Funktion get_regression_table aus der Bibliothek moderndive, die intern auf die Bibliothek broom zugreift (https://cran.r-project.org/web/packages/broom/vignettes/broom.html). broom hilft, die Ausgabe des linearen Modells eine tidy-Form zu konvertieren. Die Funktion kable() aus der Bibliothek knitr layoutet die Tabelle.Sie sehen der ersten Spalte (Intercept) und anreise. Das sind der \\(y\\)-Achsenabschnitt \\(\\beta_0\\) und die Steigung \\(\\beta_1\\) des Modells. Das hei√üt unser Modell lautet ausgeschrieben:\\[\\widehat{\\text{zeit_bib}_i} = 302.094 - 0.766 \\cdot \\text{anreise_i}\\]\nDer Index \\(\\) steht hier f√ºr die unterschiedlichen Studierenden, denn jede(r) hat nat√ºrlich eine eigene Anreise- und Arbeitszeit der Bibliothek. Wir lernen also, dass mit steigender Anreisezeit, die Arbeitszeit der Bibliothek sinkt. Und zwar k√∂nnen wir es sogar noch genauer sagen: mit jeder zus√§tzlichen Minute Anreisezeit, sinkt die Arbeitszeit der Bibliothek um 0.766 Minuten. Auf die √ºbrigen Spalten kommen wir sp√§ter zu sprechen.Welche Werte hat das Modell berechnet? Diese nennt man angepasste Werte (fitted) und man kann sie mit der Funktion fitted() abfragen. Wir sehen uns nur die ersten Eintr√§ge .Wir w√ºrden gerne diese angepassten Werte mit den echten gemessenen Werten, n√§mlich der tats√§chlichen Arbeitszeit der Bibliothek, vergleichen. Daf√ºr f√ºgen wir die gemessenen und die angepassten Werte einem tibble zusammen. Wir erstellen eine neue Spalte mit angepassten Werten und den Residuen, d.h. den Differenzen zwischen den gemessenen und den angepassten Werten.Nun k√∂nnen wir die Werte gegeneinander plotten und uns die Residuen ansehen. Diese sind als graue vertikale Linien zwischen den gemessenen und den angepassten Werten dargestellt. Die angepassten Werte liegen alle auf einer Geraden, das ist ja die Quintessenz eines linearen Modells. Zu jedem gemessenen Wert gibt es einen angepassten, modellierten Wert auf der Geraden. Das geom geom_abline zeichnet unsere Gerade.","code":"\nstr(lin_mod)## List of 12\n##  $ coefficients : Named num [1:2] 302.094 -0.766\n##   ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"anreise\"\n##  $ residuals    : Named num [1:200] 2.55 -15.31 -21.32 42.64 -21.19 ...\n##   ..- attr(*, \"names\")= chr [1:200] \"1\" \"2\" \"3\" \"4\" ...\n##  $ effects      : Named num [1:200] -3903 -312.5 -24.2 42.7 -21.2 ...\n##   ..- attr(*, \"names\")= chr [1:200] \"(Intercept)\" \"anreise\" \"\" \"\" ...\n##  $ rank         : int 2\n##  $ fitted.values: Named num [1:200] 297 293 220 283 281 ...\n##   ..- attr(*, \"names\")= chr [1:200] \"1\" \"2\" \"3\" \"4\" ...\n##  $ assign       : int [1:2] 0 1\n##  $ qr           :List of 5\n##   ..$ qr   : num [1:200, 1:2] -14.1421 0.0707 0.0707 0.0707 0.0707 ...\n##   .. ..- attr(*, \"dimnames\")=List of 2\n##   .. .. ..$ : chr [1:200] \"1\" \"2\" \"3\" \"4\" ...\n##   .. .. ..$ : chr [1:2] \"(Intercept)\" \"anreise\"\n##   .. ..- attr(*, \"assign\")= int [1:2] 0 1\n##   ..$ qraux: num [1:2] 1.07 1.05\n##   ..$ pivot: int [1:2] 1 2\n##   ..$ tol  : num 1e-07\n##   ..$ rank : int 2\n##   ..- attr(*, \"class\")= chr \"qr\"\n##  $ df.residual  : int 198\n##  $ xlevels      : Named list()\n##  $ call         : language lm(formula = zeit_bib ~ anreise, data = befragung)\n##  $ terms        :Classes 'terms', 'formula'  language zeit_bib ~ anreise\n##   .. ..- attr(*, \"variables\")= language list(zeit_bib, anreise)\n##   .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n##   .. .. ..- attr(*, \"dimnames\")=List of 2\n##   .. .. .. ..$ : chr [1:2] \"zeit_bib\" \"anreise\"\n##   .. .. .. ..$ : chr \"anreise\"\n##   .. ..- attr(*, \"term.labels\")= chr \"anreise\"\n##   .. ..- attr(*, \"order\")= int 1\n##   .. ..- attr(*, \"intercept\")= int 1\n##   .. ..- attr(*, \"response\")= int 1\n##   .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n##   .. ..- attr(*, \"predvars\")= language list(zeit_bib, anreise)\n##   .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n##   .. .. ..- attr(*, \"names\")= chr [1:2] \"zeit_bib\" \"anreise\"\n##  $ model        :'data.frame':   200 obs. of  2 variables:\n##   ..$ zeit_bib: num [1:200] 299 278 199 326 259 ...\n##   ..$ anreise : num [1:200] 7.06 11.34 106.81 24.97 28.11 ...\n##   ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language zeit_bib ~ anreise\n##   .. .. ..- attr(*, \"variables\")= language list(zeit_bib, anreise)\n##   .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n##   .. .. .. ..- attr(*, \"dimnames\")=List of 2\n##   .. .. .. .. ..$ : chr [1:2] \"zeit_bib\" \"anreise\"\n##   .. .. .. .. ..$ : chr \"anreise\"\n##   .. .. ..- attr(*, \"term.labels\")= chr \"anreise\"\n##   .. .. ..- attr(*, \"order\")= int 1\n##   .. .. ..- attr(*, \"intercept\")= int 1\n##   .. .. ..- attr(*, \"response\")= int 1\n##   .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n##   .. .. ..- attr(*, \"predvars\")= language list(zeit_bib, anreise)\n##   .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n##   .. .. .. ..- attr(*, \"names\")= chr [1:2] \"zeit_bib\" \"anreise\"\n##  - attr(*, \"class\")= chr \"lm\"\nsummary(lin_mod)## \n## Call:\n## lm(formula = zeit_bib ~ anreise, data = befragung)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -60.060 -14.063   0.836  15.662  64.151 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 302.09377    2.28190  132.39   <2e-16 ***\n## anreise      -0.76627    0.05112  -14.99   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 20.85 on 198 degrees of freedom\n## Multiple R-squared:  0.5316, Adjusted R-squared:  0.5292 \n## F-statistic: 224.7 on 1 and 198 DF,  p-value: < 2.2e-16\nget_regression_table(lin_mod) %>% kable()\nhead(fitted(lin_mod))##        1        2        3        4        5        6 \n## 296.6866 293.4026 220.2491 282.9618 280.5521 284.0451\nmodel_res <- befragung %>%\n  mutate(fitted = fitted(lin_mod), residuals = residuals(lin_mod)) \nggplot(model_res, aes(x = anreise, y = zeit_bib)) +\n  geom_segment(aes(xend = anreise, yend = fitted, lty = 'Residuen'), alpha = 0.2)  + \n  geom_abline(intercept = coef(lin_mod)[1], slope = coef(lin_mod)[2], color = \"lightblue\") +\n  geom_point(aes(col = 'observed')) +\n  geom_point(aes(y = fitted, col = 'fitted'), shape = 1, size = 2) +\n  labs(x = 'Anreisezeit (min)', y = 'Arbeitszeit in der Bibliothek (min)') +\n  scale_color_manual(name = '', values = c(observed = 'black', fitted = 'blue'), breaks = c('observed', 'fitted'), label = c('Gemessene Werte', 'Angepasste Werte')) +\n  scale_linetype_manual(name = '', values = ('Residuen' = 'solid')) +\n  theme(legend.position = 'bottom')"},{"path":"regression.html","id":"modellannahmen-√ºberpr√ºfen","chapter":"Kapitel 11 Lineare Regression","heading":"11.4.3 Modellannahmen √ºberpr√ºfen","text":"Bei der linearen Regression nehmen wir , das der Zusammenhang zwischen der Zielvariablen und dem Pr√§diktor linear ist. Das k√∂nnen wir der oberen Grafik bereits erkennen. Weiterin nehmen wir , dass die Residuen im Mittel um die Null schwanken und homoskedastisch sind, d.h. ihre Varianz ist gleich. Diese Annahmen m√ºssen wir √ºberpr√ºfen, bevor es ans Interpretieren der Modellparameter geht. Das macht man haupts√§chlich grafisch. Wir plotten die Residuen gegen die angepassten Werte.Diese Darstellung der Residuen gegen angepasste Werte nennt man Residualplot. Darin k√∂nnen wir erkennen, dass unsere Residuen um die Null schwanken. Die erste Annahme ist also schon einmal erf√ºllt. Die Homoskedastizit√§t ist nicht ganz erf√ºllt. Bei h√∂heren angepassten Werten scheinen die Residuen st√§rker zu schwanken. Allerdings gibt es nur sehr wenige Werte < 250 Minuten. Daher ist es schwierig, daraus eine echte Heteorskedastizit√§t abzuleiten. Wichtig ist, dass wir keine systematische Zunahme oder Abnahme der Variabilit√§t beobachten und auch keine sonstigen Muster den Residuen. Die zweite Modellannahme k√∂nnen wir auch als erf√ºllt abhaken. Wir k√∂nnen plausibel davon ausgehen, dass die Residuen unabh√§ngig sind. Denn wir haben weder eine Zeitreihe gemessen, noch Leute mehrfach befragt.Die Zusammenfassung des linearen Modells bietet noch weiter Informationen.Hier gibt es noch die Spaltenstd_error: Standardfehler der Sch√§tzung des jeweiligen Modellparameters (intercept oder anreise)statistics: Wert der \\(t\\)-Statistik f√ºr einen Hypothesentest, der √ºberf√ºft, ob der jeweilige Modellparameter Null ist. Die Nullhypothese lautet \\(\\beta_0 = 0\\) f√ºr den \\(y\\)-Achsenabschnitt bzw. \\(\\beta_1 = 0\\) f√ºr die Steigung. Die Alternativhypothese lautet, dass der jeweilige Modellparameter ungleich Null ist.p-value: p-Wert des Hypothesentestslower_ci, upper_ci: unterer und oberer Wert des Konfidentintervalls (Standardeinstellung 95%)F√ºr die Berechnung des Standardfehlers, des Hypothesentests und der Konfidenzintervalle wird, zus√§tzlich zu den oben genannten Annahmen, vorausgesetzt, dass die Residuen normalverteilt sind. Nur wenn sie es tats√§chlich sind, sind diese Berechnungen und der Hypothesentest korrekt, sonst m√∂glicherweise nicht. Deswegen m√ºssen wir, bevor wir diese Spalten interpretieren, √ºberpr√ºfen, ob die Residuen normalverteilt sind. Das machen wir mit einem QQ-Plot. Sie d√ºrfen auch noch einen formalen shapiro.test() machen, wenn Sie m√∂chten.Wenn die Residuen normalverteilt sind, dann liegen sie nahe der Geraden im QQ-Plot. Das ist hier der Fall. Die Annahme der Normalverteilung der Residuen ist also erf√ºllt und wir d√ºrfen den Standardfehler, den p-Wert und die Konfidenzintervalle interpretieren.","code":"\nggplot(data = model_res, aes(x = fitted, y = residuals)) + \n  geom_point() +\n  geom_hline(yintercept=0, col = 'red') + \n  labs(x = 'Angepasste Werte', y = 'Residuen')\nget_regression_table(lin_mod) %>% kable()\nggplot(model_res, aes(sample = residuals)) + \n  stat_qq() + stat_qq_line()"},{"path":"regression.html","id":"modell-interpretieren","chapter":"Kapitel 11 Lineare Regression","heading":"11.4.4 Modell interpretieren","text":"unserem fiktiven Datensatz, nimmt die Arbeitseit der Bibliothek mit der Anreisezeit ab. Wir haben bereits die Sch√§tzungen des \\(y\\)-Achsenabschnitts (intercept) und der Steigung (anreise) interpretiert. Jetzt k√∂nnen wir zus√§tzlich sagen, wie gut wir gesch√§tzt haben. Der \\(y\\)-Achsenabschnitt gibt die Anreisezeit der Bibliothek , wenn die Anreisezeit Null w√§re. Das ist zwar die korrekte Interpretation, aber eine Extrapolation au√üerhalb unseres Messbereichs der Anreisezeit. Daher sollte man den \\(y\\)-Achsenabschnitt nicht √ºberinterpretieren. Er ist erst einmal ein Modellparameter, der gut gesch√§tzt wurde mit einem Konfidenzintervall von [297.594, 306.594]. Ob die Studierenden tats√§chlich diese Zeit der Bibliothek verbringen w√ºrden, wenn sie quasi auf dem Campus wohnen w√ºrden, ist eine Spekulation.Einfacher und sinnvoller ist die Interpretation der Steigung. Ihr Konfidenzintervall lautet [-0.867, -0.665]. Es ist eine gute Sch√§tzung, da das Konfidenzintervall schmal ist. Pro zus√§tzlicher Anreiseminute sinkt die Arbeitszeit der Bibliothek um einen Wert diesem Konfidenzintervall.Die Hypothesentests k√∂nnen Sie interpretieren, wie gewohnt, m√ºssen Sie aber nicht. Wie den vorherigen Kapiteln erw√§hnt, kann man auf den Begriff signifikant getrost verzichten. Die Sch√§tzung der Parameter und deren Konfidenzintervalle bringt sehr viel mehr Information.\nRegression beinhaltet eine gro√üe Familie Modellen.\n\nLineare Regression: linearer Einfluss der Kovariablen auf \\(y\\).\n\nAnnahmen: Residuen \\(\\varepsilon_i\\) sind iid mit \\[\\mathrm{E}(\\varepsilon_i) = 0, \\qquad \\mathrm{Var}(\\varepsilon_i)=\\sigma^2.\\]\n\nNormalregression zus√§tzlich: \\(\\varepsilon_i \\sim N(0,\\sigma^2)\\).\n\n√úberpr√ºfung der Annahmen (vor allem) grafisch.\n\nKonfidenzintervalle und Hypothesentests der Normalregression d√ºrfen nur dann interpretiert werden, wenn die Annahmen erf√ºllt sind.\n","code":""},{"path":"regression.html","id":"lesestoff-9","chapter":"Kapitel 11 Lineare Regression","heading":"11.5 Lesestoff","text":"Kapitel 5 Ismay Kim (2021)","code":""},{"path":"regression.html","id":"aufgaben-8","chapter":"Kapitel 11 Lineare Regression","heading":"11.6 Aufgaben","text":"","code":""},{"path":"regression.html","id":"kategorielle-variable-als-pr√§diktor","chapter":"Kapitel 11 Lineare Regression","heading":"11.6.1 Kategorielle Variable als Pr√§diktor","text":"Arbeiten Sie selbst√§ndig das Beispiel Kapitel 5.2 Ismay Kim (2021) durch.","code":""},{"path":"daten-und-bericht.html","id":"daten-und-bericht","chapter":"Kapitel 12 Datenquellen und Struktur des Abschlussberichts","heading":"Kapitel 12 Datenquellen und Struktur des Abschlussberichts","text":"\nVerschiedenen Datenquellen kennen\n\nM√∂gliche R-Pakete f√ºr direkten Datenbankzugang kennen\n","code":""},{"path":"daten-und-bericht.html","id":"datenquellen","chapter":"Kapitel 12 Datenquellen und Struktur des Abschlussberichts","heading":"12.1 Datenquellen","text":"diesem Abschnitt beschreibe ich einige m√∂gliche Datenquellen, die Sie f√ºr Ihre Abschlussberichte nutzen k√∂nnen. Technische Unterst√ºtzung kann ich nur beim Paket eurostat anbieten. Andere Pakete m√ºssen Sie selbst√§ndig entdecken.","code":""},{"path":"daten-und-bericht.html","id":"statistsiches-bundesamt","chapter":"Kapitel 12 Datenquellen und Struktur des Abschlussberichts","heading":"12.1.1 Statistsiches Bundesamt","text":"Das Statistische Bundesamt bietet Daten zu und √ºber Deutschland. deren Datenbank GENESIS. Es ist wichtig, dass Sie beim Herunterladen (Werteabruf) das Format flat w√§hlen. Dann bekommen Sie einen tidy Datensatz.","code":""},{"path":"daten-und-bericht.html","id":"eurostat","chapter":"Kapitel 12 Datenquellen und Struktur des Abschlussberichts","heading":"12.1.2 eurostat","text":"eurostat ist das statistische Amt der Europ√§ischen Union. Auf ihrer Seite finden Sie viele Informationen und statistische Daten √ºber und zu Europa.Die Daten von eurostat k√∂nnen direkt mit dem Paket eurostat tidy heruntergeladen werden. Das Paket hat eine sehr gute Homepage und Tutorien Unter dem Reiter Articles finden Sie auch ein Tutorium, das die Darstellung der Daten als Karten (auch interaktiv) zeigt.","code":""},{"path":"daten-und-bericht.html","id":"gapminder","chapter":"Kapitel 12 Datenquellen und Struktur des Abschlussberichts","heading":"12.1.3 gapminder","text":"Sie haben einen Auszug aus den Daten von gapminder bereits kennen gelernt. Es gibt aber noch viel mehr dort zu entdecken. Die Daten k√∂nnen Sie per Hand hier herunter laden. Besser ist jedoch, sich mit dem DDF Format (data description format) auseinander zu setzten. Dieses bietet tidy .csv-Dateien . Der vollst√§ndige Datensatz von gapminder kann hier herunter geladen werden.","code":""},{"path":"daten-und-bericht.html","id":"national-oceanic-and-atmospheric-administration-noaa","chapter":"Kapitel 12 Datenquellen und Struktur des Abschlussberichts","heading":"12.1.4 National Oceanic and Atmospheric Administration (NOAA)","text":"NOAA bietet zahlreiche Datens√§tze zu Ozeanen, Wetter und Klima . Sie k√∂nnen die Daten mit dem Paket rnoaa direkt herunterladen.","code":""},{"path":"daten-und-bericht.html","id":"weitere-datenquellen","chapter":"Kapitel 12 Datenquellen und Struktur des Abschlussberichts","heading":"12.1.5 Weitere Datenquellen","text":"World Bank Open DataWorld Happiness ReportGlobal Carbon Budget 2020\nPublikation und Datensatz\nGlobal Carbon Project\nPublikation und DatensatzGlobal Carbon ProjectGehalte organischem Kohlenstoff B√∂den unter mehrj√§hrigen Kulturen\nPublikation\nDatensatz\nPublikationDatensatzPANGAEA: eine der gr√∂√üten Datenbanken f√ºr Umweltdaten√úberblick √ºber open data und Pakete f√ºr direkten Download auf ROpenScie:\nDaten\nPakete","code":""},{"path":"daten-und-bericht.html","id":"forschungsplan","chapter":"Kapitel 12 Datenquellen und Struktur des Abschlussberichts","heading":"12.2 Forschungsplan","text":"F√ºr Ihren Bericht erstellen Sie bitte einen Forschungsplan, den Sie ILAS hochladen. Nutzen Sie f√ºr den Forschungsplan das zur Verf√ºgung gestellte Template. Beachten Sie die Deadline ILIAS. Sie bekommen Feedback zu diesem Forschungsplan, bevor Sie Ihren eigentlichen Bericht erarbeiten. Damit sollten Missverst√§ndnisse bez√ºglich Inhalt und Schwere der Aufgabe vermieden werden.","code":""},{"path":"daten-und-bericht.html","id":"struktur-des-abschlussberichts","chapter":"Kapitel 12 Datenquellen und Struktur des Abschlussberichts","heading":"12.3 Struktur des Abschlussberichts","text":"","code":""},{"path":"daten-und-bericht.html","id":"struktur-des-arbeitsverzeichnisses","chapter":"Kapitel 12 Datenquellen und Struktur des Abschlussberichts","heading":"12.3.1 Struktur des Arbeitsverzeichnisses","text":"F√ºr Ihren Bericht, legen Sie bitte ein eigenes R-Projekt . Eine ausf√ºhrliche Anleitung finden sie hier. Ein Projekt hilft Ihnen, Ihre Arbeit gut zu organisieren und nicht den √úberblick √ºber verschiedene Dateien zu verlieren.Ihrem R-Projekt-Ordner legen Sie einen Ordner f√ºr Daten, gegebenenfalls einen f√ºr Hilfsskripte und gegebenenfalls einen f√ºr Abbildungen, die Sie zus√§tzlich im Bericht zeigen m√∂chten. Ihr R-Notebook verbleibt im Wurzelverzeichnis des Projekts.","code":""},{"path":"daten-und-bericht.html","id":"daten-herunterladen-und-speichern","chapter":"Kapitel 12 Datenquellen und Struktur des Abschlussberichts","heading":"12.3.2 Daten herunterladen und speichern","text":"Falls Sie Daten mit Hilfe eines R-Pakets herunterladen, dann legen Sie daf√ºr ein gesondertes R-Notebook und beschreiben Sie den Vorgang: Wann und woher wurde der Datensatz heruntergeladen. Speichern Sie den Datensatz im Ordner Daten. Analysieren Sie den gespeicherten Datensatz. kann ich sp√§ter beim Korrigieren auf denselben (abgespeicherten) Datensatz zugreifen wie Sie. Denn Datenbanken k√∂nnen zwischen Abgabe und Korrektur aktualisiert werden.","code":""},{"path":"daten-und-bericht.html","id":"struktur-des-berichts","chapter":"Kapitel 12 Datenquellen und Struktur des Abschlussberichts","heading":"12.3.3 Struktur des Berichts","text":"Strukturieren Sie Ihren Bericht wie folgt:Einf√ºhrung (Ende muss die Forschungsfrage stehen)Material und Methoden: Datenbeschreibung, Beschreibung der Methode mit Literaturangaben, gegebenenfalls Beschreibung des UntersuchungsgebietsErgebnisse: Explorative Datenanalyse, weitere AnalysenDiskussion mit Einbezug von weiterer FachliteraturSchlussfolgerungLiteraturSie k√∂nnen Ergebnisse und Diskussion zu einem Abschnitt vereinen.Jeder Bericht muss eine Challenge beinhalten. Das kann z.B. das Vorbereiten (tidy wrangle) eines besonders komplizierten Datensatzes sein. Oder die Einarbeitung ein spannendes Paket (z.B. r√§umliche Darstellung eurostat) etc.Sie k√∂nnen den Bericht Zweiergruppen anfertigen. Dazu rate ich sogar. diesem Fall m√ºssen Sie im Bericht Ihren eigenen Anteil klar mit Ihrem Namen ausweisen. Es m√ºssen sowohl Text als auch Analyse (Code) von jedem Gruppenmitglied bearbeitet werden.Es gibt keine L√§ngenvorgabe f√ºr Ihren Bericht. Seien Sie concise: viel wie n√∂tig aber nicht mehr.Ende kniten Sie den Bericht zu einem html-Dokument. √úberlegen Sie, welche Code-Chunks im Bericht zu sehen sein m√ºssen und vermeiden Sie Redundanz. Wichtig: das Notebook muss bei mir lauff√§hig sein, d.h. Sie m√ºssen alle Zusatzskripte, Daten etc. mit richtigen relativen Pfaden im Projekt ansprechen.\nLaden Sie den gesamten Projektordner als zip-Datei auf ILIAS hoch. Die Deadline steht auf ILIAS.\n","code":""},{"path":"aufgabensammlung.html","id":"aufgabensammlung","chapter":"Kapitel 13 Aufgabensammlung","heading":"Kapitel 13 Aufgabensammlung","text":"Diese Aufgabensammlung enth√§lt zus√§tzlich Aufgaben, die z.B. im Seminar bearbeitet werden. Aufgaben zu den einzelnen Kapiteln finden Sie Ende des jeweiligen Kapitels.","code":""},{"path":"aufgabensammlung.html","id":"werdeschlau","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.1 Tutorial Zusammenhangsma√üe","text":"Begleitend zu diesem Tutorial sind die Kapitel 2 und 3 Mittag (2017) ‚ÄúStatistik‚Äù zu lesen.","code":""},{"path":"aufgabensammlung.html","id":"befragung-der-studierenden-der-uni-werdeschlau","chapter":"Kapitel 13 Aufgabensammlung","heading":"Befragung der Studierenden der Uni Werdeschlau","text":"Wir besch√§ftigen uns mit einem fiktiven Beispiel.der (kleinen) Universit√§t Werdeschlau m√∂chte die Studierendenvertretung wissen, ob sich eine Station zum Ausleihen von Fahrr√§dern lohnen w√ºrde. Dazu befragen sie die Studierenden, wie lange sie zur Uni fahren. Zudem wollen Sie wissen, ob die Anreisezeit und die Zeit, die die Studierenden pro Woche der Bibliothek verbringen, zusammen h√§ngen.","code":""},{"path":"aufgabensammlung.html","id":"grundgesamtheit-generieren","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.1.1 Grundgesamtheit generieren","text":"Unsere Grundgesamtheit sind alle 12000 Studierende von der Uni Werdeschlau. Wir erstellen uns diesem Beispiel selbst unsere Grundgesamtheit aus der Gleichverteilung. Die Regeln dazu sind absolut frei erfunden. Die meisten Studierenden sind zwischen 5 und 40 Minuten unterwegs; 20% jedoch haben eine l√§ngere Anreise zwischen 60 und 120 Minuten.Lassen Sie den Code laufen.Wir setzen geschlecht, wohnort, verkehrsmittel, anreise und zeit_bib zu einer Datenmatrix (tibble) zusammen und nennen das Objekt grundgesamtheit.Lassen Sie den folgenden Code laufen.","code":"\nlibrary(tidyverse)\nset.seed(123)\nanreise <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n                     runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngeschlecht <- sample(c('m', 'w'), size = 12000, replace = TRUE)\nwohnort <- sapply(anreise, function(x) {\n  if(x < 30) 'stadt'\n  else 'land'\n})\nverkehrsmittel <- sapply(anreise, function(x) {\n  if(x <= 10) 'zu_fuss'\n  else if(x > 10 & x <= 15) sample(c('zu_fuss', 'fahrrad'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'fahrrad', 'auto'), size = 1)\n  else sample(c('bus', 'auto'), size = 1)\n})\n\nzeit_bib <- 5 * 60 - 0.7 * anreise + rnorm(length(anreise), 0, 20)\ngrundgesamtheit <- tibble(geschlecht, wohnort, verkehrsmittel, anreise, zeit_bib)"},{"path":"aufgabensammlung.html","id":"befragung-simulieren","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.1.2 Befragung simulieren","text":"der Realit√§t werden nat√ºrlich nicht alle 12000 Studierende befragt (wer hat schon viele Kapazit√§ten?), sondern eine zuf√§llige Stichprobe erhoben, also eine Teilmenge der Grundgesamtheit.Frage: Wie nennt man die StudierendenMerkmalstr√§gerBefragte oderModalwerte?Falls Sie nicht sicher sind: Lesen Sie Kapitel 2 Mittag (2017) ‚ÄúStatistik!‚ÄùUm unsere Stichprobe zu erstellen, ziehen wir ohne Zur√ºcklegen aus unserer Grundgesamtheit. Sagen wir mal, die Kapazit√§ten der Befragenden reichen f√ºr 200 Befragungen.Lassen Sie den folgenden Code laufen.","code":"\nbefragung <- grundgesamtheit[sample(1:dim(grundgesamtheit)[1], size = 200, replace = FALSE), ]"},{"path":"aufgabensammlung.html","id":"kurze-explorative-datenanalyse","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.1.3 Kurze explorative Datenanalyse","text":"Von hier arbeiten wir mit unserer Stichprobe befragung. Sehen wir uns mal die empirische Verteilung, d.h. die Verteilung der Stichprobe .Plotten Sie ein Histogramm der Anreisezeiten unserer Stichprobe. Lassen Sie den Code laufen und passen Sie die bins sinnvoll .","code":"\nggplot(data = befragung, aes(x = anreise)) +\n  geom_histogram(bins = 5) +\n  xlab('Tagliche Anreise (min)') +\n  ylab('H√§ufigkeit')"},{"path":"aufgabensammlung.html","id":"lage--und-streuungsma√üe","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.1.4 Lage- und Streuungsma√üe","text":"Erstellen Sie die F√ºnf-Punkte-Zusammenfassung der Stichprobe. Lassen Sie den Code laufen.Wie Sie sehen k√∂nnen, behandelt R die kategorischen Daten anders als numerische. Kategorische Variablen werden als absolute H√§ufigkeiten pro Kategorie dargestellt (d.h. ausgez√§hlt).Wir vergleichen nun gew√∂hnliche und robuste Lage- und Streuungsma√üe. Zu gew√∂hnlichen geh√∂ren der Mittelwert (genauer arithmetisches Mittel) und die Standardabwechung.Berechnen Sie den Mittelwert und die Standardabweichung der Anreise.Zu den robusten Ma√üen geh√∂ren der Median, der Interquartilabstand und die mittlere absolute Abweichung vom Median (MAD). MAD beschreibt die durchschnittlich Abweichung der Stichprobe von ihrem Median.Berechnen Sie den Median der Stichprobe.Den Interquartilabstand bekommt man mit Hilfe der Funtion quantile, die die Quantile berechnet.Lassen Sie den folgenden Code laufen.Der Interquartilabstand kann auch direkt mit der Funktion IQR() berechnet werden und MAD mit mad().Berechnen Sie den Interquartilabstand und MAD der Stichprobe.Vergleichen Sie die gew√∂hnlichen und die robusten Ma√üe. Es gilt: je gr√∂√üer die au√üergew√∂hnlichen Datenpunkte, d.h. je h√∂her die h√∂chsten Anfahrtszeiten, desto st√§rker reagieren der Mittelwert und die Standardabweichung. Die robusten Ma√üe bleiben (solange der Anteil der hohen Anfahrtszeiten nicht zu gro√ü ist) relativ konstant.Abgesehen vom Aspekt der Robustheit, ist das arithmetische Mittel nicht immer ein geeigneter Durchschnittswert. F√ºr Wachstumsfaktoren (Preissteigerung, Zinsen) ist das geometrische Mittel die korrekte Mittelung und f√ºr die Berechnung von Durchschnittsgeschwindigkeiten das harmonische Mittel (Details z.B. bei Fahrmeir et al.¬†(2016). Statistik. Der Weg zur Datenanalyse).","code":"\nsummary(befragung)\nquantile(x = befragung$anreise, probs = 0.75) - quantile(x = befragung$anreise, probs = 0.25)"},{"path":"aufgabensammlung.html","id":"zufall-in-der-befragung","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.1.5 Zufall in der Befragung","text":"Wir haben unsere Befragten rein zuf√§llig ausgew√§hlt. D.h. die Kenngr√∂√üen h√§ngen von dieser zuf√§lligen Auswahl ab. Wie w√ºrden sie sich ver√§ndern, wenn wir eine andere Stichprobe ausgesucht h√§tten?Wir simulieren eine wiederholte Befragung und sehen uns , wie sich der Mittelwert und der Median entwickeln. Lassen Sie den Code laufen.Stellen Sie die Mittelwerte mean_all als Histogramm dar. Lassen Sie den Code laufen und passen Sie die bins sinnvoll .Stellen Sie die Mediane median_all als Histogramm dar. Lassen Sie den Code laufen und passen Sie die bins sinnvoll .","code":"\nanzahl_der_befragungen <- 100\n\n# Leere Vektoren erstellen, in die sp√§ter die Ergebnisse geschrieben werden.\nmean_all <- vector()\nmedian_all <- vector()\n\n\nfor (i in 1:anzahl_der_befragungen){\n  dat <- grundgesamtheit[sample(1:dim(grundgesamtheit)[1], size = 200, replace = FALSE), ]\n  mean_all[i] <- mean(dat$anreise)\n  median_all[i] <- median(dat$anreise)\n}\n\nkennzahlen <- tibble(mean_all, median_all)\nggplot(data = kennzahlen, aes(x = mean_all)) +\n  geom_histogram(bins = 5) +\n  xlab('Mittelwerte der taglichen Anreise (min)') +\n  ylab('H√§ufigkeit')\nggplot(data = kennzahlen, aes(x = median_all)) +\n  geom_histogram(bins = 5) +\n  xlab('Mediane der taglichen Anreise (min)') +\n  ylab('H√§ufigkeit')"},{"path":"aufgabensammlung.html","id":"zusammenhangsma√üe-f√ºr-nominalskalierte-merkmale","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.1.6 Zusammenhangsma√üe f√ºr nominalskalierte Merkmale","text":"Gibt es Pr√§ferenzen f√ºr bestimmte Verkehrsmittel je nach Geschlecht?\nWir sehen uns die Kontingenztabelle dazu .Lassen Sie den folgenden Code laufen.Nun berechnen wir die Zusammenhangsma√üe auf dieser Tabelle. Dazu nutzen wir die Bibliothek vcd (Visualizing Categorical Data), sehr empfehlenswert, wenn Sie kategorische Daten analysieren wollen.Lassen Sie den folgenden Code laufen.Der Phi-Koeffizient ist NA, weil er nur auf \\(2 \\times 2\\)-Tabellen definiert ist. Die beiden anderen, Contingency Coeff. (Pearson Koeffizient K) und Cramer‚Äôs V zeigen unterschiedliche Werte , je nachdem, wie unsere zuf√§llige Stichprobe ausf√§llt. Aber der Zusammenhang sollte eher klein sein.Allgemein gilt: Contingency Coeff. (Pearson Koeffizient K) und Cramer‚Äôs V schwanken zwischen 0 (kein Zusammenhang) und 1 (perfekter Zusammenhang). Wichtig: Sie zeigen keine Richtung . D.h. wir wissen nicht, ob der Zusammenhang positiv - ‚Äúje mehr desto mehr‚Äù oder negativ ‚Äúje mehr desto weniger‚Äù ist.Die ersten beiden Zeilen, die assocstats liefert, geh√∂ren zum Thema Hypothsentests. Das besprechen wir sp√§ter.Gibt es einen Zusammenhang zwischen dem Wohnort und dem ausgesuchten Verkehrsmittel?Berechnen Sie die Kontingenztabelle. Nutzen Sie die Funktion table() auf den Spalten wohnort und verkehrsmittel im Datensatz befragung.Berechnen Sie die Zusammenhangsma√üe. Lassen Sie den Code laufen.Gibt es eine Pr√§ferenz f√ºr den Wohnort je Geschlecht? Lassen Sie den Code laufen.Da Sie ja genau wissen, wie die Daten erstellt wurden, sollten Ihnen die Antworten nicht zu schwer fallen.","code":"\ntable(befragung$geschlecht, befragung$verkehrsmittel)\nlibrary(vcd)\nassocstats(table(befragung$geschlecht, befragung$verkehrsmittel))\nassocstats(table(befragung$wohnort, befragung$verkehrsmittel))\nassocstats(table(befragung$wohnort, befragung$geschlecht))"},{"path":"aufgabensammlung.html","id":"zusammenhangsma√üe-f√ºr-metrische-merkmale","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.1.7 Zusammenhangsma√üe f√ºr metrische Merkmale","text":"Zum Schluss wenden wir uns den beiden numerischen Variablen, anreise und zeit_bib. Besteht hier eine Korrelation?Zuerst stellen wir die Daten dar.Lassen Sie den folgenden Code laufen.Der Zusammenhang zwischen den beiden Variablen ist (ziemlich) linear und negativ.Wir berechnen beide Korrelationsma√üe, den Pearson Korrelationskoeffizienten f√ºr lineare Zusammenh√§nge und den Spearman Korrelationskoeffizient f√ºr monotone Zusammenh√§nge.Lassen Sie den Code laufen.Beide Koeffizienten sind nah dran dem Faktor 0.7, den wir zum Simulieren unserer Daten verwendet haben. Man darf hier beide verwenden. Wichtig ist es zu berichten, welchen Sie verwenden. D.h. es ist nicht genug, zu schreiben, dass Sie eine Korrelation berechnet haben.Gut gemacht! Lassen Sie den Code laufen.","code":"\nggplot(data = befragung, aes(x = anreise, y = zeit_bib)) +\n  geom_point() +\n  xlab('T√§gliche Anreise (min)') +\n  ylab('Zeit in der Bibliothek pro Woche (min)')\n# Pearson Korrelationskoeffizient\ncor(befragung$anreise, befragung$zeit_bib, method = 'pearson')\n\n# Spearman Korrelationskoeffizient\ncor(befragung$anreise, befragung$zeit_bib, method = 'spearman')\nlibrary(ggplot2)\nlibrary(emojifont)\nggplot() + geom_fontawesome(\"fa-coffee\", color='lightblue', size = 80) + theme_void() + ggtitle(\"It's time for coffee\")"},{"path":"aufgabensammlung.html","id":"aufgabe-einreichen","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.1.8 Aufgabe einreichen","text":"Speichern Sie Ihr Notebook und laden Sie es vom Server herunter.Laden Sie Ihr Notebook ILIAS hoch. Sie bekommen eine Musterl√∂sungVergleichen Sie Ihre L√∂sung mit der Musterl√∂sung.Fertig!","code":""},{"path":"aufgabensammlung.html","id":"der-explorative-workflow-mit-tidyverse","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.2 Der explorative Workflow mit tidyverse","text":"","code":""},{"path":"aufgabensammlung.html","id":"r-hausaufgaben","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.2.1 R-Hausaufgaben","text":"dem Kurs ‚ÄúEinf√ºhrung R‚Äù nehmen 49 Studierende teil. Der Leistungsnachweis besteht aus Hausaufgaben, die insgesamt mit 100 Punkten bewertet werden. Ab 50 Punkten gilt der Kurs als bestanden.Lesen Sie den Datensatz R-.txt, der die Endpunkte enth√§lt, ein.Ermitteln Sie, wie viele Teilnehmer bestanden und wie viele nicht bestanden haben. Nutzen Sie dazu die Funktion mutate() und die Funktion ifelse().","code":""},{"path":"aufgabensammlung.html","id":"klausur","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.2.2 Unfaire Klausur?","text":"Ar belegt im 4. Semester die Veranstaltung ‚ÄúSpa√ü mit R.‚Äù Bei der Klausur gibt es 2 Aufgabengruppen mit jeweils 60 Punkten. Aufgabengruppe 1 wird Studierende auf ungeraden Sitzpl√§tzen und Aufgabengruppe 2 Studierende auf geraden Sitzpl√§tzen ausgegeben.Lesen Sie den Datensatz Klausurpunkte.txt ein.√úberpr√ºfen Sie Ars Vermutung, dass die Aufgabengruppe 1 im Schnitt leichter war als Aufgabengruppe 2 (d.h. der Gruppe 1 im Schnitt mehr Punkte erzielt wurden). Nutzen Sie die Funktionen mutate(), um eine Spalte mit der Gruppenzugeh√∂rigkeit zu ermitteln und summarize(), um den Mittelwert zu berechnen. Tipp: mit dem Operator %% k√∂nnen Sie √ºberpr√ºfen, ob eine Zahl z.B. durch 2 teilbar ist.","code":""},{"path":"aufgabensammlung.html","id":"flederm√§use","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.2.3 Flederm√§use","text":"Ar Stat untersucht im Rahmen eines √∂kologischen Praktikums Flederm√§use Ecuador. Er misst die Gr√∂√üe der Tiere und bestimmt ihr Geschlecht.Laden Sie den Datensatz Fledermaus.txt. Dieser enth√§lt die Gr√∂√üe der Tiere cm.Die ersten 20 untersuchten Tiere sind M√§nnchen. Erstellen Sie einen factor ‚Äúgeschlecht,‚Äù der das Geschlecht der Tiere enth√§lt. Benutzen Sie dazu die Funktionen rep().F√ºgen Sie die Informationen √ºber die Gr√∂√üe und das Geschlecht einem tibble zusammen und bennen Sie die Spalten entsprechend.Bei dem 3. Individuum hat sich Ar vertippt. Die Gr√∂√üe des Tieres lautet Wirklichkeit 5,37 cm. Korrigieren Sie den Fehler.Speichern Sie den korrigierten Datensatz ab.Berechnen Sie die Mittelwerte und die Standardabweichungen der Gr√∂√üen der Tiere je Geschlecht.Plotten Sie die Gr√∂√üen je Geschlecht einem Boxplot und speichern Sie diesen ab.","code":""},{"path":"aufgabensammlung.html","id":"flederm√§use-revisited","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.2.4 Flederm√§use, revisited","text":"Wir gehen davon aus, dass das Alter der Flederm√§use mit ihrer Gr√∂√üe zusammenh√§ngt. Pauschal legen wir fest, dass ein Tier, das kleiner als 5 cm gro√ü ist, ein Jungtier ist.Klassifizieren Sie die Tiere J (Jungtier) und E (Erwachsen). Erstellen Sie dazu mit mutate() ein eigene Spalte.Wie viele Jungtiere gibt es im Datensatz?Sind die Jungtiere weiblich oder m√§nnlich?","code":""},{"path":"aufgabensammlung.html","id":"zeitreihen-aus-dem-lehstenbacheinzugsgebiet-ne-bayern","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.2.5 Zeitreihen aus dem Lehstenbacheinzugsgebiet (NE Bayern)","text":"Im Lehstenbacheinzugsgebiet wurden √ºber eine l√§ngere Zeit Niederschlag, Abfluss und Temperatur gemessen. Die Messeinheiten sind f√ºr Temperatur ¬∞C, f√ºr Niederschlag und Abfluss mm.Laden Sie den Datensatz Data.dat.Wandeln Sie die Spalte Date ein richtiges Datum um.Plotten Sie die Temperatur, den Niederschlag und den Abfluss (verschiedene Grafiken) untereinander. Beschriften Sie alles korrekt und f√ºgen Sie Titel hinzu. √úberlegen Sie, welche Darstellungsart (geom) f√ºr den Niederschlag besten ist.Speichern Sie die Grafik als pdf ab.","code":""},{"path":"aufgabensammlung.html","id":"umweltdaten-entlang-der-d√§nischen-k√ºste","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.2.6 Umweltdaten entlang der d√§nischen K√ºste","text":"Die Datei Temperatur.csv aus Zuur, Ieno, Meesters (2009) enth√§lt Messungen von Temperatur, Salinit√§t und Chlorophyl 31 Orten entlang der d√§nischen K√ºste. Der Datensatz kann hier heruntergeladen werden. Die Daten stammen vom d√§nischen Institut RIKZ (Monitoringprogramm MWTL: Monitoring Waterstaatkundige Toestand des Lands). Die Messungen wurden zwischen 1990 und 2005 durchgef√ºhrt mit einer H√§ufigkeit von 0‚Äì4 mal pro Monat je nach Jahreszeit.Lesen Sie den Datensatz Temperatur.csv ein.Konvertieren Sie die Spalte Date ein richtiges Datumsformat und plotten Sie die Temperaturen pro Station (facet_wrap()) als Zeitreihen.Berechnen Sie die Anzahl der Messwerte, Monatsmittelwerte der Temperatur f√ºr alle Stationen, sowie die Standardabweichungen.Stellen Sie die Monatsmittel der Temperatur als Linien dar.Beschriften Sie die Grafik sinnvoll.F√ºgen Sie die Standardabweichungen als Band hinzu.Speichern Sie die Grafik als pdf ab.","code":""},{"path":"aufgabensammlung.html","id":"umweltdaten-entlang-der-d√§nischen-k√ºste-revisited","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.2.7 Umweltdaten entlang der d√§nischen K√ºste, revisited","text":"Berechnen Sie die Monatsmittelwerte und Standardabweichungen je Station. Tipp: group_by(Station, Month).Stellen Sie die Daten mit einem Fehlerband dar (verschiedenen Plots mit facet_wrap()) und speichern Sie sie ab.","code":""},{"path":"aufgabensammlung.html","id":"bootstrapping-konfidenzintervalle-und-hypothesentests","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.3 Bootstrapping, Konfidenzintervalle und Hypothesentests","text":"","code":""},{"path":"aufgabensammlung.html","id":"unfaire-klausur-revisited","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.3.1 Unfaire Klausur, revisited","text":"Wir kommen zur√ºck auf die Klausurpunkte aus der Aufgabe 13.2.2. Waren die Aufgaben Gruppe 1 leichter? Nutzen Sie das Framework infer.","code":""},{"path":"aufgabensammlung.html","id":"artenvielfalt-in-grasl√§ndern","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.3.2 Artenvielfalt in Grasl√§ndern","text":"Sie erhalten Daten aus dem Grasland-Monitoring im Yellowstone Nationalpark und dem National Bison Range (USA) aus Zuur, Ieno, Meesters (2009). Der Datensatz kann hier heruntergeladen werden. Das Ziel des Monitorings ist die Untersuchung m√∂glicher √Ñnderungen der Biodiversit√§t und deren Zusammenhang mit Umweltfaktoren. Biodiversit√§t wurde durch die Anzahl unterschiedlicher Arten quantifiziert (Spalte R). Insgesamt haben die Forscher ca. 90 Arten 8 Transekten kartiert. Die Aufnahmen wurden alle 4 bis 10 Jahre wiederholt. Insgesamt liegen 58 Beobachtungen vor. Die Daten sind der Datei Vegetation2.xls gespeichert.Laden Sie den Datensatz R und sehen Sie sich seine Struktur . Von welchem Typ sind die einzelnen Variablen? Entspricht das Ihren Erwartungen? Diese Aufgabe dient dazu, das Einlesen von Excel-Dateien mit tidyverse zu erarbeiten. Tipp: nutzen Sie die Funktion read_xls() der Bibliothek readxl. Lesen Sie der Hilfe nach, wie Sie ein bestimmtes Tabellenblatt aus Excel einlesen k√∂nnen.Laden Sie den Datensatz R und sehen Sie sich seine Struktur . Von welchem Typ sind die einzelnen Variablen? Entspricht das Ihren Erwartungen? Diese Aufgabe dient dazu, das Einlesen von Excel-Dateien mit tidyverse zu erarbeiten. Tipp: nutzen Sie die Funktion read_xls() der Bibliothek readxl. Lesen Sie der Hilfe nach, wie Sie ein bestimmtes Tabellenblatt aus Excel einlesen k√∂nnen.Kurze explorative Analyse: Berechnen Sie die Anzahl der Messungen, den Mittelwert und die Standardabweichung der Artenzahl R pro Transekt.Kurze explorative Analyse: Berechnen Sie die Anzahl der Messungen, den Mittelwert und die Standardabweichung der Artenzahl R pro Transekt.Plotten Sie die Artenzahl gegen die Variable BARESOIL (Anteil von unbewachsenem Boden). F√§rben Sie die Punkte je nach Transekt unterschiedlich ein. Tipp: Transekt als as_factor() konvertieren.Plotten Sie die Artenzahl gegen die Variable BARESOIL (Anteil von unbewachsenem Boden). F√§rben Sie die Punkte je nach Transekt unterschiedlich ein. Tipp: Transekt als as_factor() konvertieren.F√ºgen Sie eine gl√§ttende Linie ohne Konfidenzband hinzu, die alle Punkte unabh√§ngig vom Transekt ber√ºcksichtigt (Abschnitt 4.2 im Buch ggplot2 (Wickham 2020)). Damit eine einzige gl√§ttende Linie f√ºr alle Transekte hinzugef√ºgt wird, m√ºssen Sie die aes f√ºr Farbe statt ggplot() geom_point() angeben.F√ºgen Sie eine gl√§ttende Linie ohne Konfidenzband hinzu, die alle Punkte unabh√§ngig vom Transekt ber√ºcksichtigt (Abschnitt 4.2 im Buch ggplot2 (Wickham 2020)). Damit eine einzige gl√§ttende Linie f√ºr alle Transekte hinzugef√ºgt wird, m√ºssen Sie die aes f√ºr Farbe statt ggplot() geom_point() angeben.Beschriften Sie die Grafik (auch die Legende!) sinnvoll und speichern Sie sie als pdf ab.Beschriften Sie die Grafik (auch die Legende!) sinnvoll und speichern Sie sie als pdf ab.Stellen Sie die Artenzahl je Transekt als Zeitreihe dar. Die Symbole sollen sowohl Punkte als auch Linien sein. Skalieren Sie die Gr√∂√üe der Punkte je nach Anteil des unbewachsenen Bodens (Abschnitt 12.1 im Buch ggplot2 (Wickham 2020)). Denken Sie nach, welcher Stelle die aes size stehen muss, damit nur die Punkte skaliert werden.Stellen Sie die Artenzahl je Transekt als Zeitreihe dar. Die Symbole sollen sowohl Punkte als auch Linien sein. Skalieren Sie die Gr√∂√üe der Punkte je nach Anteil des unbewachsenen Bodens (Abschnitt 12.1 im Buch ggplot2 (Wickham 2020)). Denken Sie nach, welcher Stelle die aes size stehen muss, damit nur die Punkte skaliert werden.Beschriften Sie die Grafik (auch die Legenden!) sinnvoll und speichern Sie sie als pdf ab.Beschriften Sie die Grafik (auch die Legenden!) sinnvoll und speichern Sie sie als pdf ab.Setzen Sie nun beide Grafiken untereinander, platzieren Sie die Legenden (ja nach gew√§hltem Layout) sinnvoll und speichern Sie die Grafiken ab.Setzen Sie nun beide Grafiken untereinander, platzieren Sie die Legenden (ja nach gew√§hltem Layout) sinnvoll und speichern Sie die Grafiken ab.Berechnen Sie den linearen Korrelationskoeffizienten zwischen der Biodiversit√§t und dem Anteil von unbewachsenem Boden und ermitteln Sie das 95%-Bootstrap-Konfidenzintervall.Berechnen Sie den linearen Korrelationskoeffizienten zwischen der Biodiversit√§t und dem Anteil von unbewachsenem Boden und ermitteln Sie das 95%-Bootstrap-Konfidenzintervall.Wenn Sie statt eines 95%-Konfidenzintervalls, das 90%-Konfidenzintervall berechnen, wird das Intervall breiter oder schmaler? Warum?Wenn Sie statt eines 95%-Konfidenzintervalls, das 90%-Konfidenzintervall berechnen, wird das Intervall breiter oder schmaler? Warum?","code":""},{"path":"aufgabensammlung.html","id":"verdichtung","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.3.3 Bodenverdichtung","text":"Schwere landwirtschaftliche Maschinen k√∂nnen beim Bearbeiten des Bodens zu Bodenverdichtung f√ºhren. einem randomisierten Design wurden zuf√§llig Parzellen (Spalte plots) auf einem sonst homogenen Feld entweder mit einer schweren Maschine bearbeitet (compacted) oder nicht (control). Auf allen Parzellen wurde die Lagerungsdichte bestimmt. Die Lagerungsdichte (auch Trockenrohdichte) ist ein Ma√ü f√ºr Bodenstruktur und gibt das Verh√§ltnis der Trockenmasse eines Bodens zu seinem Volumen. Sie wird h√§ufig [g/cm¬≥] gemessen und kann als ein Indikator f√ºr Bodenverdichtung genutzt werden. Der Datensatz ist der Date bd_compaction.csv gespeichert.Lesen Sie den Datensatz ein und f√ºhren Sie eine kurze explorative Datenanalyse durch.√úberpr√ºfen Sie, ob sich die Lagerungsdichte auf den bearbeiteten Feldern erh√∂ht hat.","code":""},{"path":"aufgabensammlung.html","id":"world-bank","chapter":"Kapitel 13 Aufgabensammlung","heading":"13.3.4 Anteil von besch√§ftigten Frauen im privaten und √∂ffentlichen Sektor","text":"Diese √úbung ist inspiriert von Anrew Heiss Blog: https://www.andrewheiss.com/blog/2018/12/05/test--hypothesis/Wir nutzen den Datensatz Worldwide Bureaucracy Indicators (WWBI). Diesen kann man bei der Weltbank herunter laden.Laden Sie den Datensatz als csv file herunter, speichern Sie Ihn auf Ihrer Festplatte ab. Der Datensatz wird als zip-Datei heruntergeladen. Laden Sie diese zip-Datei direkt auf den R-Server den Ordner Data hoch. De Server entpackte die Datei automatisch.Die Daten sind der Datei WWBIData.csv gespeichert. Sehen Sie sich die beiden anderen csv-Dateien . steht drin?Lesen Sie den Datensatz WWBIData.csv ein.Gemeinsame √úbung: Wir bereinigen den Datensatz und machen ihn tidy.Filtern Sie Daten f√ºr das Jahr 2012 und die Indikatoren ‚ÄúBI.PWK.PRVS.FE.ZS‚Äù und ‚ÄúBI.PWK.PUBS.FE.ZS.‚Äù Enfernen Sie NAs. bedeuten diese Indikatoren?Stellen Sie die Histogramme dieser Indikatoren dar.Gibt es Unterschied zwischen den Anteilen der besch√§ftigen Frauen im privaten und √∂ffentlichen Sektor?","code":""},{"path":"literatur.html","id":"literatur","chapter":"Literatur","heading":"Literatur","text":"","code":""}]
